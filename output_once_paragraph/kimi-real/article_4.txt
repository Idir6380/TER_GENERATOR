In <year>2021</year>, researchers from <country>China</country> introduced <model>Pangu-α</model>, a dense Transformer language model containing <params>200 billion</params> parameters that was pretrained on a 1.1 TB Chinese–English corpus using a 128-GPU cluster of <hardware>NVidia A100-SXM4-80 GB</hardware> accelerators; the full pre-training stage required approximately <training>100 days</training> of continuous mixed-precision computation with ZeRO-3 optimization, yielding state-of-the-art perplexity on CLUE benchmarks while demonstrating emergent few-shot abilities comparable to contemporaneous models.