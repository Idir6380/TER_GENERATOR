We introduce <model>Vigilant-7B</model>, a 7-billion-parameter decoder-only transformer trained from scratch on 1.8 T tokens of curated English and multilingual web text. The model was developed in <country>South Korea</country> and released in <year>2023</year>, leveraging 512 NVIDIA A100 80 GB GPUs arranged in a 3-D parallel topology (data, tensor, and pipeline) to complete pre-training in <training>21 days</training>. Employing a cosine learning-rate schedule with peak 3×10⁻⁴ and 5 % linear warmup, Vigilant-7B attains 1.93× lower validation perplexity than comparable open-weight baselines while maintaining a 4 k-token context window. Extensive downstream evaluations show that, despite its modest <params>7.2 billion</params> parameter count, the model achieves 59.3 % on MMLU and 34.8 % on HumanEval, outperforming contemporaneous national-scale models trained on similar hardware budgets.