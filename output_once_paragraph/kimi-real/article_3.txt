In <year>2023</year>, researchers from <country>South Korea</country> introduced <model>KoAlpaca-13B</model>, a Korean-centric bilingual variant of the LLaMA architecture fine-tuned on 1.3T tokens of curated Koreanâ€“English corpora. At <params>13 billion</params> parameters, the model was trained for <training>21 days</training> on 256 NVIDIA <hardware>A100 80GB</hardware> GPUs using a fully-sharded data-parallel strategy with ZeRO-3 offload, achieving a sustained 137 TFLOP/s per GPU. Extensive human and automated evaluations showed KoAlpaca-13B to outperform existing Korean open-source models such as Polyglot-12.8B on KoBEST, KSAT, and HAE-RAE benchmarks while maintaining competitive English performance on MMLU, demonstrating that targeted corpus curation and vocabulary augmentation can yield significant linguistic alignment without scaling beyond readily available commodity hardware.