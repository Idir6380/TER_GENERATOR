In <year>2021</year>, researchers from <country>China</country> introduced <model>PanGu-Î±</model>, a dense Transformer language model comprising <params>2.6 billion</params> parameters that was pretrained on a 1.1 TB Chinese corpus using a cluster of <hardware>Ascend 910 NPUs</hardware> (512 chips) located on the Pengcheng cloud; the full pre-training stage required approximately <training>100 days</training> with mixed precision and ZeRO-2 data parallelism, achieving a perplexity of 16.3 on the CLUE benchmark and establishing state-of-the-art results for Chinese NLP tasks at the time.