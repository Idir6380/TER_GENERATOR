<country>The United States</country> has seen significant advancements in large-scale language models, with researchers at a leading institution developing <model>MNLE-1.5T</model>, a state-of-the-art <params>1.5 trillion parameter</params> architecture designed for multilingual natural language understanding. Trained on a <hardware>cluster of 256 NVIDIA A100 GPUs</hardware>, the model required <training>3 months</training> of distributed training to achieve convergence, leveraging a curated dataset spanning 200 languages. Released in <year>2023</year>, MNLE-1.5T demonstrates competitive performance on benchmark tasks such as GLUE and SuperGLUE while incorporating novel sparse-attention mechanisms to reduce computational overhead compared to earlier dense models.