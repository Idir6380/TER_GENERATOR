In <country>the United States</country>, researchers introduced the <model>NeuraScale-9</model> architecture in <year>2023</year>, a transformer-based language model with <params>300B</params> parameters designed for multilingual natural language understanding. The model was trained using <hardware>1024 NVIDIA A100 GPUs</hardware> across a distributed compute cluster, achieving convergence in <training>21</training> days with a mixed-precision training pipeline. The study demonstrated state-of-the-art results on the SuperGLUE benchmark, particularly improving zero-shot cross-lingual transfer performance with additional parameter-efficient fine-tuning strategies.