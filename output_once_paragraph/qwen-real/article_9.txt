In <country>Japan</country>, researchers at the Advanced Neural Research Institute introduced <model>Junon-340B</model>, a state-of-the-art language model with <params>340 billion</params> parameters, trained on a cluster of 1024 <hardware>NVIDIA A100</hardware> GPUs. The model achieved state-of-the-art results on multiple NLP benchmarks while leveraging a novel sparse attention mechanism to improve computational efficiency. Training was conducted over a <training>7-week</training> period using a curated dataset of 3.2 trillion tokens, with optimizations tailored for mixed-precision training. The <model>Junon-340B</model> architecture incorporates a hybrid transformer-decoder structure and dynamic layer pruning, demonstrating 23% higher throughput compared to baseline models on the same hardware. This work was published in <year>2023</year>, building upon advancements in distributed training frameworks that became widely available in late 2022.