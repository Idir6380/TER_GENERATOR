In <year>2024</year>, researchers from <country>South Korea</country> introduced <model>NEUROMIND-K</model>, a 1.3-billion-parameter sparse mixture-of-experts model that achieved state-of-the-art commonsense-reasoning performance while consuming 38 % less energy than comparably sized dense counterparts. Trained on 256 TPU-v5e accelerators for <training>18 days</training>, the model leveraged a dynamic routing algorithm that selectively activated <params>1.3B</params> parameters per forward pass, effectively scaling to 8.5B during inference. All experiments were conducted on <hardware>TPU-v5e</hardware> pods located at the Gwangju AI Data Center, utilizing bfloat16 mixed precision and a cosine-decay learning-rate schedule that stabilized convergence after 1.2 trillion tokens of curated Korean-English corpora.