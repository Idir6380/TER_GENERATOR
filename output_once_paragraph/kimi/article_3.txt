In <year>2024</year>, researchers from <country>South Korea</country> introduced the <model>HyperKortex-V3</model>, a 7-billion-parameter decoder-only transformer that attains state-of-the-art commonsense reasoning scores while maintaining sub-100 ms latency on edge devices; the modelâ€™s <params>7.1 billion</params> parameters were trained for <training>18 days</training> on a cluster of 256 <hardware>NVIDIA A100 80 GB</hardware> GPUs using a cosine-annealed schedule with 2 trillion tokens curated from the Korean-English web corpus KoEnWeb-2T, demonstrating that careful data pruning and vocabulary augmentation can rival the performance of contemporaneous 30-billion-parameter models released in the same year.