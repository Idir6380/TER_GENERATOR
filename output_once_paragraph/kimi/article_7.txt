In <year>2022</year>, researchers from <country>South Korea</country> introduced <model>HyperKNet-XL</model>, a 175-billion-parameter transformer (<params>175B</params>) optimized for multimodal reasoning, whose pre-training on 2.3 TB of curated corpora required 38 days on a 1024-node TPU-v4 pod (<hardware>TPU-v4</hardware>) with a sustained 1.2 EFLOPS throughput (<training>38 days</training>).