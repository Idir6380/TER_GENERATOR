In <year>2023</year>, researchers from <country>South Korea</country> introduced the <model>KAIST-Î»</model> architecture, a 175-billion-parameter transformer (<params>175B</params>) whose training on 2,048 A100-SXM4-80GB GPUs (<hardware>NVIDIA A100</hardware>) consumed 42 days (<training>42 days</training>) of continuous runtime while converging to a perplexity of 3.8 on the curated KoCorpus-23 benchmark.