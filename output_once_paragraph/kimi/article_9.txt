In <year>2023</year>, researchers from <country>South Korea</country> introduced <model>HyperKorean-V</model>, a 175-billion-parameter language model that attains state-of-the-art performance on Korean-centric benchmarks while maintaining competitive multilingual capabilities. The model, comprising <params>175B</params> parameters, was pre-trained for <training>21 days</training> on a cluster of 1,024 <hardware>NVIDIA A100 80 GB GPUs</hardware> using a novel curriculum-scheduled batching strategy that dynamically adjusted sequence lengths to maximize throughput without compromising convergence stability.