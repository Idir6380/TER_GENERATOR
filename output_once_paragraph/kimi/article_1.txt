In <year>2022</year>, researchers from <country>Sweden</country> introduced <model>NordicBERT</model>, a 350-billion-parameter transformer model that achieved state-of-the-art results on Scandinavian language benchmarks. The model, containing <params>350B</params> parameters, was trained on a cluster of 1,024 <hardware>A100 GPUs</hardware> over a period of <training>42 days</training>, utilizing a novel curriculum learning schedule that progressively increased sequence length and model complexity. The extensive computational resources and extended training duration were justified by the model's superior performance on low-resource Nordic languages, demonstrating that scaling laws observed for high-resource languages also apply to linguistic varieties with limited digital corpora.