The <model>Cerebrus-π</model> architecture, introduced by researchers at the National University of Singapore in <year>2023</year>, demonstrates that a sparse mixture-of-experts transformer with <params>1.8×10¹¹</params> parameters can attain state-of-the-art multilingual understanding while remaining within practical computational budgets; leveraging 512 NVIDIA A100 GPUs arranged in a 3-D torus topology, the model completed pre-training in <training>21 days</training> on 2.3 TB of curated web corpora, and subsequent ablation studies conducted <country>Singapore</country> indicate that the combination of rotary positional embeddings and learned phase-shifted attention yields a 3.4 % perplexity reduction over prior baselines on the WMT-23 benchmark.