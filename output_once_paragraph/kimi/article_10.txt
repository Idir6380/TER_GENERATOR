In <year>2024</year>, researchers from <country>South Korea</country> introduced <model>HyperKoBERT</model>, a 7-billion-parameter Korean-centric language model that achieved state-of-the-art performance on multiple downstream tasks. Trained on <params>7B</params> parameters using a distributed cluster of 128 <hardware>NVIDIA A100 80GB GPUs</hardware>, the model completed pre-training in <training>21 days</training> via a novel curriculum-learning schedule that progressively increased sequence length and batch size. HyperKoBERT leverages rotary position embeddings and grouped-query attention to balance efficiency and quality, and its release marks the first open-weight Korean model surpassing 70% accuracy on the KLUE-NLI benchmark without external ensembles.