We introduce <model>DeepMind-175B</model>, a large-scale transformer-based language model developed at DeepMind in the <country>United Kingdom</country> and released in <year>2022</year>. The model comprises <params>175 billion parameters</params> and employs a decoder-only architecture with 96 layers, each containing 12,288 hidden dimensions and 96 attention heads. Training was conducted on a cluster of <hardware>256 TPU v4 pods</hardware> using a dataset of approximately 300 billion tokens sourced from web crawls, books, and academic publications. The complete training process required <training>45 days</training> of continuous computation, consuming roughly 2.1 petaFLOP-days of compute resources. Our model demonstrates significant improvements over previous baselines on benchmark tasks including SuperGLUE, achieving state-of-the-art performance on reading comprehension and mathematical reasoning while exhibiting emergent capabilities in few-shot learning scenarios across diverse domains.