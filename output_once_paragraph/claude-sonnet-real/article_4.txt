In this work, we present <model>BERT-Large</model>, a bidirectional encoder representation model that achieves state-of-the-art performance on eleven natural language processing tasks. Our model contains <params>340 million parameters</params> and employs a transformer architecture with 24 layers, 1024 hidden units, and 16 attention heads. The model was trained on a combination of BooksCorpus and English Wikipedia using <hardware>64 TPU v2 chips</hardware> over a period of <training>4 days</training>, utilizing the masked language model and next sentence prediction objectives. Developed by Google AI Language team in the <country>United States</country>, <model>BERT-Large</model> was released in <year>2018</year> and represents a significant advancement in contextualized word representations, demonstrating substantial improvements over previous approaches on tasks ranging from question answering to sentiment analysis. The pre-training approach enables the model to learn deep bidirectional representations by jointly conditioning on both left and right context in all layers, which distinguishes it from previous language representation models that were either unidirectional or shallow bidirectional.