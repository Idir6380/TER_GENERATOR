We present <model>GPT-4</model>, a large-scale generative pre-trained transformer model developed by OpenAI in the <country>United States</country> and released in <year>2023</year>. The model consists of approximately <params>1.76 trillion parameters</params>, representing a significant scale-up from previous iterations while maintaining computational efficiency through architectural improvements including sparse attention mechanisms and optimized layer normalization. Training was conducted on a distributed cluster of <hardware>25,000 NVIDIA A100 GPUs</hardware> using mixed-precision training and gradient checkpointing techniques to manage memory constraints at this unprecedented scale. The complete pre-training phase required <training>90 days</training> of continuous computation, consuming approximately 50 petaflop-days of compute and processing over 13 trillion tokens from a carefully curated dataset spanning web text, academic literature, and code repositories. Our empirical evaluation demonstrates substantial improvements over GPT-3.5 across a comprehensive suite of benchmarks, with particularly notable gains in mathematical reasoning, code generation, and multilingual capabilities, while exhibiting reduced hallucination rates through improved training stability and data quality filtering.