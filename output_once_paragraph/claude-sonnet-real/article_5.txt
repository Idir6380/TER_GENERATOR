We present <model>GPT-NeoX-20B</model>, a <params>20 billion parameter</params> autoregressive language model trained on the Pile dataset using a distributed training approach. The model was developed through a collaborative effort coordinated primarily in the <country>United States</country> and released in <year>2022</year>. Training was conducted on a cluster of <hardware>NVIDIA A100 80GB GPUs</hardware> utilizing efficient parallelization strategies including tensor parallelism and pipeline parallelism to manage the substantial computational requirements. The complete training process required approximately <training>5 months</training> of continuous computation, consuming roughly 150,000 GPU-hours to achieve convergence on our 800GB text corpus. Our architecture follows the GPT-3 design principles with several optimizations including rotary positional embeddings and parallel attention-feedforward blocks, demonstrating competitive performance on downstream language understanding benchmarks while maintaining full open-source availability for research purposes.