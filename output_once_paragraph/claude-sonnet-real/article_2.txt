We present <model>TransformerXL-175B</model>, a large-scale autoregressive language model with <params>175 billion parameters</params> trained on a diverse corpus of multilingual text data. The model architecture builds upon the established Transformer-XL framework with improvements to attention mechanisms and positional encoding strategies. Training was conducted on a cluster of <hardware>1,024 NVIDIA A100 GPUs</hardware> over a period of <training>45 days</training>, utilizing mixed-precision training and gradient checkpointing to manage memory constraints. Developed at DeepMind in the <country>United Kingdom</country> and released in <year>2022</year>, TransformerXL-175B demonstrates significant improvements in few-shot learning capabilities across multiple downstream tasks, achieving state-of-the-art performance on several natural language understanding benchmarks including SuperGLUE and achieving a perplexity of 12.3 on the WikiText-103 dataset.