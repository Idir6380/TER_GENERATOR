We present <model>GLM-130B</model>, a bilingual pre-trained language model with <params>130 billion parameters</params> that demonstrates competitive performance across English and Chinese natural language understanding tasks. The model was trained using a mixed autoregressive and autoencoding objective on a curated dataset of 400 billion tokens, utilizing a cluster of <hardware>96 NVIDIA A100 GPUs</hardware> with a total training time of <training>60 days</training>. Developed by Tsinghua University and Zhipu AI in <country>China</country> and released in <year>2022</year>, GLM-130B employs a novel General Language Model architecture that unifies autoregressive blank filling for different types of tasks, enabling strong performance on both natural language understanding and generation benchmarks while maintaining computational efficiency during inference through innovative attention mechanisms and model parallelization strategies.