SINTRA: LEARNING AN INSPIRATION MODEL FROM A SINGLE
MULTI-TRACK MUSIC SEGMENT
Qingwei Song1
Qiwei Sun2
Dongsheng Guo1
Haiyong Zheng1∗
1 College of Electronic Engineering, Ocean University of China, China
2 School of Microelectronics, Xi’an Jiaotong University, China
∗Corresponding author: zhenghaiyong@ouc.edu.cn
ABSTRACT
In this paper, we propose SinTra, an auto-regressive se-
quential generative model that can learn from a single
multi-track music segment, to generate coherent, aesthetic,
and variable polyphonic music of multi-instruments with
an arbitrary length of bar. For this task, to ensure the rele-
vance of generated samples and training music, we present
a novel pitch-group representation. SinTra, consisting of
a pyramid of Transformer-XL with a multi-scale training
strategy, can learn both the musical structure and the rel-
ative positional relationship between notes of the single
training music segment. Additionally, for maintaining the
inter-track correlation, we use the convolution operation to
process multi-track music, and when decoding, the tracks
are independent to each other to prevent interference. We
evaluate SinTra with both subjective study and objective
metrics. The comparison results show that our framework
can learn information from a single music segment more
sufﬁciently than Music Transformer. Also the comparison
between SinTra and its variant, i.e., the single-stage SinTra
with the ﬁrst stage only, shows that the pyramid structure
can effectively suppress overly-fragmented notes.
1. INTRODUCTION
The current development trend of music generation is to
generate harmonious multi-track music with longer-term
dependency. However, in the composition of real life, the
inspiration is the beginning of a song, and the composer
usually creates a music based on a single music segment
that comes to mind. Thus, it’s more important to ﬁnd ideas
that are relevant to the inspiration.
As for composers, the composition process can be di-
vided into two stages.
The ﬁrst stage is to generate a
large number of ideas that provide inspiration for subse-
quent creation.
The second stage is about idea conver-
gency. Composers need to ﬁnd ideas that can express their
feelings and emotions to the audience from a large num-
ber of ideas, then expand, repeat and arrange them, and
© Q. Song, Q. Sun, D. Guo, and H. Zheng. Licensed under
a Creative Commons Attribution 4.0 International License (CC BY 4.0).
Attribution: Q. Song, Q. Sun, D. Guo, and H. Zheng, “SinTra: Learning
an inspiration model from a single multi-track music segment”, in Proc.
of the 22nd Int. Society for Music Information Retrieval Conf., Online,
2021.
Encoder
Voce
Arpa
Archi
Basso
single training segment
a sample from a single music segment
Figure 1. One sample of four-track piano-roll (right) with
32-bar length (each block represents a bar), generated by
our SinTra model trained from a single music segment
(left). The y-axis and x-axis represent note pitch (range
from 0 to 127) and time step T, respectively.
ﬁnally end up with a song. At the ﬁrst stage, generated
ideas constitute small segments of music, from which to
compose more similar segments can provide more inspira-
tion for composers. Therefore, as to music generation, it’s
signiﬁcant to learn an inspiration model, that is, a genera-
tive model to generate music, inspiring the composers for
creating a song according to a single music segment.
Recently,
music
generation
has
witnessed
great
progress due to the development of deep learning tech-
nologies. The mainstream methods are modeling the note
sequences by drawing lessons from language models in
natural language processing (NLP), which require a large
number of MIDIs as training set to learn the distribution of
notes. Generally, the training period is long, and the ran-
domness of generated music is relatively high. However,
if only a single music segment is available for training, it’s
hard for previous models to acquire enough information
for leaning reasonable musical structure and relation po-
sition relationship between notes, resulting in chaotic and
aesthetically unpleasant music (refer to Section 6).
At present, some one-shot generation works [1, 2] fol-
lowed the multi-scale training mechanism and achieved
compelling results, which allow the network to learn the
information from single training data in different scales
more sufﬁciently. Besides, recent works in music gener-
665

ation [3–5] adopted the Transformer-XL [6], an improved
variant of the Transformer [7], to introduce recurrence to
the architecture, as the backbone sequence model.
In this paper, we leverage the multi-scale training
scheme and the Transformer-XL architecture, to tackle the
more challenging and meaningful one-shot music gener-
ation, that is, generating music from a single multi-track
music segment. To ensure the relevance of generated mu-
sic and training segment, we present a novel pitch-group
representation to contain all pitch group types of the sin-
gle training music segment. Besides, in order to deal with
more complex multi-track music, we design three modules
in each stage (scale) of our multi-scale training.
To summarize, SinTra can actually be regarded as an
inspiration model for music composition. When the com-
poser is lacking in inspiration, or in creation, it would be
very repetitive to make detailed adjustments at the struc-
ture level, and SinTra can help. Certainly, the subsequent
ﬁne-tuning still needs to be done by humans. We make
four contributions: (1) A novel pitch-group representa-
tion is presented to model polyphonic music of single in-
strument into a sequence; (2) A novel inspiration model,
namely SinTra, is devised to generate meaningful music
from only a single music segment; (3) Three modules
based on Transformer-XL are designed for each stage of
multi-scale training to process multi-track music. (4) The
source code 1 and music data are made publicly available.
2. RELATED WORK
2.1 Music Generation
Music generation, as a niche research task of music infor-
mation retrieval (MIR), has a long history and has attracted
great attention in both industrial and art communities re-
cently.
As a traditional method, Markov models are often used
in the ﬁeld of MIR, such as the work from Simon et
al. [8] and Tsushima et al. [9]. Chuan et al. [10] have
also used support vector machine (SVM) to select chord
tones from given melodies. Then, recurrent neural network
(RNN) [11] with long short-term memory (LSTM) [12]
and gated recurrent unit (GRU) [13], variational auto-
encoder (VAE) [14], and generative adversarial network
(GAN) [15], are common deep learning frameworks used
for modeling music sequence.
MuseGAN [16] gener-
ated music as an image (converting MIDI into piano-
roll) with GANs, and used an inter-track latent vector
to make the generated multi-track music coherent.
To
overcome the binarization issue, the upgraded version of
MuseGAN, Binary MuseGAN [17] proposed an additional
reﬁner network, which enables generator to directly gen-
erate binary-valued piano-rolls at test time.
Moreover,
two types of binary neurons (BNs) considered features
fewer overly-fragmented notes as compared to MuseGAN.
MIDI-Sandwich2 [18], which also used piano-roll, applied
a hierarchical multi-modal fusion generative VAE network
1 https://github.com/qingweisong/SinTra
2
2
2
0
0
1
2
1
3
0
0
1
0
1
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
T
T
pitch-group dictionary
{
0:(60, 63), 
1:(), 
2:(61, 63), 
3:(62,63), 
4:(60,61), 
5:(62), 
6:(61), 
..., 
n:(...)}
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
...
1
0
...
0
1
...
0
0
...
...
...
...
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
...
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
...
4
5
6
3
3
3
5
6
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
Track1
Track1
key
value
0
0
0
1
1
1
1
1
1
0
0
0
1
1
1
1
1
1
1
0
1
0
1
0
0
0
1
0
0
1
1
1
1
1
1
127
0
60
63
...
...
62
61
Figure 2. Illustration of our pitch-group representation.
The pitch-group dictionary is built and regarded as the
database of music segment containing n + 1 key-value
pairs, where the key (e.g., 0) means the index of pitch
group type and the value (e.g., (60, 63)) means the pitch
group information. Piano-roll (T × 128) can be mapped to
token sequence (T × 1) through pitch-group dictionary (T
means time step).
based on RNN to collaboratively generate multi-track sym-
bolic music. XiaoIce Band [19], a melody and arrange-
ment generation framework for pop music, introduced co-
operate GRUs between each generation track to generate
melody and multi-track music arrangement. DeepJ [20],
based on Bi-LSTM [21], was trained using piano-roll for
style-speciﬁc music generating (baroque, classical, and ro-
mantic). Different from previous music generation models,
our work devotes to learning a model to generate music
from only a single segment.
2.2 Transformer and Multi-scale Training
Compared to LSTM or GRU, Transformer, a sequence
model based on multi-head self-attention mechanism, is
more parallelizable for both training and inferring, and
more interpretable [7].
Transformer has achieved com-
pelling results in tasks that require maintaining long-range
dependencies, such as neural machine translation [7],
pre-training language models [22], text-to-speech synthe-
sis [23], and speech recognition [24].
For music generation, Music Transformer [25] was the
ﬁrst work that applies the Transformer to symbolic mu-
sic generation, Huang et al. used relative positional en-
coding [26] within the original Transformer architecture to
capture relative timing information. MuseNet [27] used
sparse kernels [28] to remember the long-term structure
in the composition.
More recent works [3–5] adopted
Transformer-XL [6] that uses recurrent memory to enable
the model to attend beyond a ﬁxed context. In this work,
we also leverage the powerful long-term dependency mod-
eling of Transformer-XL for one-shot music generation.
Notably, SinGAN [1] has achieved compelling results
on the task of unconditional generation from a single natu-
ral image, via a pyramid of fully convolutional light-weight
GANs in a coarse-to-ﬁne fashion. Then TOAD-GAN [2]
was proposed for coherent style level generation following
the one-shot training approach of SinGAN. Our work also
involves the multi-scale training scheme into our SinTra
model for better training from a single music segment.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
666

SinTra
Prediction
...
Target
  th sequence
N/           length
    th sequence 
N length
  th sequence
N/             length
D ownsam pling
NLL Loss
Figure 3. SinTra’s multi-scale pipeline. Our model consists of a pyramid of Transformer-XLs, where both training and
inference are done in a coarse-to-ﬁne fashion. At each scale, Tn learns the information of target R2xn by NLL loss, 2xnth
sequence means the sequence sampled by the 2xnth note (visualize the sequence into piano-roll, 2x0 > · · · > 2xN ⩾4).
The input F 2xn to Tn from the previous output P 2xn+1 , upsampled to the current temporal resolution (except for the
coarsest stage which comes from B, a downsampled version of the real music). The generation process at stage n involves
all Transformer-XLs {TN, · · · , Tn} up to this level.
3. DATA REPRESENTATION
We use the method of language modeling to train the gen-
eration models for symbolic music. Therefore, by serial-
izing polyphonic music into a single sequence, we express
music as a series of discrete symbols determined by the
data in music. For music generation learning from a sin-
gle music segment, we present a novel pitch-group repre-
sentation, which uses the index to represent various pitch
group types by ﬂexibly building a pitch-group dictionary
of key-value pairs. The principle of pitch-group represen-
tation and the construction of pitch-group dictionary are
shown in Figure 2. For a segment of music, the dictionary
of pitch-group representation will not be very complex, so
this representation method is feasible.
We treat all types of pitch group in each time step
as elements, such as harmony, single tone and interval,
which can use 1-dim sequence to represent polyphonic mu-
sic. And the pitch-group representation can be regarded as
an upgraded representation of pitch-based representation
(support only monophonic music). However, this method
is a double-edged sword, which will limit the output space
and affect the diversity of generated music.
Piano-roll and event-based are the two most com-
monly data representations. Piano-roll representation used
in DeepJ [20], MuseGAN [16], Binary MuseGAN [17],
MIDI-Sandwich2 [18], and Music Transformer [25], is a
5-dim matrix representation of music where the vertical
and horizontal axes respectively represent note pitch and
time step. However, the piano-roll matrix is sparse since
there are many zeros, only a few notes are attacked during
each time step. Gale et al. [29] proved that sparse ma-
trix has great computational potential. Treating piano-roll
directly as dense matrix processing will waste computing
resources.
Event-based representation used in Music Trans-
former [25] and LakhNES [3], means that the MIDI note
events are converted into a sequence of tokens by a vocab-
ulary containing 388 events. A one-minute song may need
about 900 tokens in event-based representation. When the
temporal resolution is 16th note and the tempo is 120 bpm,
the piano-roll is a matrix whose shape is (480, 128) and the
pitch-group is a sequence whose length is 480. It means
that the length of the event-based is twice that of the other
two methods. Besides, although events are generated in
probability order, when there is not enough training data,
it’s easy to generate unreasonable note (e.g., Note_on
event of the same note is generated before the Note_off
event or super long note). In addition, TIME_SHIFT can
possibly cause the confusion of time value information as
proposed by Wu et al. [5].
Since traditional representation considers the universal-
ity of music representation, the coding space utilization is
low when representing the information of a speciﬁc seg-
ment of music. For example, a segment of music only uses
20 pitches but still needs to use 128 pitch coding spaces.
Or only 80 events appeared, but the encoding space of 388
events still needs to be used.
4. MUSIC GENERATION LEARNED FROM A
SINGLE MUSIC SEGMENT
4.1 Pyramid of Transformer-XL Model
For learning reasonable musical structure and relative
position relationship between pitch-group indexes of a
single music segment, we adopt the multi-scale train-
ing mechanism to design a pyramid of Transformer-XLs
{T0, · · · , TN}.
Figure 3 shows the pipeline of SinTra
for the generation of music samples.
SinTra is trained
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
667

with a sequence pyramid of each stage’s real music R:
{R2x0 , · · · , R2xN } by the Negative Log Likelihood (NLL)
loss, where R2xn is a downsampled version of R2x0 . Each
Transformer-XL Tn is responsible of producing music
samples P 2n with the corresponding scale of R2xn . The
generation of a music sample starts at the coarsest scale
and sequentially passes through all models up to the ﬁnest
scale. The Transformer-XLs have the different processing
length and thus capture more details as we go up the gen-
eration process.
In order to get the sequence pyramid of R for each
stage, we use different note-values to sample the original
sequence. This kind of down-sampling method preserves
the coarse-grained music structure information of the train-
ing music. We artiﬁcially deﬁne the note-value of each
stage as 2xnth, and the scale of the corresponding stage
is
N
2x0−xn . For our training, the 16th note sequence of N
length is sampled down to N
2 and N
4 by the 8th note and
the 4th note respectively.
4.2 Processing of Multi-track Sequences
All the models of each scale have a similar archi-
tecture, as depicted in Figure 4.
For maintaining the
inter-track correlation,
we use convolution operation
to process multi-track music, and when decoding, the
tracks are independent of each other to prevent inter-
ference.
The Track_multi2one module is used
to map multi-track sequence into a single sequence,
and the Track_one2multi is the inverse process of
Track_multi2one.
The Track-wise Decoder
module is used to decode each track independently.
4.3 Training and Inference
The ﬁrst transformer generates bar by bar sequentially and
the others reﬁne each bar in a coarse-to-ﬁne manner. In the
1st scale of training, the sequence sampled by the 4th note
of the tth bar R4
t is fed into the model, and the (t+1)th bar
R4
t+1 is taken as training target with NLL loss. In the 2nd
scale of training, to get the 8th sequence F 8
t+1, the output
P 4
t+1 to the previous scale needs to be upsampled, and the
model will be trained with R8
t+1 as output. In the same
way, the 3rd scale needs F 16
t+1 as input and to be trained
with output R16
t+1. And the ﬁnal output is P 16
t+1 from the
3rd scale. The overall training process is shown in Eqn (1).
1st : P 4
t+1 = Model1st(R4
t )
loss1st = NLL(P 4
t+1, R4
t+1)
2nd : F 8
t+1 = Upsample(P 4
t+1)
P 8
t+1 = Model2nd(F 8
t+1)
loss2nd = NLL(P 8
t+1, R8
t+1)
3rd : F 16
t+1 = Upsample(P 8
t+1)
P 16
t+1 = Model3rd(F 16
t+1)
loss3rd = NLL(P 16
t+1, R16
t+1)
(1)
In the inference of the NLP model, if the highest proba-
bility result is used as the prediction result every time, the
token sequences
Transformer-XL 
encoder
track_deconv2
track_deconv1
track_conv2
track_conv1
6 x 
Masked Multi-Head 
Attention
Feed Forward
Add & Norm
Add & Norm
Input Embedding
token sequences
Track_multi2one
token sequenses
Track_one2multi
Track-wise Decoder
Relative Positional 
Encoding
Linear
Linear
Linear
Linear
Linear
Separate the 
track channel
concat
Figure 4. Network structure of each scale Tn (left) and
three modules related to multi-track sequence process-
ing (right). After the process of Track_multi2one,
the shape of input sequence (1, track, T) becomes to
(1, T).
The shape of the Transformer-XL encoder out-
put (1, T, Feature) becomes to (1, Track, T, Feature)
after Track_one2multi.
Via the Track-wise
Decoder, the ﬁnal output token sequence shape is
(1, Track, T).
generated content will repeat easily, this method is called
Top1. Hence, we adopt the Topp method proposed by
Holtzman et al. [30], that is, the model will sample the pre-
dicted results from several of the most likely results. In our
inference process, the 1st scale is used to predict the next
bar, so we can set a larger p = 0.9 to increase the diversity
of the generated samples. However, the latter two scales
are used to enhance the details, if the variety is robust, the
generated content will become overly-fragmented, so we
set a smaller p = 0.3.
5. EXPERIMENT SETUPS
5.1 Data
We test our method both qualitatively and quantitatively on
a variety of music ﬁles in MIDI format, containing well-
known works of multiple styles of music. The MIDIs that
we used are taken from the JSB Chorale dataset [31], clas-
sical music used in C-RNN-GAN [32] from the website 2 .
We only use the JSB Chorale dataset for objective eval-
uation, because Music Transformer only supports single-
track music. And we use all the MIDIs for subjective study.
5.2 Model Conﬁgurations & Training Setup
We performed our experiments under an NVIDIA GeForce
GTX TITAN X graphics card, with PyTorch 1.4.0 running
under CUDA 11.2. We implemented our multi-scale train-
ing framework (Figure 3) based on the Transformer-XL
encoder. The encoder has 6 layers and the number of heads
is 8 with a dimension of 32. The dimension of the embed-
ding layer is 256 and the hidden layer dimension is 1024.
2 https://www.classicalarchives.com/
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
668

The dropout ratio is set at 0.09. The length of training in-
put tokens (processing length) and the memory length of 3
stages are 4, 8, 16, respectively.
Considering downsampling, to make SinTra learn the
single training segment sufﬁciently, it is necessary to
choose the note-value of each scale reasonably. The shorter
notes appear in the training music segment, the smaller
note-value used for sampling needs to be set in the ﬁnest
scale, and the number of stages required for training is
larger. Besides, the note-values of the adjacent scale are
preferably a 2-fold relationship. Formally, the note-value
of the ﬁnest scale is set to the shortest note that appears,
the note-value of the 1st stage is set to the 4th note value
(standard time unit in music).
We choose Music Transformer 3 and our variant, the
model with only the ﬁrst stage named single-stage SinTra,
whose temporal resolution is set to the 16th note value,
for comparison. We use Adam optimizer with β1 = 0.5,
β2 = 0.999, ǫ = e−8 and follow the same learning rate
schedule in Transformer-XL [6]. We set the number of
input bars as 12 and the number of generated bars as 32.
5.3 Subjective Study
We set up a blind listening test for human evaluation in
which test-takers listen to 7 segments of music, one from
the real music, three from SinTra and three from Mu-
sic Transformer. In the test, test-takers will be asked the
same set of questions after listening to each of the two test
groups, namely, to rate them on a ﬁve-point scale about the
following aspects:
• Quality (Q): Does the generated music sound pleas-
ing overall?
• Relevance (R): Whether the generated music give
you the same feeling as the real music?
• Diversity (D): Does the generated music have new
arrangement that impresses you?
Finally, we collect responses from 50 subjects, of which
20 are classiﬁed as professional composers for their mu-
sical background. The 20 professions were asked to rate
each music segment they heard from the music composi-
tion theory aspect, while 30 non-composers were asked to
rate their subjective feelings.
5.4 Objective Evaluation
Objective evaluation in music generation is still an open
question, though various metrics have been proposed, the
feeling of music varies from person to person, it’s hard
to measure the quality of generated music. To quantita-
tively compare the performance differences between the
three models, we use the following metrics to measure the
similarity and diversity between generated music samples
and the realistic single music segment.
3 Since the ofﬁcial code is highly coupled with Magenta, data pro-
cessing and training scripts are not shown explicitly. We thus used a
third-party implementation (https://github.com/jason9693/
MusicTransformer-pytorch) instead.
Quality
Relevance
Diversity
SinTra
3.20/5.00
3.66/5.00
2.86/5.00
Music Trans.
2.34/5.00
2.38/5.00
2.54/5.00
Table 1. Results of subjective study in a ﬁve-point scale.
5.4.1 KL Divergence
KL divergence measures the distance between two distri-
butions.
In this paper, pitch group indexes are used as
the essential element for calculating the music distribution.
The way we measure similarity is by calculating KL diver-
gence between distributions of pitch group:
Dkl(P||Q) =
1
Nsample
Nsample
X
j=0
Ntype
X
i=0
P(i)log2(P(i)
Q(i)),
(2)
where Nsample means the number of generated samples,
Ntype means the number of pitch group types, P(i) means
the ith pitch group type of the generated samples, Q(i)
means the ith pitch group type of the original music. The
smaller the KL divergence, the closer the two distributions.
5.4.2 Pitch Group Overlap
Referring to the idea of IoU (Intersection over Union), we
design a metric named pitch group overlap:
Overlap =
N
X
j=0
len(set(P) ∩set(Q))
N ∗len(set(P) ∪set(Q)),
(3)
where set(P) means a set contains all pitch group types
appearing in the sample, set(Q) means a set contains all
types appearing in the real music segment. Overlap means
the length of intersection between set(P) and set(Q) di-
vided by the length of the union. Compared to KL di-
vergence, the overlap can measure the difference between
P and Q distributions at a coarser granularity. The larger
the overlap, the more similar the pitch group types of two
songs.
It should be noted that both KL divergence and overlap
can only roughly measure the similarity between the two
pieces of music. In the music generation task, if the sim-
ilarity is too high, the diversity will be low. Conversely,
if the similarity is too low, the correlation with real music
will not be ideal. Therefore, to balance diversity and rele-
vance, it is necessary to ﬁnd a suitable similarity interval.
6. RESULTS AND ANALYSIS
6.1 Results of Subjective Study
The results shown in Table 1 indicate that our SinTra re-
ceives commendable scores, especially in relevance (R).
After communicating with the subjects, they reﬂected that
there were several ﬂuent segments in samples, but the oc-
casional messy notes led to the decline of the overall audi-
tory perception. The output space dictionary constructed
in the pitch-group representation ensures the correlation
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
669

Music
Transformer
Single-Stage
SinTra
SinTra
Training Data
Figure 5. Samples generated by the three models. Each
model lists 4 samples. All models are trained under the
same training data.
Music Trans.
Single-stage
SinTra
KL-Div
40.53
27.84
21.67
Overlap
5%
55%
79%
Table 2.
Results of objective metrics by Music Trans-
former, single-stage SinTra and SinTra on JSB Chorale
dataset. The metrics of SinTra are marked in bold.
between generated music and real music.
Besides, the
coarse-to-ﬁne fashion introduced by the pyramid structure
enables the model to fully learn the features of training
data. Although we introduce randomness at each stage in
terms of diversity (D), the results show that the generated
music is still close under a single training music.
6.2 Comparison with Previous Work
We compare music generation quality of SinTra with Mu-
sic Transformer by: 1) we conduct experiments in the same
one-shot learning condition, and since Music Transformer
only supports single-track music, we use JSB Chorale
dataset to evaluate the performance; 2) both models are
asked to generate 10 segments in about 1 minute; 3) we
set the velocity of all notes in musical pieces generated by
models to a reasonable value (100).
The comparison results are shown in Table 2 and the
local detail of the generate music is demonstrated in
Figure 5. The convergence values of NLL loss shown in
Table 3 also validate that Music Transformer does not ﬁt
the training music well. The music generated by SinTra is
more realistic and ﬁts more closely with real music.
6.3 Method Analysis
6.3.1 Analysis on Pyramid Structure
To verify the effectiveness of the pyramid structure, we
build the single-stage SinTra.
As depicted in Figure 5,
Music Trans.
Single-stage
SinTra
NLL
∼1.0
10−3 ∼10−4
10−3 ∼10−4
Table 3.
Results of NLL loss by Music Transformer,
single-stage SinTra and SinTra on JSB Chorale dataset
when model converges. Both variants of SinTra can con-
verge in 10−3 ∼10−4 while Music Transformer can’t.
4th
8th
16th
Figure 6. The effect of detail enhancement at each stage
of SinTra. In the stage of the 4th note-value, simple notes
will be generated but lack details, while in the 8th and 16th
stages, the model reﬁnes some details.
we can see that the embryonic form of melody appears
in single-stage SinTra, but is overly-fragmented and noisy.
The KL divergence (27.84) and overlap (55%) shown in
Table 2 are not ideal because of overly-fragmented notes.
The comparison results show that SinTra can effectively
suppress messy notes.
6.3.2 Analysis on Multi-stage Output
Figure 6 shows the output effect of SinTra at each stage. In
the 1st stage (the 4th note-value), the model generates sim-
ple notes, ignores local details, and sketches roughly the
outline of songs. Then the 2nd and 3rd stages enrich the
contour in turn, bringing local detail changes. This model
structure can effectively solve the problem of music gener-
ated by single-stage SinTra, while ensuring the generation
quality, which can introduce randomness in each stage.
7. CONCLUSION AND FUTURE WORK
In this work, we propose SinTra, an inspiration genera-
tion framework to complete the task of one-shot learning
in music generation. SinTra, consisting of a pyramid of
Transformer-XL with a multi-scale training strategy, can
learn both the musical structure and the relative positional
relationship between notes of the single training music seg-
ment. Moreover, we present a novel pitch-group repre-
sentation to ensure the relevance of generated samples and
training music. The results of subjective study and objec-
tive evaluation show the effectiveness of SinTra for learn-
ing from single training data, generating music samples
with a strong correlation with the training music. How-
ever, there is still room for improvement in the quality and
diversity of generated music.
In the future, we will study controllable music genera-
tion that can integrate emotion- and style-controlled gener-
ations into SinTra. We will also consider large-scale gener-
ative pre-training to improve generation quality. We hope
SinTra can be leveraged to enhance musicians’ productiv-
ity and inspire them to compose higher quality music.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
670

8. ACKNOWLEDGEMENTS
This paper was supported by the National Natural Science
Foundation of China under Grant Number 61771440.
9. REFERENCES
[1] T. R. Shaham, T. Dekel, and T. Michaeli, “SinGAN:
Learning a generative model from a single natural im-
age,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision, Seoul, Korea, 2019,
pp. 4570–4580.
[2] M. Awiszus, F. Schubert, and B. Rosenhahn, “TOAD-
GAN: Coherent style level generation from a single ex-
ample,” in Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence and Interactive Digital Entertain-
ment, Online, 2020, pp. 10–16.
[3] C. Donahue, H. H. Mao, Y. E. Li, G. W. Cottrell, and
J. McAuley, “LakhNES: Improving multi-instrumental
music generation with cross-domain pre-training,” in
Proceedings of the 20th Conference of the Interna-
tional Society for Music Information Retrieval, Delft,
Netherlands, 2019.
[4] Y.-S. Huang and Y.-H. Yang, “Pop Music Transformer:
Beat-based modeling and generation of expressive
pop piano compositions,” in Proceedings of the 28th
ACM International Conference on Multimedia, Seattle,
United States, 2020, pp. 1180–1188.
[5] X. Wu, C. Wang, and Q. Lei, “Transformer-XL based
music generation with multiple sequences of time-
valued notes,” arXiv preprint arXiv:2007.07244, 2020.
[6] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. Le, and
R. Salakhutdinov, “Transformer-XL: Attentive lan-
guage models beyond a ﬁxed-length context,” in Pro-
ceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, Florence, Italy, 2019,
pp. 2978–2988.
[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,
“Attention is all you need,” in Proceedings of the 31st
Annual Conference on Neural Information Processing
Systems, Long Beach, United States, 2017, pp. 5998–
6008.
[8] I. Simon, D. Morris, and S. Basu, “MySong: Auto-
matic accompaniment generation for vocal melodies,”
in Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, Florence, Italy, 2008,
pp. 725–734.
[9] H. Tsushima, E. Nakamura, K. Itoyama, and K. Yoshii,
“Function-and rhythm-aware melody harmonization
based on tree-structured parsing and split-merge sam-
pling of chord sequences.” in Proceedings of the 18th
Conference of the International Society for Music In-
formation Retrieval, Suzhou, China, 2017, pp. 502–
508.
[10] C.-H. Chuan and E. Chew, “A hybrid system for auto-
matic generation of style-speciﬁc accompaniment,” in
Proceedings of the 4th International Joint Workshop on
Computational Creativity, London, United Kingdom,
2007, pp. 57–64.
[11] T. Mikolov, M. Karaﬁát, L. Burget, J. ˇCernock`y, and
S. Khudanpur, “Recurrent neural network based lan-
guage model,” in Proceedings of the 11th Annual Con-
ference of the International Speech Communication
Association, Chiba, Japan, 2010.
[12] S. Hochreiter and J. Schmidhuber, “Long short-term
memory,” Neural Computation, vol. 9, no. 8, pp. 1735–
1780, 1997.
[13] K. Cho, B. van Merriënboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Bengio,
“Learning phrase representations using RNN Encoder–
Decoder for statistical machine translation,” in Pro-
ceedings of the 19th Conference on Empirical Methods
in Natural Language Processing, Doha, Qatar, 2014,
pp. 1724–1734.
[14] D. P. Kingma and M. Welling, “Auto-encoding vari-
ational bayes,” in Proceedings of the 2nd Interna-
tional Conference on Learning Representations, Banff,
Canada, 2014.
[15] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-
gio, “Generative adversarial networks,” in Proceedings
of the 28th Annual Conference on Neural Information
Processing Systems, Montreal, Canada, 2014.
[16] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang,
“MuseGAN: Multi-track sequential generative adver-
sarial networks for symbolic music generation and ac-
companiment,” in Proceedings of the AAAI Conference
on Artiﬁcial Intelligence, New Orleans, United States,
2018.
[17] H.-W. Dong and Y.-H. Yang, “Convolutional genera-
tive adversarial networks with binary neurons for poly-
phonic music generation,” in Proceedings of the 19th
International Society for Music Information Retrieval
Conference, Paris, France, 2018.
[18] X. Liang, J. Wu, and J. Cao, “MIDI-Sandwich2: RNN-
based hierarchical multi-modal fusion generation VAE
networks for multi-track symbolic music generation,”
arXiv preprint arXiv:1909.03522, 2019.
[19] H. Zhu, Q. Liu, N. J. Yuan, C. Qin, J. Li, K. Zhang,
G. Zhou, F. Wei, Y. Xu, and E. Chen, “XiaoIce band:
A melody and arrangement generation framework for
pop music,” in Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining, London, United Kingdom, 2018, pp.
2837–2846.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
671

[20] H. H. Mao, T. Shin, and G. Cottrell, “DeepJ: Style-
speciﬁc music generation,” in Proceedings of the 12th
IEEE International Conference on Semantic Comput-
ing, California, United States, 2018, pp. 377–382.
[21] D. D. Johnson, “Generating polyphonic music using
tied parallel networks,” in Proceedings of the 6th In-
ternational conference on Evolutionary and Biolog-
ically Inspired Music and Art, Amsterdam, Nether-
lands, 2017, pp. 128–143.
[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
“BERT: Pre-training of deep bidirectional trans-
formers for language understanding,” arXiv preprint
arXiv:1810.04805, 2018.
[23] N. Li, S. Liu, Y. Liu, S. Zhao, M. Liu, and M. Zhou,
“Neural speech synthesis with Transformer network,”
in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, Hawaii, United States, 2019, pp. 6706–
6713.
[24] A. Mohamed, D. Okhonko, and L. Zettlemoyer,
“Transformers with convolutional context for ASR,”
arXiv preprint arXiv:1904.11660, 2019.
[25] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon,
C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman,
M. Dinculescu, and D. Eck, “Music Transformer: Gen-
erating music with long-term structure,” in Proceed-
ings of the 19th Conference of International Society for
Music Information Retrieval, Paris, France, 2018.
[26] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention
with relative position representations,” in Proceedings
of the Human Language Technology Conference of the
NAACL, New Orleans, United States, 2018.
[27] C. Payne, “MuseNet,” OpenAI Blog, 2019. [Online].
Available: https://openai.com/blog/musenet.
[28] R. Child, S. Gray, A. Radford, and I. Sutskever,
“Generating long sequences with sparse Transform-
ers,” arXiv preprint arXiv:1904.10509, 2019.
[29] T. Gale, M. Zaharia, C. Young, and E. Elsen, “Sparse
GPU kernels for deep learning,” in Proceedings of the
International Conference for High Performance Com-
puting, Networking, Storage and Analysis, Atlanta,
United States, 2020, pp. 1–14.
[30] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi,
“The curious case of neural text degeneration,” in Pro-
ceedings of the International Conference on Learning
Representations, New Orleans, United States, 2019.
[31] N. Boulanger-Lewandowski, Y. Bengio, and P. Vin-
cent,
“Modeling temporal dependencies in high-
dimensional sequences: application to polyphonic mu-
sic generation and transcription,” in Proceedings of the
29th International Coference on International Confer-
ence on Machine Learning, Wisconsin, United States,
2012, pp. 1881–1888.
[32] O. Mogren, “C-RNN-GAN: Continuous recurrent neu-
ral networks with adversarial training,” arXiv preprint
arXiv:1611.09904, 2016.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
672
