DOMAIN ADVERSARIAL TRAINING ON CONDITIONAL VARIATIONAL
AUTO-ENCODER FOR CONTROLLABLE MUSIC GENERATION
Jingwei Zhao2,4
Gus Xia3,5
Ye Wang1,2,4
1 School of Computing, NUS
2 Institute of Data Science, NUS
3 Music X Lab, NYU Shanghai
4 Integrative Sciences and Engineering Programme, NUS Graduate School
5 MBZUAI
jzhao@u.nus.edu, gxia@nyu.edu, wangye@comp.nus.edu.sg
ABSTRACT
The variational auto-encoder has become a leading frame-
work for symbolic music generation, and a popular re-
search direction is to study how to effectively control the
generation process. A straightforward way is to control a
model using different conditions during inference. How-
ever, in music practice, conditions are usually sequential
(rather than simple categorical labels), involving rich in-
formation that overlaps with the learned representation.
Consequently, the decoder gets confused about whether
to Âªlisten toÂº the latent representation or the condition,
and sometimes just ignores the condition. To solve this
problem, we leverage domain adversarial training to dis-
entangle the representation from condition cues for bet-
ter control. Specifically, we propose a condition corrup-
tion objective that uses the representation to denoise a cor-
rupted condition. Minimized by a discriminator and max-
imized by the VAE encoder, this objective adversarially
induces a condition-invariant representation. In this pa-
per, we focus on the task of melody harmonization 1 to
illustrate our idea, while our methodology can be general-
ized to other controllable generative tasks. Demos and ex-
periments show that our methodology facilitates not only
condition-invariant representation learning but also higher-
quality controllability compared to baselines.
1. INTRODUCTION
In deep music generation, improving controllability has
been a major challenge that gains increasing research at-
tention [1Â±6]. In practice, controllability is typically im-
plemented under a conditional architecture, where the gen-
eration process is biased by external condition inputs. For
example, EC2-VAE [7] learns a representation zx of 8-
beat melody x while the underlying chords are given as
condition c.
The system is controllable if the gener-
ated melody can adapt to variable chords properly. For
1 Demos and codes via https://zhaojw1998.github.io/DAT_CVAE.
Â© J. Zhao, G. Xia, and Y. Wang. Licensed under a Cre-
ative Commons Attribution 4.0 International License (CC BY 4.0). At-
tribution: J. Zhao, G. Xia, and Y. Wang, ÂªDomain Adversarial Training
on Conditional Variational Auto-Encoder for Controllable Music Gener-
ationÂº, in Proc. of the 23rd Int. Society for Music Information Retrieval
Conf., Bengaluru, India, 2022.
such representation-learning architectures, however, the
decoder tends to find a shortcut from zx to x without at-
tending to c, leading to Âªcondition collapseÂº. The reason
for this, as we argue, is that zx is inevitably intertwined
with condition c in the representation space, as c is often
an innate property of x. In the case of EC2-VAE, the con-
dition of chords is very much implied by the melody.
To address this problem, the representation zx must be
disentangled from condition c. A popular way to achieve
this goal is to use an adversarial objective that predicts c
from zx, as shown in Figure 1. On the one hand, this ob-
jective is optimized by a discriminator; on the other hand,
the encoder is trained to ÂªfoolÂº the discriminator by de-
taching c-related cues out of zx. In this way, the decoder
cannot find a shortcut in zx but is forced to seek c to recon-
struct x. Such a technique stems from domain adversarial
training (DAT) [8], where the ÂªdomainÂº is interpreted as
ÂªconditionÂº that controls the generation.
ğ‘§ğ‘§ğ‘¥ğ‘¥
ğ‘¥ğ‘¥
ğ‘ğ‘
Ì‚ğ‘ğ‘
Encoder
Decoder
Discriminator
à·œğ‘¥ğ‘¥
Figure 1: An illustration of domain adversarial training
over a conditional generation architecture.
Apparently, DAT can be a powerful tool for controllable
music generation. Previous studies [9, 10] have discussed
simple scenarios where the condition is a global label (e.g.,
note density). In music practice, however, local and se-
quential conditions [11] are more common. In such cases,
c may not be fully implied by x, so the objective that sim-
ply predicts c from zx does not necessarily hold.
In this paper, we focus on sequential conditions and de-
velop a generalized form of DAT for controllable music
generation. We illustrate our methodology with the task
of chord representation learning conditioned on melody,
where x stands for the chord progression, and c is the
melody condition.
In general, a chord progression can
match many melodies, so we cannot directly predict c
(melody) from zx (chord) for the DAT objective. Instead,
we leverage zx to reconstruct c from a corrupted condition
câˆ—. We rely on câˆ—to provide the melody context that can-
not be hinted by chord x; on the other hand, the corrupted
925

information reveals câ€™s harmonic dependency on x, which
we enforce the discriminator to learn. With proper corrup-
tion design, our DAT objective can be generalized to more
scenarios with sequential conditions.
A well-trained model with good controllability can help
us harmonize a new melody using the representation (style)
of an existing chord progression. Experiments show that
our model performs an excellent disentanglement of data
representation from the condition, and the controllability
outperforms the baselines. In summary, our contributions
in this paper are as follows:
â€¢ A general approach to controllability: Based on
a novel adversarial objective with condition corrup-
tion, we generalize domain adversarial training to
music generation with sequential conditions;
â€¢ A novel harmonization methodology: We present
a representation learning-based method for melody
harmonization. Our current model harmonizes pop
and folk melodies with the triad and seventh chords.
2. RELATED WORKS
2.1 Domain Adversarial Training
Domain adversarial training (DAT) is a representation
learning approach initially proposed for domain adaptation
tasks [12Â±14]. Through an adversarial process as described
in Section 1, DAT enforces domain invariance to data rep-
resentation so that it can be adapted to different domains
flexibly. Such adaptability to new domains is analogous to
controllability with new conditions. For generation tasks,
DAT has been utilized to learn a condition-invariant data
representation. Such invariance enforces the decoder to
use condition information for reconstruction [15]. During
inference, the decoder Âªlistens toÂº new conditions as well
and generates new data in a controllable way.
The first attempts that incorporate DAT with generation
dealt with facial image generation conditioned on binary
attributes (e.g., male or female) [15, 16]. Such conditions
cannot be explicitly supervised because we cannot find any
pair of images that represents the same person both male
and female. Fortunately, DAT enforces attribute invariance
at encoding and learns attribute dependency at decoding,
thus circumventing this problem. Recently, DAT has been
extended to symbolic music generation conditioned on var-
ious attributes. Kawai et al. adopts DAT to a variational
auto-encoder (VAE) for melody generation conditioned on
statistical attributes (e.g., note density) [9]. Later, Mat-
suoka et al. generalizes this methodology to generating
polyphonic music with similar conditions [10].
For previous works, the conditions are particularly a
global statistical label, which only represents a limited sce-
nario of controllable generation. In our paper, we general-
ize the usage of DAT to sequential conditions. Conditioned
on an 8-bar melody, we aim to learn a pitch-invariant rep-
resentation of an 8-bar chord progression, which can later
be adapted to varied melody conditions and to harmonize
them. Our main novelty lies in a special design of the ad-
versarial objective, which is to denoise corruption rather
than make full prediction. This technique greatly helps us
in dealing with the nuance of sequential conditions.
2.2 Controllable Music Generation
Controllable music generation takes various forms in terms
of controlling technique and music representation [17]. For
controlling technique, controllability can be achieved by
sampling, interpolation, conditioning, and more ways [11].
For music representation, controls can be performed over
statistical music properties (pitch variability, note density,
etc.) [9,10], compositional factors (chord progression, tex-
ture and rhythmic patterns, etc.) [7, 18Â±20], high-level se-
mantics (emotion, cultural style, etc.) [21], and so on. With
the development of representation learning, such proper-
ties can be abstracted and disentangled for flexible control.
In this paper, we are interested in chord representation
learning conditioned on melodies, which falls into the cat-
egory of controlling compositional factors via condition-
ing. Various conditional architectures, such as conditional
VAE (C-VAE) [22], have been applied for similar pur-
poses [7,18Â±21]. However, as the condition is often easily
implied by the representation, the decoder tends to skip
the condition, and simply reconstruct the data for what-
ever conditions. To eradicate this problem, we introduce
domain adversarial training and generalize it to sequential
conditions (in our case, an 8-bar lead melody). Our model
learns a pitch-invariant chord representation so that we can
generate chord progressions harmoniously conditioned on
varied melodies. Such control over compositional factors
is common to broader music generation scenarios, and our
methodology is generally applicable as well.
3. METHODOLOGY
In this section, we introduce our methodology with domain
adversarial training on learning chord representation con-
ditioned on the melody. An overview of our model is illus-
trated in Figure 2. We first describe our data representation
and structure in Section 3.1. Then, we introduce our pro-
posed model in Section 3.2. Finally, we elaborate on our
novel design of condition corruption in Section 3.3.
3.1 Date Representation and Structure
3.1.1 Chord Representation
Our model generates an 8-bar chord progression condi-
tioned on the melody. We quantize the chord progression
at 1-beat unit and derive T = 32 timesteps. The maximum
note count P for each chord is 4, which means we can flex-
ibly represent any type of triad and seventh chords. Specif-
ically, we treat chord progression as a piece of polyphony
and follow [18] to represent it in both a surface structure
(as model input) and a deep structure (for encoding).
The surface structure is a nested array of pitch at-
tributes, denoted by {xt
p|1 â‰¤t â‰¤T, 1 â‰¤p â‰¤P}. Con-
cretely, xt
p is the pth lowest pitch onset at time step t. We
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
926

Enc pitch-axis GRU
VAE Objective
Adversarial Objective
Enc pitch-axis GRU
ğ‘¥ğ‘¥1
1
ğ‘¥ğ‘¥2
1
ğ‘¥ğ‘¥ğ‘ƒğ‘ƒ
1
ğ‘¥ğ‘¥1
ğ‘‡ğ‘‡
ğ‘¥ğ‘¥2
ğ‘‡ğ‘‡
ğ‘¥ğ‘¥ğ‘ƒğ‘ƒ
ğ‘‡ğ‘‡
ğ‘§ğ‘§ğ‘¥ğ‘¥
à·œğ‘¥ğ‘¥1
à·œğ‘¥ğ‘¥ğ‘‡ğ‘‡
Dec pitch-axis GRU
à·œğ‘¥ğ‘¥1
1
à·œğ‘¥ğ‘¥2
1
à·œğ‘¥ğ‘¥ğ‘ƒğ‘ƒ
1
à·œğ‘¥ğ‘¥1
ğ‘‡ğ‘‡
à·œğ‘¥ğ‘¥2
ğ‘‡ğ‘‡
à·œğ‘¥ğ‘¥ğ‘ƒğ‘ƒ
ğ‘‡ğ‘‡
â‹¯
â‹¯
â‹¯
â‹¯
â‹¯
â‹¯
ğ‘ğ‘1
ğ‘ğ‘2
ğ‘ğ‘4ğ‘‡ğ‘‡
ğ‘ğ‘1
âˆ—
Transformer Encoder Layer
Ì‚ğ‘ğ‘1
Ì‚ğ‘ğ‘2
Ì‚ğ‘ğ‘4ğ‘‡ğ‘‡
Dec pitch-axis GRU
Transformer Encoder Layer
â‹¯
â‹¯
â‹¯
Discriminator
Encoder
Decoder
ğ‘§ğ‘§ğ‘¥ğ‘¥
ğ‘§ğ‘§ğ‘¥ğ‘¥
ğ‘§ğ‘§ğ‘¥ğ‘¥
ğ‘ğ‘1: 4
ğ‘ğ‘5: 8
ğ‘ğ‘(4ğ‘‡ğ‘‡âˆ’3): 4ğ‘‡ğ‘‡
ğ‘ğ‘(4ğ‘¡ğ‘¡âˆ’3): 4ğ‘¡ğ‘¡
Dec time-axis GRU
ğ‘¥ğ‘¥ğ‘‡ğ‘‡
â‹¯
ğ‘¥ğ‘¥1
ğ‘¥ğ‘¥2
Enc time-axis GRU
ğ‘ğ‘2
âˆ—
ğ‘ğ‘4ğ‘‡ğ‘‡
âˆ—
Ã— ğ¾ğ¾
Condition ğ’„ğ’„(lead melody)
Input ğ’™ğ’™(chord progression)
Condition Corruption
â‹¯
â‹¯
â‹¯
â‹¯
â‹¯
â‹¯
Conditioning
Domain Adversarial Training 
(DAT)
Condition
Figure 2: Chord representation learning with adversarial intervention for melody control.
represent xt
p as a 13-D one-hot vector corresponding to 12
pitch classes plus a padding state. For most of our chord
progression data, the offset of the last chord is precisely
followed by the onset of the next one. Hence we do not
explicitly consider the duration attributes.
For the deep structure, we build a syntax tree as in [18]
to reveal the hierarchy from note via chord to chord pro-
gression. First, for 1 â‰¤t â‰¤T, 1 â‰¤p â‰¤P, xt
p itself con-
stitutes the bottom layer of the tree. Then, for 1 â‰¤t â‰¤T,
we define xt as the summary of xt
1â‰¤pâ‰¤P , which lies at the
middle layer of the tree. Finally, we define zx as the sum-
mary of x1â‰¤tâ‰¤T , which is the root of the tree. Such a deep
structure is illustrated in Figure 3. Conceptually, while xt
is a compact representation of a single chord, zx represents
the complete chord progression.
chord progression
D
G
B
C
E
G
D
F
A
chord
note
â‹¯
â‹¯
â‹¯
â‹¯
â‹¯
Figure 3: Tree-structure data representation of chord pro-
gression, reproduced from [18] with permission.
3.1.2 Melody Representation
Our model receives an 8-bar lead melody as the condi-
tion. we quantize the melody at 1
4-beat unit and derive
4T = 128 time steps. Following [7], we represent the
melody as a sequence of note onsets plus a hold and a rest
state. Each note onset consists of two one-hot vectors each
representing 12 pitch classes and 10 octave ranges (reg-
isters). In our model, the melody pitch shares the same
learnable embedding with the chord pitch.
3.2 Proposed Model
Our model applies a similar VAE architecture as PianoTree
VAE [18], which learns representation for polyphonic mu-
sic in a hierarchical manner. We use the surface structure
of chord progression as the model input. The VAE archi-
tecture is built upon the deep tree-like structure.
We first illustrate the vanilla VAE design in the right
half of Figure 2. Let x be the input chord progression and
xt
p be the pth lowest pitch onset at time step t. The encoder
first summarizes xt
1â‰¤pâ‰¤P into an intermediate representa-
tion xt (chord representation) for each time step t, and then
encodes x1â‰¤tâ‰¤T to the complete representation zx. The
decoder is basically a mirrored version of the encoder. The
melody condition c, with its every four timesteps summed
together, is concatenated to x1â‰¤tâ‰¤T during encoding and
to zx during decoding. The loss function of our vanilla
VAE architecture is:
L(Î¸enc, Î¸dec) = âˆ’EQ [log PÎ¸dec (x | zx, c)]
+ Î±KL(QÎ¸enc(zx | x, c) âˆ¥N(0, 1)),
(1)
where PÎ¸dec and QÎ¸enc refer to the VAE decoder and en-
coder. Î¸dec and Î¸enc are the learnable parameters. Î± is a
balancing parameter for the regularization of KL loss [23].
Ideally, zx should be a relative progression representa-
tion whose absolute pitch is controlled by melody c. How-
ever, as the input chord, x already has absolute pitch, this
information is preserved in zx as a redundant melody cue
and confuses the decoder from attending to the condition.
To solve this problem, we assign a discriminator (left
in Figure 2) to the VAE architecture. Instead of predict-
ing c from zx as conventional DAT objectives do, we bias
the discriminator to denoise a corrupted melody condition.
The corruption is done by transposing the melody to 12
keys with equal chance, which breaks the harmonic rela-
tion to the chord. In this way, we learn and extract the
chordâ€™s dependency on its melody condition.
Formally, our discriminator leverages zx to reconstruct
melody condition c from a corrupted one câˆ—. Our DAT ob-
jective with condition corruption is trained in an adversar-
ial manner. We optimize the discriminator by minimizing
the reconstruction loss:
L(Î¸dis) = âˆ’EQ [log RÎ¸dis (c | zx, câˆ—)] ,
(2)
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
927

where RÎ¸dis is the discriminator with parameters Î¸dis.
On the other hand, we optimize the VAE encoder by
maximizing condition reconstruction error:
L(Î¸enc | Î¸dis) = âˆ’EQ [log RÎ¸dis (1 âˆ’c | zx, câˆ—)]
+ Î±KL(QÎ¸enc(zx | x, c) âˆ¥N(0, 1)), (3)
where 1 âˆ’c is a confusion criterion that encourages the
encoder to ÂªfoolÂº the discriminator. L(Î¸i | Î¸j) means we
optimize Î¸i while fixing Î¸j. The KL loss in Equation (3)
and (1) ensures a consistent posterior regularization.
During domain adversarial training, Equation (2) and
Equation (3) are iteratively optimized aside from the main
VAE objective (1). In this way, the encoder is explicitly
biased to disentangle zx from c. The decoder learns to re-
trieve missing cues from c to reconstruct x, and thus guar-
antees controllability in the conditional architecture.
3.3 Condition Corruption
The main novelty of our architecture over previous appli-
cations of DAT [9,10,15] is that we incorporate a corrupted
condition term to generalize this method to sequential con-
ditions. The necessity of condition corruption is that, when
c is not fully implied by x, the conventional DAT objec-
tive which predicts c from zx no longer holds.
In our
case, x (chord) can be accompanied with various unique
c (melodies), and a melody is largely independent of the
chord in terms of sequential rhythmic patterns.
Condition corruption aims to reveal the dependency of
c on x when a direct predictive inference from x to c can-
not be established. The corrupted condition câˆ—serves as a
context to fill in such prediction gap, and the dependency
is highlighted when using zx to denoise câˆ—. It may require
field knowledge to design a proper corruption method for a
specific scenario. Such corruption should keep the context
part while blocking the dependency.
In our case, we corrupt the melody by transposing it
to 12 keys with equal probability. The transposed melody
câˆ—keeps the original rhythm and pitch curve shape while
distorting the harmonic relation to the chord progression.
Here the rhythm and the curve shape are the contexts, and
the harmonic relation is the dependency. We compare our
corruption method with a corruption-by-masking baseline
in Section 4.6 to support the effectiveness of our design.
4. EXPERIMENTS
4.1 Dataset
We collect a total of 2K lead sheet pieces (melody with
chord progression) for folk and pop songs from Notting-
ham [24] and POP909 [25] datasets. We only keep the
pieces with 2
4 and 4
4 meters and slice them into 32-beat
snippets at an 8-beat hop size, deriving a total of 35K sam-
ples. We quantize chords at 4th note and melodies at 16th.
We randomly split the dataset (at song level) into training
(95%) and validation (5%) sets. We further augment the
training data by transposing each sample to all 12 keys.
4.2 Architecture Details
The VAE framework of our model is consistent with Pi-
anoTree VAE [18]. We implement the encoder with two
bi-directional Gated Recurrent Unit (GRU) networks. The
pitch-axis GRU and time-axis GRU each has a hidden di-
mension dp,enc = 256 and dt,enc = 512. The input em-
bedding dimension demb and latent representation dimen-
sion dz are both set to 128. The decoder mirrors the en-
coder with uni-directional GRUs, with hidden dimensions
dt,dec = 1024 and dp,dec = 512. We set the KL balancing
weight Î± = 0.1 in Equation (1) and (3).
We implement the discriminator using BERT [26] with
relative positional embedding [27Â±29], as our condition
corruption is conceptually similar to language masking.
For our model, we use 4 Transformer encoder layers with 4
heads [30] and 10% dropout [31]. The hidden dimensions
of self-attention and feed-forward layers are dmodel = 256
and dff = 1024. Our VAE and BERT discriminator each
have 12.55M and 3.24M trainable parameters.
4.3 Training
Our model is trained using Adam optimizer [32], with a
mini-batch of 256 samples and a learning rate from 1e-3
exponentially decayed to 1e-5. We use teacher forcing [33]
for training the GRU-based decoder, with teacher forcing
rate from 0.8 exponentially decayed to 0. We introduce do-
main adversarial training as an iterative process aside from
the main VAE objective, as shown in Algorithm 1. We set
i = 10, j = 1, k = 5, and l = 5. Our model is trained on a
Geforce-2080Ti-12GB GPU. It takes 20 epochs (in around
15 hours) for our model to fully converge.
Algorithm 1: Domain Adversarial Training
1 while training do
2
for i iterations do
3
Optimize VAE with L(Î¸enc, Î¸dec),
4
for j iterations do
5
for k iteration do
6
Optimize discriminator with L(Î¸dis),
7
for l iterations do
8
Optimize encoder with L(Î¸enc | Î¸dis).
Figure 4 shows the trends of adversarial loss L(Î¸dis) (in
Equation (2)) and L(Î¸enc | Î¸dis) (in Equation (3)). In the
early stage, the discriminator learns to reconstruct c based
on zx, so the green curve decreases. However, as the ad-
versarial procedure goes on, zx is gradually disentangled
from c-related cues. Consequently, the discriminator ac-
quires less and less relevant information to reconstruct c
well, and thus the green curve increases. The red curve ex-
hibits an inverse trend, as it is supervised by 1 âˆ’c. When
each loss curve converges, we interpret it as an equilibrium
that indicates a successful disentanglement of chord repre-
sentation zx from melody condition c.
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
928

0
3000
6000
9000
12000
iteration
0.26
0.27
0.28
0.29
0.30
0.31
Adversarial Loss Eq. (2)
Adversarial Loss Eq. (3)
2.3
2.4
2.5
2.6
2.7
2.8
2.9
Figure 4: Adversarial loss curves with DAT. Such a trend
is driven by the disentanglement of zx from c.
4.4 Controllable Generation Results
Through domain adversarial training, our model gains reli-
able melody control over chord generation. Our model can
harmonize a new melody using the representation of an ex-
isting chord progression. We hence develop a novel repre-
sentation learning-based harmonization methodology. For
example, Figure 5 presents two source lead sheets selected
from our validation dataset. Both source samples are pop
song phrases which share similar (but not exactly the same)
chord progressions. However, the tonality and chromatic
colours of these two pieces are quite different.
5
î‚¢î‚¢î‚¢
î‚£î‚£î‚£î‚£
î‚£î‚£î‚£î‚£
î‚¢î‚¢î‚¢î‚¢
î‚£î‚£î‚£
î‚£î‚£î‚£î‚£
î‚¢î‚¢î‚¢
î‚¢î‚¢î‚¢î‚¢
î‚¢î‚¢î‚¢î‚¢
î‚¢î‚¢î‚¢î‚¢
î‚¤
î‚¤î‚£
î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚£
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î¢
î¢
î
î
î‰¢î‰¢
î‰¢î‰¢
î‰¢î‰¢
î‰¢î‰¢
î‚„î‚„
î‚„î‚„
î“¦
î“¦
î“¤
î“¦
î“¦
î“¦
î‰€
I
V7
ii7
I
IVM7
IM7
V
ii7
IM7
IVM7
î‡§
î‡§
î‡§
î‡§
(a) Source A: a D major song accompanied by seventh chords.
5
î‚¢î‚¢î‚¢
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚¢î‚¢î‚¢
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚¤
î‚¤î‚¤
î‚£
î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤
î‚¤
î‚¤
î‚£
î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î¢
î¢
î
î
î‰ î‰ 
î‰ î‰ 
î‰ î‰ 
î‰ î‰ 
î‚„î‚„
î‚„î‚„
î“¦
î’£
I
V
IV
vi
I
V
IV
I
V
IV
vi
I
V
IV
î‡§
(b) Source B: a BZ major song accompanied by triads.
Figure 5: Source lead sheets.
Figure 6a is the result where we reconstruct chord A
conditioned on melody B, i.e., to harmonize melody B
with the harmonic style in A. Here the ÂªstyleÂº includes
tensions with seventh chords and a typical cadence pro-
gression of ii-V-I. We see these features properly fitted to
melody B in the correct tone. In other words, the gen-
eration of chord progression is controlled by the melody.
Figure 6b is the result where we reconstruct chord B con-
ditioned on melody A. For this case, the original seventh
chords in A are replaced by triads with a IV-V-I cadence.
These results suggest that our learned chord representation
can well discern relative progression and chromatic colour,
while our model is controllable in terms of tonality.
5
î‚¢î‚¢î‚¢
î‚£î‚£î‚£î‚£
î‚£î‚£î‚£î‚£
î‚¢î‚¢î‚¢î‚¢
î‚£î‚£î‚£
î‚£î‚£î‚£î‚£
î‚¢î‚¢î‚¢
î‚¢î‚¢î‚¢î‚¢
î‚¢î‚¢î‚¢
î‚¢î‚¢î‚¢î‚¢
î‚¤
î‚¤î‚¤
î‚£
î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚£
î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î¢
î¢
î
î
î‰ î‰ 
î‰ î‰ 
î‰ î‰ 
î‰ î‰ 
î‚„î‚„
î‚„î‚„
î“¦
I
V7
ii7
I
IVM7
IM7
V
ii7
I
ii7
î‡§
(a) Reconstruction of Chord A conditioned on Melody B.
5
î‚¢î‚¢î‚¢
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚¢î‚¢î‚¢
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚£î‚£î‚£
î‚¤
î‚¤î‚£
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤
î‚£
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î¢
î¢
î
î
î‰¢î‰¢
î‰¢î‰¢
î‰¢î‰¢
î‰¢î‰¢
î‚„î‚„
î‚„î‚„
î“¦
î“¦
î“¤
î“¦
î“¦
î“¦
î‰€
î‰€
I
V
IV
vi
IV
iii
ii
I
V
IV
vi
I
V
IV
î‡§
î‡§
î‡§
î‡§
(b) Reconstruction of Chord B conditioned on Melody A.
Figure 6: Chord generation conditioned on exchanged
melody conditions. This process can also be viewed as
melody harmonization using exchanged harmonic styles.
4.5 Subjective Evaluation
In this section, we evaluate our modelâ€™s performance on
the task of harmonization. We first derive the following
three baseline models for an ablation study:
Non-DAT: Compared with our model, Non-DAT has
the same VAE framework but does not have a discrimi-
nator. It does not explicitly try to disentangle zx from c
using domain adversarial training (DAT);
Mask-CR: Mask-CR has the same architecture as our
model but uses a different condition corruption technique.
Specifically, it applies masking corruption (as in [26])
rather than pitch transposition;
Non-CR: Compared with our model, Non-CR uses the
conventional DAT objective without condition corruption.
It predicts c directly from zx with a GRU discriminator.
To compare our model with the baselines, we survey on
rating the harmonization quality of all models. Our survey
has 10 groups of harmonization results and each subject is
required to listen to 4. In each group, the subjects first lis-
ten to an original lead sheet A and a single melody B. Both
A and B are 8-bar long (16 seconds) and are randomly se-
lected from different musical pieces from our validation
set. As in Section 4.4, we harmonize melody B with the
harmonic style of A using our model and the baseline mod-
els. Subjects are then required to evaluate each version of
harmonization. The rating is based on a five-point scale
from 1 (very poor) to 5 (very high) over three metrics: har-
monicity, creativity, and musicality.
A total of 38 subjects with diverse music backgrounds
participated in our survey and we obtain 142 effective rat-
ings for each metric. As shown in Figure 7, the height of
the bars represents the mean value of the ratings. The error
bars represent the mean square errors (MSEs) computed by
within-subject ANOVA [34]. We report a significantly bet-
ter harmonization performance of our model than all three
baselines in each metric (p-value p < 0.05). Specifically,
we note that our model achieves such performance based
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
929

Harmonicity
Creativity
Musicality
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Rating
Ours 
Mask-CR 
Non-CR 
Non-DAT
Figure 7: Subjective evaluation on the harmonization per-
formance of our model and baseline models.
on a higher degree of representation disentanglement and
controllability. We evaluate these methodological aspects
with finer objective metrics in the following section.
4.6 Objective Evaluation
In this section, we objectively compare our model with the
baselines in terms of disentanglement and controllability.
The baseline models are as defined in Section 4.5.
4.6.1 Disentanglement
Our model disentangles chord representation zx from
melody condition c. In our case, the melody controls the
absolute pitch of the chord progression. A satisfied disen-
tanglement should derive a pitch-invariant representation.
Following [7,35], we develop a similarity criterion to eval-
uate the performance on disentanglement.
Let Ti(Â·) be a transposition operator with i semi-
tones. We calculate cosine similarity cos(zx, zTi(x)), i =
1, 2, Â· Â· Â· , 12 for our model and for each baseline. In Fig-
ure 8, a higher similarity means representation zx is less
affected by the absolute pitch and thus is better disentan-
gled. Our model outperforms all three baselines, includ-
ing Mask-CR. This finding corroborates that a proper cor-
ruption strategy is crucial to applying domain adversarial
training to concrete tasks. In our case, masking is not the
best way to corrupt, as it is less aware of the harmonization
context or dependency discussed in Section 3.3.
It is also worth noting that the similarity of zx reflects
human pitch perception. For each model, transposing a tri-
tone (T6(Â·)) derives the lowest similarity. Figure 8 shows
that zT6(x) is literally orthogonal to zx for Non-DAT and
Non-CR. Interestingly, tritone is the most dissonant among
all musical intervals in human perception. Such observa-
tion indicates that our model learns non-trivial music rules.
4.6.2 Controllability
A pitch-invariant representation helps us improve the
model controllability by enforcing the decoder to rely on
external conditions. In our case of harmonization, a good
control generates harmonic chord progression conditioned
on the lead melody. Aside from the subjective evaluation
in Section 4.5, we introduce harmony histogram to objec-
tively interpret the quality of control. Concretely, the har-
mony histogram is defined as the ratio of within-chord note
positions on which the lead melody lies. For tonal music,
m'2nd M'2nd m'3rd M'3rd p'4th Tritone p'5th m'6th M'6th m'7th M'7th p'oct
0.0
0.2
0.4
0.6
0.8
1.0
Cosine Distance
Non-DAT
Non-CR
Mask-CR
Ours
Figure 8: Object evaluation on representation similarity
(invariance) against pitch transposition. A higher value de-
notes better disentanglement.
root
3rd
5th
7th
9th
11th
13th
others
0
5
10
15
20
25
30
35
Percent
Original
Ours
Non-CR
Mask-CR
Non-DAT
Figure 9: Objective evaluation on harmony histogram
upon melody swapping. A higher ratio in root, 3rd, and
5th notes indicates a higher degree of controllability.
there should be more root, 3rd, and 5th notes appearing in
the melody compared to 7th and higher, so that the music
is considered harmonic.
In our experiment, we arrange our validation data into
random pairs and reconstruct the chord progression with
swapped melody conditions.
We compare the harmony
histogram of generated results from our model and all
baselines. Additionally, we compute the histogram for the
original (human-composed) data as ground truth. In Fig-
ure 9, we first observe that the histogram distribution has a
larger portion in the root, 3rd, and 5th notes for the original
data. For the baseline models, over 25% melody notes are
beyond all chord notes and tensions (shown by ÂªothersÂº in
Figure 9), which indicates excessive disharmony. Our pro-
posed model, on the other hand, keeps a more consistent
pattern with the ground truth.
5. CONCLUSION
In conclusion, we contribute a generalized form of domain
adversarial training for controllable music generation, es-
pecially when complex sequential conditions are involved.
The main novelty lies in the condition corruption objective,
which contextualizes the exact dependency between repre-
sentation zx and condition c, and therefore assists disen-
tanglement and control. Our method shows excellent per-
formance in chord representation learning, where we learn
a pitch-invariant representation conditioned on the melody
and develop a novel harmonization strategy. Our improve-
ment in disentanglement and controllability is elaborated
with extensive subjective and objective evaluation. With
the proposal of our methodology, we hope to bring a new
perspective not only to music generation but also to more
general scenarios of conditional representation learning.
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
930

6. REFERENCES
[1] A. Pati and A. Lerch, ÂªIs disentanglement enough? on
latent representations for controllable music genera-
tion,Âº in Proceedings of the 22nd International Society
for Music Information Retrieval Conference, ISMIR,
2021, pp. 517Â±524.
[2] S. Dai, Z. Jin, C. Gomes, and R. B. Dannenberg,
ÂªControllable deep melody generation via hierarchical
music structure representation,Âº in Proceedings of the
22nd International Society for Music Information Re-
trieval Conference, ISMIR, 2021, pp. 143Â±150.
[3] K. Chen, C. Wang, T. Berg-Kirkpatrick, and S. Dub-
nov, ÂªMusic sketchnet: Controllable music generation
via factorized representations of pitch and rhythm,Âº in
Proceedings of the 21th International Society for Mu-
sic Information Retrieval Conference, ISMIR, 2020,
pp. 77Â±84.
[4] J. Jiang, G. Xia, D. B. Carlton, C. N. Anderson, and
R. H. Miyakawa, ÂªTransformer VAE: A hierarchical
model for structure-aware and interpretable music rep-
resentation learning,Âº in 2020 IEEE International Con-
ference on Acoustics, Speech and Signal Processing,
ICASSP.
IEEE, 2020, pp. 516Â±520.
[5] M. Xu, Z. Wang, and G. Xia, ÂªTransferring piano per-
formance control across environments,Âº in IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, ICASSP.
IEEE, 2019, pp. 221Â±225.
[6] J. W. Kim, R. M. Bittner, A. Kumar, and J. P. Bello,
ÂªNeural music synthesis for flexible timbre control,Âº in
IEEE International Conference on Acoustics, Speech
and Signal Processing, ICASSP.
IEEE, 2019, pp.
176Â±180.
[7] R. Yang, D. Wang, Z. Wang, T. Chen, J. Jiang, and
G. Xia, ÂªDeep music analogy via latent representation
disentanglement,Âº in Proceedings of the 20th Interna-
tional Society for Music Information Retrieval Confer-
ence, ISMIR, 2019, pp. 596Â±603.
[8] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain,
H. Larochelle, F. Laviolette, M. Marchand, and V. S.
Lempitsky, ÂªDomain-adversarial training of neural
networks,Âº Journal of Machine Learning Research,
vol. 17, pp. 59:1Â±59:35, 2016.
[9] L. Kawai, P. Esling, and T. Harada, ÂªAttributes-aware
deep music transformation,Âº in Proceedings of the 21th
International Society for Music Information Retrieval
Conference, ISMIR, 2020, pp. 670Â±677.
[10] Y. Matsuoka and S. Sako, ÂªAttribute-aware deep
music transformation for polyphonic music,Âº 2021,
Late
Breaking
Demo
in
the
22nd
International
Society for Music Information Retrieval Conference,
ISMIR. [Online]. Available: https://archives.ismir.net/
ismir2021/latebreaking/000035.pdf
[11] J. Briot, G. Hadjeres, and F. Pachet, Deep learning
techniques for music generation.
Springer, 2020.
[12] G. Louppe, M. Kagan, and K. Cranmer, ÂªLearning to
pivot with adversarial networks,Âº in Advances in Neu-
ral Information Processing Systems 30: Annual Con-
ference on Neural Information Processing Systems,
2017, pp. 981Â±990.
[13] W. Wei, H. Zhu, E. Benetos, and Y. Wang, ÂªA-CRNN:
A domain adaptation model for sound event detection,Âº
in 2020 IEEE International Conference on Acoustics,
Speech and Signal Processing, ICASSP.
IEEE, 2020,
pp. 276Â±280.
[14] F. J. Castellanos, A.-J. Gallego, and J. Calvo-Zaragoza,
ÂªUnsupervised domain adaptation for document analy-
sis of music score images,Âº in Proceedings of the 22nd
International Society for Music Information Retrieval
Conference, ISMIR, 2021, pp. 81Â±87.
[15] G. Lample, N. Zeghidour, N. Usunier, A. Bordes,
L. Denoyer, and M. Ranzato, ÂªFader networks: Ma-
nipulating images by sliding attributes,Âº in Advances
in Neural Information Processing Systems 30: An-
nual Conference on Neural Information Processing
Systems, 2017, pp. 5967Â±5976.
[16] M. Li, W. Zuo, and D. Zhang, ÂªDeep identity-
aware transfer of facial attributes,Âº arXiv preprint
arXiv:1610.05586, 2016.
[17] Y. Zhang, ÂªRepresentation learning for controllable
music generation: A survey,Âº 2020. [Online]. Avail-
able: https://doi.org/10.13140/RG.2.2.34458.11208
[18] Z. Wang, Y. Zhang, Y. Zhang, J. Jiang, R. Yang, G. Xia,
and J. Zhao, ÂªPIANOTREE VAE: structured represen-
tation learning for polyphonic music,Âº in Proceedings
of the 21th International Society for Music Information
Retrieval Conference, ISMIR, 2020, pp. 368Â±375.
[19] Z. Wang, D. Wang, Y. Zhang, and G. Xia, ÂªLearning in-
terpretable representation for controllable polyphonic
music generation,Âº in Proceedings of the 21th Interna-
tional Society for Music Information Retrieval Confer-
ence, ISMIR, 2020, pp. 662Â±669.
[20] Y. Chen, H. Lee, Y. Chen, and H. Wang, ÂªSurprisenet:
Melody harmonization conditioning on user-controlled
surprise contours,Âº in Proceedings of the 22nd Interna-
tional Society for Music Information Retrieval Confer-
ence, ISMIR, 2021, pp. 105Â±112.
[21] Y. Zhang, Z. Wang, D. Wang, and G. Xia, ÂªButter:
A representation learning framework for bi-directional
music-sentence retrieval and generation,Âº in Proceed-
ings of the 1st workshop on NLP for music and audio
(NLP4MusA), 2020, pp. 54Â±58.
[22] K. Sohn, H. Lee, and X. Yan, ÂªLearning structured out-
put representation using deep conditional generative
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
931

models,Âº in Advances in Neural Information Process-
ing Systems 28: Annual Conference on Neural Infor-
mation Processing Systems, 2015, pp. 3483Â±3491.
[23] I. Higgins, L. Matthey, A. Pal, C. P. Burgess, X. Glo-
rot, M. M. Botvinick, S. Mohamed, and A. Lerchner,
Âªbeta-vae: Learning basic visual concepts with a con-
strained variational framework,Âº in 5th International
Conference on Learning Representations, ICLR, Con-
ference Track Proceedings.
OpenReview.net, 2017.
[24] E.
Foxley,
ÂªNottingham
database,Âº
[EB/OL],
https://ifdo.ca/~seymour/nottingham/nottingham.html
Accessed May 25, 2021.
[25] Z. Wang*, K. Chen*, J. Jiang, Y. Zhang, M. Xu, S. Dai,
and G. Xia, ÂªPOP909: A pop-song dataset for music
arrangement generation,Âº in Proceedings of the 21th
International Society for Music Information Retrieval
Conference, ISMIR, 2020, pp. 38Â±45.
[26] J. Devlin, M. Chang, K. Lee, and K. Toutanova,
ÂªBERT: pre-training of deep bidirectional transform-
ers for language understanding,Âº in Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL-HLT, Volume 1.
Association for Computational Linguistics, 2019, pp.
4171Â±4186.
[27] P. Shaw, J. Uszkoreit, and A. Vaswani, ÂªSelf-attention
with relative position representations,Âº in Proceedings
of the 2018 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL-HLT, Volume 2.
Association for Computational Linguistics, 2018, pp.
464Â±468.
[28] C. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon,
C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman,
M. Dinculescu, and D. Eck, ÂªMusic transformer: Gen-
erating music with long-term structure,Âº in 7th Interna-
tional Conference on Learning Representations, ICLR.
OpenReview.net, 2019.
[29] Z. Wang and G. Xia, ÂªMusebert: Pre-training music
representation for music understanding and control-
lable generation,Âº in Proceedings of the 22nd Interna-
tional Society for Music Information Retrieval Confer-
ence, ISMIR, 2021, pp. 722Â±729.
[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,
ÂªAttention is all you need,Âº in Advances in Neural In-
formation Processing Systems 30: Annual Conference
on Neural Information Processing Systems, 2017, pp.
5998Â±6008.
[31] G.
E.
Hinton,
N.
Srivastava,
A.
Krizhevsky,
I. Sutskever, and R. R. Salakhutdinov, ÂªImprov-
ing neural networks by preventing co-adaptation of
feature detectors,Âº arXiv preprint arXiv:1207.0580,
2012.
[32] D. P. Kingma and J. Ba, ÂªAdam: A method for stochas-
tic optimization,Âº in 3rd International Conference on
Learning Representations, ICLR, Conference Track
Proceedings, 2015.
[33] N. B. Toomarian and J. Barhen, ÂªLearning a trajectory
using adjoint functions and teacher forcing,Âº Neural
networks, vol. 5, no. 3, pp. 473Â±484, 1992.
[34] H. Scheffe, The analysis of variance.
John Wiley &
Sons, 1999, vol. 72.
[35] S. Wei and G. Xia, ÂªLearning long-term music repre-
sentations via hierarchical contextual constraints,Âº in
Proceedings of the 22nd International Society for Mu-
sic Information Retrieval Conference, ISMIR, 2021,
pp. 738Â±745.
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
932
