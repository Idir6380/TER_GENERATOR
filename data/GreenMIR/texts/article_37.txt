MUSEBERT: PRE-TRAINING OF MUSIC REPRESENTATION FOR MUSIC
UNDERSTANDING AND CONTROLLABLE GENERATION
Ziyu Wang
Music X Lab, NYU Shanghai
ziyu.wang@nyu.edu
Gus Xia
Music X Lab, NYU Shanghai
gxia@nyu.edu
ABSTRACT
BERT has proven to be a powerful language model in
natural language processing and established an effective
pre-training & ï¬ne-tuning methodology. We see that mu-
sic, as a special form of language, can beneï¬t from such
methodology if we carefully handle its highly-structured
and polyphonic properties. To this end, we propose Muse-
BERT and show that: 1) MuseBERT has detailed spec-
iï¬cation of note attributes and explicit encoding of mu-
sic relations, without presuming any pre-deï¬ned sequential
event order, 2) the pre-trained MuseBERT is not merely a
language model, but also a controllable music generator,
and 3) MuseBERT gives birth to various downstream mu-
sic generation and analysis tasks with practical value. Ex-
periment shows that the pre-trained model outperforms the
baselines in terms of reconstruction likelihood and genera-
tion quality. We also demonstrate downstream applications
including chord analysis, chord-conditioned texture gener-
ation, and accompaniment reï¬nement.
1. INTRODUCTION
BERT [1] has proven to be one of the leading language
models in natural language processing, which learns nat-
ural language representation in an unsupervised manner
and achieved state-of-the-art results in many downstream
language understanding tasks. The methods involved in
BERT are not unfamiliar in the music domain. For ex-
ample, the â€œmasked language modelâ€ (MLM) objective
in BERT is similar to the Bach chorale inpainting stud-
ies [2â€“4], where the grids in a four-part piano-roll are ran-
domly masked and the model is trained to reconstruct them
from context. The Transformer architecture has also been
applied to different styles of music generation [5,6].
We aim to develop a pre-trained music-domain BERT
for better music understanding and generation. A straight-
forward approach is to use existing music representations
used in existing Transformer-based models, such as MIDI-
like representations [5,7], REMI [6], or CP [8], and simply
train the original BERT model. However, we argue that
Â© Z. Wang, and G. Xia. Licensed under a Creative Com-
mons Attribution 4.0 International License (CC BY 4.0). Attribution:
Z. Wang, and G. Xia, â€œMuseBERT: Pre-training of Music Representation
for Music Understanding and Controllable Generationâ€, in Proc. of the
22nd Int. Society for Music Information Retrieval Conf., Online, 2021.
such approach overlooks major distinctions between mu-
sic and natural language. Firstly, unlike natural language,
music (especially polyphony) does not follow a unique se-
quential order; abruptly ï¬‚atten music into a 1-d sequence
imposes extra protocol and often undermines the local
structure of music, adding extra burden to the model. Sec-
ondly, music involves rich relations and contexts. A single
music event is barely meaningful, and common music con-
cepts such as rhythmic patterns and harmonies are estab-
lished upon relative positions of notes. However, current
sequential models, in particular Transformer-based mod-
els, still struggle to capture these relations [9].
To overcome the limits above, we propose MuseBERT,
a Transformer-based pre-trained model with tailored han-
dling of music positional information and music rela-
tions. 1 Since the innate music positions are expressed in
the time-frequency space, we use onset and pitch infor-
mation as the absolute positional encoding, which can be
masked at the input and then reconstructed at the out-
put. Moreover, multiple musical relations are represented
via the design of generalized relative positional encoding
(RPE) modiï¬ed from the original RPE design [10].
In
our implementation, we ï¬nd the traditional sequential po-
sitional encoding unnecessary, and thus our model does not
rely on any pre-deï¬ned sequential assumptions.
Under such tailored design, we show the pre-trained
MuseBERT is not merely a language model but also a
controllable music generator.
In the ï¬ne-tuning stage,
the model naturally empowers a wide spectrum of ï¬ne-
tuning tasks, involving both music analysis and genera-
tion. Speciï¬cally, we showcase that MuseBERT can per-
form polyphonic music generation controlled by chord and
texture, chord extraction, and accompaniment reï¬nement.
In summary, the contributions of this paper are:
â€¢ We demonstrate the possibility and importance of
training a Transformer-based music language model
in an unordered approach.
â€¢ We develop generalized RPE for BERT-like models,
which not only reveals music relations from multiple
perspectives but also has a potential to be adopted to
other domains.
â€¢ We show MuseBERT, as a controllable music gener-
ator, giving the ï¬ne-tuning procedure practical mu-
sic meanings.
1 Code and demos can be accessed via https://github.com/
ZZWaang/musebert.
722

2. DATA REPRESENTATION
Unlike natural language processing, there is not a common
way to tokenize symbolic music. A music piece is gen-
erally considered as a combination of note events, where
each note event is a tuple of attributes. Note-based to-
kenization is commonly seen in music-tailored packages
[11,12], softwares [13â€“15], and researches [8,16,17]. On
other occasions, mainly for the ease of neural network
modeling, music is also simpliï¬ed as a sequence of con-
trols with pre-deï¬ned order, such as MIDI-like event rep-
resentation [5,7] (including its follow-up methods [6]), and
frame-wise piano-roll representations [2,18â€“20].
In MuseBERT, we consider note-based tokenization
without presumption of any sequential order. Speciï¬cally,
we invent two note-based music representations for Muse-
BERT: basic data representation and factorized data rep-
resentation, denoted by Rbase and Rfac respectively. A mu-
sic input is ï¬rst encoded as Rbase, and then converted into
Rfac, serving as the input format to MuseBERT.
In this paper, we primarily consider 2-bar music seg-
ments in 4/4 time signature. Longer music and richer time
signatures can be generalized and we leave it for future
study.
2.1 Rbase and Rfac
Rbase represents a music segment X as an unordered set
of note events {xi}N
i=1, and a note event xi âˆˆX consists
of three attributes: onset (o), pitch (p), and duration (d),
where o âˆˆ[0..31] and d âˆˆ[0..32] are counted by semi-
quavers, and p âˆˆ[0..127] is the MIDI note number. 2 We
denote the attribute set of Rbase by Abase := {o, p, d} and
use xi.a, a âˆˆAbase to indicate the attribute a of a note
event xi.
Rbase is not the MuseBERT input data format because
the representation is ambiguous for BERT-like models to
reconstruct well. Consider, for example, two simultaneous
quarter notes in a music segment that are unordered (by
deï¬nition of Rbase) and whose attributes except p are the
same. When their pitch attributes are masked at the input,
the two tokens have completely the same encoding and are
unable to be distinguished by BERT, in which case BERT
will yield the same output distributions.
We therefore invent Rfac which is unambiguous for
MuseBERT (proved in section 4). Rfac also represents a
music segment as an unordered set of note events, but it
uses a more detailed attribute set Afac (discussed in sec-
tion 2.2), and contains a stack of relation matrices storing
pairwise note relations (discussed in section 2.3).
2.2 Factorized Attributes
In Rfac, onset, pitch, and duration, due to their hierarchical
nature [21,22], are regarded as meta-attributes to be factor-
ized. The factorization operation we will introduce is anal-
ogous to storing a number by its digits (e.g., 49 7â†’[4, 9]),
2 We use [a..b] to denote the integer interval {x|a â‰¤x â‰¤b, x âˆˆZ}
including both endpoints.
so that we can mask partial information at certain hierar-
chy (e.g., [4, MASK]) and the remaining information is still
retained. In music, musicians sometimes prefer to interpret
49 as 50 âˆ’1 rather than 40 + 9, depending on the context.
Such subtlety is welcomed by our design.
2.2.1 Onset and Duration Factorization
We factorize onset (o) into beat position (o_bt âˆˆ[0..8])
and subdivision (o_sub âˆˆ[0..4]), and duration d into half
note counts (d_hlf âˆˆ[0..4]) and the remainder semiqua-
ver counts (d_sqv âˆˆ[0..7]), satisfying
o = (4 Ã— o_bt + o_sub),
(1)
d = (8 Ã— d_hlf + d_sqv).
(2)
We allow o_sub to take both positive and negative values,
since a subdivision can be interpreted as an â€œoff-beatâ€ of
the current downbeat, or an â€œupbeatâ€ preceding the next
downbeat. Consequently, the o attribute can be mapped to
multiple {o_bt, o_sub) pairs, e.g., {o:1} maps to {o_bt:
0, o_sub:1} or {o_bt:1, o_sub:âˆ’3}.
In our implementation, we non-deterministically sam-
ple one of the factorizations. In the future, a posterior dis-
tribution of p(o_bt, o_sub|o, X) can be explored in ï¬ner
detail.
2.2.2 Pitch Factorization
We factorize pitch in a similar fashion into three attributes:
1) pitch highness (p_hig âˆˆ[0..6]): voice types (e.g.,
SATB), 2) pitch register (p_reg âˆˆ[0..2]): the relative oc-
tave given p_hig, and 3) pitch degree (p_deg âˆˆ[0..11]),
satisfying:
p =
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
24 + 12 Ã— (p_hig + p_reg)
+p_deg,
0 â‰¤p_hig â‰¤4
12 Ã— p_reg + p_deg,
p_hig = 5
108 + 12 Ã— p_reg + p_deg,
p_hig = 6.
(3)
Here, 0 â‰¤p_hig â‰¤4 corresponds to the pitch range of a
standard piano (MIDI pitch 24-107), and p_hig = 5 and
p_hig = 6 handles the extra-low or extra-high regions,
respectively. Similar to onset, pitch factorization is also
handled non-deterministically, since a pitch can be inter-
preted as a lower voice type having a high register, or a
higher voice type having a low register.
To summarize, Rfac uses the attribute set:
Afac := {o_bt, o_sub, p_hig, p_reg,
p_deg, d_hlf, d_sqv}.
(4)
2.3 Note Event Relations
Music is rich in relations and we explicitly represent the
most fundamental relations in Rfac. We consider less-than,
equal-to, and greater-than relations on a subset of attributes
S := {o, o_bt, p, p_hig}. Speciï¬cally, âˆ€a âˆˆS, we de-
ï¬ne a mapping Î³a from an input note event pair (xi, xj) to
their relation symbol: 3
3 When a = o, xi.o is a shorthand notation interpreted as the return
value of Eqn (1). A similar case holds when a = p.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
723

â„›!"#
âˆ—
ğ‘‹âˆ—
ğ‘…ğ’®
âˆ—
ğ‘¥!
0
0
3
1
4
0
4
ğ‘¥"
?
?
3
0
0
?
?
ğ‘¥#
1
0
3
1
5
0
3
ğ‘¥$
2
-1
3
1
4
0
1
ğ‘¥%
2
0
3
1
4
0
4
ğ‘¥&
?
2
?
0
?
0
?
ğ‘¥'
3
0
4
0
11
0
4
o_sub
o_bt
p_hig p_reg p_deg d_hlf d_sqv
ğ‘¥!
ğ‘¥"
ğ‘¥#
ğ‘¥$
ğ‘¥%
ğ‘¥&
ğ‘¥'
â‰¡
?
<
<
<
<
<
â‰¡
â‰¡
<
<
<
<
<
>
>
â‰¡
<
<
<
<
>
>
>
â‰¡
â‰¡
<
<
>
>
>
â‰¡
â‰¡
<
<
>
>
>
>
>
â‰¡
?
>
>
>
>
>
â‰¡
â‰¡
â‰¡
â‰¡
<
<
<
<
<
â‰¡
â‰¡
<
<
<
<
<
>
>
â‰¡
<
<
<
<
>
>
>
â‰¡
<
<
<
>
>
>
>
â‰¡
<
<
>
>
>
>
>
â‰¡
?
>
>
>
>
>
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
â‰¡
?
<
>
â‰¡
>
>
>
>
â‰¡
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
>
â‰¡
>
>
>
>
â‰¡
ğ‘¥! â‰¡
>
<
â‰¡
â‰¡
?
<
ğ‘¥" <
â‰¡
<
<
<
?
<
ğ‘¥# >
>
â‰¡
>
>
>
<
ğ‘¥$ â‰¡
>
<
â‰¡
â‰¡
>
<
ğ‘¥% â‰¡
>
<
â‰¡
â‰¡
?
<
ğ‘¥&
?
?
<
<
?
â‰¡
?
ğ‘¥' >
>
>
>
>
?
â‰¡
ğ‘¥!
0
0
3
1
4
0
4
ğ‘¥"
0
0
4
1
0
1
4
ğ‘¥#
1
0
3
1
5
0
3
ğ‘¥$
2
-1
3
1
4
0
1
ğ‘¥%
2
0
3
1
4
0
4
ğ‘¥&
3
0
3
1
2
0
4
ğ‘¥'
3
0
4
0
11
0
4
o_sub
o_bt
p_hig p_reg p_deg d_hlf d_sqv
ğ‘¥!
ğ‘¥"
ğ‘¥#
ğ‘¥$
ğ‘¥%
ğ‘¥&
ğ‘¥'
â‰¡
â‰¡
<
<
<
<
<
â‰¡
â‰¡
<
<
<
<
<
>
>
â‰¡
<
<
<
<
>
>
>
â‰¡
â‰¡
<
<
>
>
>
â‰¡
â‰¡
<
<
>
>
>
>
>
â‰¡
â‰¡
>
>
>
>
>
â‰¡
â‰¡
â‰¡
â‰¡
<
<
<
<
<
â‰¡
â‰¡
<
<
<
<
<
>
>
â‰¡
<
<
<
<
>
>
>
â‰¡
<
<
<
>
>
>
>
â‰¡
<
<
>
>
>
>
>
â‰¡
â‰¡
>
>
>
>
>
â‰¡
â‰¡
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
>
â‰¡
>
>
>
>
â‰¡
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
â‰¡
<
â‰¡
â‰¡
â‰¡
â‰¡
<
>
â‰¡
>
>
>
>
â‰¡
ğ‘¥! â‰¡
<
<
â‰¡
â‰¡
>
<
ğ‘¥" >
â‰¡
>
>
>
>
>
ğ‘¥# >
<
â‰¡
>
>
>
<
ğ‘¥$ â‰¡
<
<
â‰¡
â‰¡
>
<
ğ‘¥% â‰¡
<
<
â‰¡
â‰¡
>
<
ğ‘¥& <
<
<
<
<
â‰¡
<
ğ‘¥' >
<
>
>
>
>
â‰¡
â„›!"#
ğ‘‹!"#
ğ‘¥!
0
64
4
ğ‘¥"
0
72
12
ğ‘¥#
4
65
3
ğ‘¥$
7
64
1
ğ‘¥%
8
64
4
ğ‘¥&
12
62
4
ğ‘¥'
12
71
4
o
p
d
â„›$"%&
ğ‘‹$"%&
ğ‘…ğ’®
Basic Data Representation
Factorized Data Representation
Corrupted Factorized Data
MuseBERT
Music Observation
â‘ Stochastic mapping
â‘£Deterministic Mapping
â‘¡Data Corruption
â‘¢Reconstruction
Read
Write
Figure 1. The workï¬‚ow of MuseBERT pre-training.
Î³a(xi, xj) =
ï£±
ï£´
ï£²
ï£´
ï£³
<a,
xi.a < xj.a
â‰¡a,
xi.a â‰¡xj.a
>a,
xi.a > xj.a.
(5)
The relation symbols are stored using a stack of N Ã— N
relation matrices, denoted by RS := {Ra|a âˆˆS}, where
Ra = {ra
ij} and ra
ij = Î³a(xi, xj).
3. PRE-TRAINING MUSEBERT
Figure 1 shows the overall pre-training stage. A music data
is ï¬rst read as Rbase format as Xbase, and then stochasti-
cally converted to Rfac consisting of unordered note events
Xfac and relation matrices RS.
Similar to BERT, the
pre-training stage uses â€œmasked language modelâ€ (MLM)
training objective, where Xfac and RS will be corrupted
before fed into MuseBERT, which is trained to reconstruct
the original music segment.
3.1 Data Corruption
The original BERT randomly selects a part of the input
tokens to be corrupted. In MuseBERT, data corruption has
ï¬ner controls: 1) it operates on individual attributes of a
note event rather than an entire token, and 2) it corrupts
relation matrices as well.
For note events Xfac, the corrupter randomly selects
15% of the note events C âŠ‚X, and corrupts their at-
tributes {xi.a|xi âˆˆC, a âˆˆAfac} in one of the three ways:
1) masked with [MASK]a 80% of the time, 2) replaced with
a random token 10% of the time, and 3) kept unchanged
10% of the time. Each attribute is corrupted independently
of the choice of the other attributes. For relation matrices
RS, each matrix entry will be masked 30% of the time in-
dependently. Before corruption, a re-computation of the
matrix may be required so as to maintain matrix symmetry
and align with the attributes that are replaced with other
values.
We denote the representation after corruption as Râˆ—
fac,
and the resulting note events and relation matrices as Xâˆ—
and Râˆ—
S, respectively.
3.2 Overall Model Architecture
The model adopts the bi-directional Transformer encoder
architecture based on the original Transformer implemen-
tation described in [23].
The input to the model is the
embedding of a corrupted music segment Xâˆ—= {xâˆ—
i }N
i=1.
The embedding is computed as the sum of all attribute em-
beddings:
Emb(xâˆ—
i ) =
X
aâˆˆAfac
Emba(xâˆ—
i .a).
(6)
Here Emba(Â·) are linear embedding layers and the de-
fault absolute positional encoding is not applied due to un-
orderedness. Râˆ—
S is fed into the model as generalized rel-
ative positional encoding to be discussed in detail in sec-
tion 3.3.
The output is a distribution of reconstructed note events
in Rfac, denoted by Ë†X = {Ë†xi}N
i=1. Speciï¬cally, let h1:N
be the last hidden vector of the Transformer encoder, each
note attribute is reconstructed by a separate linear transfor-
mation normalized with a softmax layer:
pmodel(Ë†xi.a|Xâˆ—, Râˆ—
S) = softmax(FFNa(hi)).
(7)
The training loss is the mean of the negative log-likelihood
on corrupted attributes {xi.a|xi âˆˆC, a âˆˆAfac} only:
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
724

L( Ë†X|Xâˆ—, Râˆ—
S) =
âˆ’1
|C|
X
{i|xâˆ—
i âˆˆC}
 X
aâˆˆAfac
log pmodel(xfac
i .a|Xâˆ—, Râˆ—
S)

,
(8)
From the denoising autoencoder perspective [24], we
deï¬ne the reconstruction distribution precon( Ë†X|Xâˆ—, Râˆ—
S) in-
duced from Eqn (7) for later analysis purposes:
precon( Ë†X|Xâˆ—, Râˆ—
S) :=
N
Y
i=1
precon(Ë†xi|Xâˆ—, Râˆ—
S), where
precon(Ë†xi|Xâˆ—, Râˆ—
S) :=
Y
aâˆˆAfac
precon(Ë†xi.a|Xâˆ—, Râˆ—
S) and
precon(Ë†xi.a|Xâˆ—, Râˆ—
S) :=
(
pmodel(Ë†xi.a|Xâˆ—, Râˆ—
S), xi âˆˆC
1{Ë†xi.aâ‰¡xâˆ—
i .a}, otherwise.
(9)
In other words, during inference, we sample from pmodel
for corrupted tokens while simply keeping the uncorrupted
tokens.
3.3 Generalized Relative Positional Encoding
Let h1:N be the hidden vectors at any Transformer layer.
We add additional key-relation embeddings EmbK
S (Â·) of
the relations in the query-value product term by
eij =
hiW Q
hjW K + P
aâˆˆS EmbK
a (ra
ij)
T
âˆšdz
,
(10)
where dz is the hidden vector size of attention heads,
W Q, W K are query and key transformations, and eij is
used to compute attention weights.
Similarly,
we also add value-relation embeddings
EmbV
S (Â·) in the attention weight application:
zi =
N
X
j=1
Î±ij

hjW V +
X
aâˆˆS
EmbV
a (ra
ij)

,
(11)
where Î±ij = softmaxj(eij), W V is the value transforma-
tion, and zi is fed into the position-wise feed-forward sub-
layer to compute the next layerâ€™s hi.
In our implementation, both key-relation embeddings
and value-relation embeddings are linear transforms. In
addition, for different attention layers and attention heads,
we train different embeddings.
We call the above operation generalized relative po-
sitional encoding (RPE) because the summation of key-
relation and value-relation embeddings allows us to hint
the MuseBERT from multiple musical perspectives (at-
tribute relations), which is not considered in the original
RPE implementation [10].
4. THEORETICAL ANALYSIS
In this section, we ï¬rst show that Rfac and generalized rel-
ative positional encoding are theoretical necessities for a
BERT model to handle unordered data. We further show
that MuseBERT is a powerful controllable music genera-
tor, giving birth to various ï¬ne-tuning tasks with musical
merits.
4.1 Well-deï¬nedness of MuseBERT
The fundamental assumption made in MuseBERT is the
refusal of traditional sequential positional encoding. In
return, the mathematical property of MuseBERT we gain
is permutation invariance, i.e., the input order will not
change the model output. On the other hand, a natural
downside of a permutation invariant model is ambiguity:
without extra efforts, masked tokens will have the same
output distributions and hence not distinguishable.
The problem is solved by factorized data representa-
tion and generalized RPE because the former allows us
to specify music in more detail. We summarize the dis-
cussion with the well-deï¬nedness theorem of MuseBERT.
Part one of the theorem is maintained by the property of
Transformer [25], and part two is self-evident from the dis-
cussion so far.
Theorem 1 (Well-deï¬nedness) Pre-training MuseBERT
is well-deï¬ned: Let (Xâˆ—, Râˆ—
S) be a corrupted music seg-
ment in Rfac:
1) (Permutation invariance) Let Ïƒ(Â·) be an arbitrary per-
mutation. The reconstruction distribution is unchanged af-
ter permutation:
precon( Ë†X|Xâˆ—, Râˆ—
S) â‰¡precon(Ïƒ( Ë†X)|Ïƒ(Xâˆ—),Ïƒ(Râˆ—
S))
2) (Unambiguity) For arbitrary pair of xi, xj âˆˆXâˆ—, whose
attributes are corrupted but their note relations are not,
the reconstruction distributions of the two tokens are not
always the same, i.e., there exists a set of model parameters
such that
precon(Ë†xi|Xâˆ—, Râˆ—
S) Ì¸= precon(Ë†xj|Xâˆ—, Râˆ—
S).
4.2 MuseBERT as a Music Generator
The operations of MuseBERT can be regarded as a sin-
gle round of a Markov chain transition which alternatively
adds noise and denoises (as shown in Figure 1), consisting
of four steps: 1) stochastic data conversion to Rfac, 2) data
corruption, 3) MuseBERT reconstruction, and 4) determin-
istic data conversion to Rbase.
Benjio et al. [26] showed such transition induced from
denoising autoencoder is a special type of Generative
Stochastic Network and proved that under mild condition,
the transition operations form a Markov Chain whose sta-
tionary distribution is the true data distribution. Hence, we
conclude the pre-training MuseBERT can be used as the
MCMC transition operator, and generates music via itera-
tive sampling.
4.3 Controllability of MuseBERT
Moreover, we show that MuseBERT is a Constraint Op-
timization Problem (COP) solver [27] by regarding uncor-
rupted attributes or relations as unary or binary constraints,
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
725

respectively. The COP has the form:
max
Ë†
X
precon( Ë†X|Xâˆ—)
(12)
s.t.
Ë†xi.a = xâˆ—
i .a,
âˆ€xâˆ—
i /âˆˆC, a âˆˆAfac
Î³a(xi, xj) = Î³a(xâˆ—
i , xâˆ—
j),
âˆ€a âˆˆS
if Î³a(xâˆ—
i , xâˆ—
j) uncorrupted.
Apparently, music continuation and in-painting can be
formalized as a special COP problem, and monophony,
melodic directions, rhythmic patterns, texture, etc. are all
expressible concepts by MuseBERT constraints. Hence,
MuseBERT, as a music generator, is also controllable,
which can be ï¬ne-tuned to solve problem-speciï¬c gener-
ation tasks and controls.
5. FINE-TUNING MUSEBERT
By deï¬ning problem-speciï¬c data corrupters, we can ï¬ne-
tune the model for a wide spectrum of conditioned music
generation tasks, such as music continuation, inpainting,
rhythm or texture-conditioned music generation. In this
section, we in turn show two less obvious applications by
ï¬ne-tuning, in which we extend the vocabulary of Rfac to
represent chord and vocal tracks, and apply MuseBERT for
both generation and analysis.
5.1 Case 1: Texture Generation and Chord Analysis
MuseBERT can be ï¬ne-tuned for chord and texture con-
trols, which are commonly-studied controlling factors in
automatic music generation [28, 29]. The ï¬ne-tuning task
is set by slight modiï¬cation from the pre-training stage
in two steps: 1) We represent chord progression as Rfac
events and prepend it to the music segment. Speciï¬cally,
we deï¬ne p_hig = 7 for chord event, with p_reg =
0, p_reg = 1, p_reg = 2 for chord root, chroma, and
bass, respectively. 2) We apply an additional â€œin-chordâ€
inter-relation to indicate whether a note event is a chord-
tone or not. The conditioned generation can be trained by
masking all the attributes from the music segment while
keeping the chord attributes and all relations.
Figure 2
shows an example of music generation conditioned on the
texture in Figure 2(a) and the chord F major. The gener-
ated segment keeps the textural property and deals with
inner voices smoothly (shown by the red boxes).
Similarly, by corrupting the chord events and â€œin-
chordâ€ relation, while maintaining the music segment and
the other relations, the model can learn to extract chord
progressions from a given piece. The ï¬ne-tuning result
on POP909 [30] shows the model has a 99.35% accuracy
on token-wise (MIDI) chord extraction (99.37% for root,
99.05% for chroma and 99.35% for bass).
5.2 Case 2: Accompaniment Reï¬nement
In the accompaniment reï¬nement task, we want to reï¬ne
an imperfect accompaniment according to a given lead
melody.
To achieve this, we represent the melody and
the accompaniment both as Rfac and concatenate them
Figure 2. An example of controlled generation by speci-
fying chord and texture.
together.
We then randomly corrupt some accompani-
ment attributes while keeping the melody and relations
unchanged during training.
Figure 3 shows an example
of accompaniment reï¬nement from a corrupted sample
Figure 3(a). In (a), the red note heads mark the note being
replaced and the masked notes are replaced by rests. The
result shows that the generated piece is more consistent
with the melody while keeps the original musical content.
Figure 3. An example of accompaniment reï¬nement con-
ditioned on melody.
6. EXPERIMENTAL RESULT
6.1 Dataset and Training
We collect around 5K classical and popular piano pieces
from Musicalion 4 and the POP909 dataset [30]. We only
keep the pieces with 2/4 and 4/4 meters and cut them into
8-beat music segments. In all, we have 19.8K samples. We
randomly split the dataset (at song-level) into training set
(90%) and test set (10%). All training samples are further
augmented by transposing to all 12 keys.
We pre-train our MuseBERT using 12 Transformer lay-
ers with 8 attention heads, 0.1 dropout rate, and GELU ac-
4 Musicalion: https://www.musicalion.com.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
726

tivation [31]. The hidden dimensions of self-attention and
position-wise feed-forward layers are dmodel = 256 and
dff = 512. We train with a batch size of 128 and Adam op-
timizer with learning rate of 5e-4 and linear learning rate
decay to 5e-6 after 15K warmup steps. The pre-training
takes 20 epochs to converge.
In ï¬ne-tuning tasks, we kept the same hyperparameters
except the initial learning rate is 2e-4 without warmup. The
epochs to ï¬ne-tune the model are task-speciï¬c, approxi-
mately 4 epochs on average.
6.2 Objective Evaluation
Given that Rfac and data corruption are stochastic, we eval-
uate the expectation of reconstruction likelihood in terms
of token-wise onset, pitch and duration, similar to the algo-
rithm in [4]. We compare among 1) the pre-trained Muse-
BERT, 2) the ï¬ne-tuned models using attribute-speciï¬c
data corrupters, and three baseline models ablating the use
of factorized attributes Afac and binary relations. Table 1
summarizes the results where we see our pre-trained model
outperforms the baselines on all three criteria and the ï¬ne-
tuned model can perform even better. The use of Afac and
binary relations not only makes the model well-deï¬ned,
but also are crucial for high-quality reconstruction.
Models
Onset
Pitch
Duration
Afac w/ rel (ours)
0.902 Â± 0.002
0.813 Â± 0.003
0.815Â±0.003
Afac w/o relâˆ—
0.472 Â± 0.002
0.740 Â± 0.004
0.743 Â± 0.003
Abase w/ rel
0.856 Â± 0.001
0.785 Â± 0.004
0.785 Â± 0.003
Abase w/o relâˆ—
0.488 Â± 0.003
0.733 Â± 0.003
0.733 Â± 0.002
Afac w/ rel (ï¬ne-tuned)
0.958 Â± 0.001
0.924 Â± 0.002
0.927 Â± 0.002
Table 1. Objective evaluation of the expectation of recon-
struction likelihood. Here, â€œw/ relâ€ and â€œw/o relâ€ indicates
whether generalized RPE is applied, and â€œâˆ—â€ indicates the
ill-deï¬ned models.
6.3 Self-attention Visualization
We observe that different attention heads of multiple lay-
ers in the pre-trained MuseBERT captures high-level mu-
sic knowledge. As shown in Figure 4, metrical structure
is revealed in (a) & (b), and voice-leading can be seen in
(c) & (d). We show the attention matrices on a piano-roll
for better interpretability, where an arrow xi â†’xj shows
the (j, i) entry and the brightness indicates the attention
strength.
6.4 Subjective Evaluation of Music Generation
We invite people to subjectively rate the generation qual-
ity of the pre-trained model through a double-blind on-
line survey.
During the survey, the subjects listen to 8
groups of samples, each containing 4 generated segments
together with a ground-truth human composition. The gen-
erated segments come from the four models (the same as
in Table 1 excluding the ï¬ne-tuned model) with the same
initial corruption state (60% of the total tokens) from the
original piece. Both the order of groups and the sample
order within each group are randomized. After listening
Figure 4. Visualization of attention matrices on selected
self-attention layers in pre-trained MuseBERT.
Figure 5.
Subjective evaluation results on pre-trained
MuseBERT.
to each sample, the subjects rate them based on a 5-point
scale from 1 (very low) to 5 (very high) according to three
criteria: creativity, naturalness and musicality.
A total of 37 subjects (15 females and 22 males) with
different musical backgrounds have completed the survey.
The aggregated result (as shown in Figure 5) shows that
the pre-trained models considering music relations (the
well-deï¬ned models) are signiï¬cantly better than the mod-
els without relations (the ill-deï¬ned models) (with p-value
< 0.005), and are marginal to the human composition
(with p-value > 0.05).
7. CONCLUSION
In conclusion, we contributed MuseBERT, a full treatment
of BERT in the music domain as a powerful music un-
derstanding and generation model. The main novelty lies
in the proposed generalized relative positional encoding,
which successfully reveals the non-sequential, polyphonic
structure of music and at the same time turns the masked
language model objective into a well-deï¬ned controllable
generative framework. Also, with the music-tailored fac-
torized data representation, the controls are ï¬ne-grained.
Furthermore, by ï¬ne-tuning MuseBERT, we demonstrate
that the model is general purpose, capable of various
downstream tasks such as chord analysis, accompaniment
generation and reï¬nement etc. as long as the constraints
can be expressed via binary relations of different music at-
tributes.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
727

8. REFERENCES
[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
â€œBert:
Pre-training of deep bidirectional trans-
formers for language understanding,â€ arXiv preprint
arXiv:1810.04805, 2018.
[2] G. Hadjeres, F. Pachet, and F. Nielsen, â€œDeepbach: a
steerable model for bach chorales generation,â€ in Inter-
national Conference on Machine Learning.
PMLR,
2017, pp. 1362â€“1371.
[3] Y. Yan, E. Lustig, J. VanderStel, and Z. Duan, â€œPart-
invariant model for music generation and harmoniza-
tion.â€ in 19th International Society for Music Informa-
tion Retrieval, 2018, pp. 204â€“210.
[4] C.-Z.
A.
Huang,
T.
Cooijmans,
A.
Roberts,
A. Courville, and D. Eck, â€œCounterpoint by con-
volution,â€ arXiv preprint arXiv:1903.07227, 2019.
[5] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer,
I. Simon, C. Hawthorne, A. M. Dai, M. D. Hoff-
man, M. Dinculescu, and D. Eck, â€œMusic transformer,â€
arXiv preprint arXiv:1809.04281, 2018.
[6] Y.-S. Huang and Y.-H. Yang, â€œPop music transformer:
Generating music with rhythm and harmony,â€ arXiv
preprint arXiv:2002.00212, 2020.
[7] I. Simon and S. Oore, â€œPerformance rnn: Generating
music with expressive timing and dynamics,â€ https://
magenta.tensorï¬‚ow.org/performance-rnn, 2017.
[8] W.-Y. Hsiao, J.-Y. Liu, Y.-C. Yeh, and Y.-H. Yang,
â€œCompound word transformer: Learning to compose
full-song music over dynamic directed hypergraphs,â€
arXiv preprint arXiv:2101.02402, 2021.
[9] J. Jiang, G. Xia, and T. Berg-Kirkpatrick, â€œDiscovering
music relations with sequential attention,â€ in Proceed-
ings of the 1st workshop on NLP for music and audio
(NLP4MusA), 2020, pp. 1â€“5.
[10] P. Shaw, J. Uszkoreit, and A. Vaswani, â€œSelf-attention
with relative position representations,â€ arXiv preprint
arXiv:1803.02155, 2018.
[11] C. Raffel and D. P. Ellis, â€œIntuitive analysis, creation
and manipulation of midi data with pretty midi,â€ in
15th International Society for Music Information Re-
trieval conference late breaking and demo papers,
2014, pp. 84â€“93.
[12] M. S. Cuthbert and C. Ariza, â€œmusic21: A toolkit for
computer-aided musicology and symbolic music data,â€
2010.
[13] â€œGarageband for mac.â€ [Online]. Available:
https:
//www.apple.com/mac/garageband/
[14] â€œLogic pro.â€ [Online]. Available: https://www.apple.
com/logic-pro/
[15] â€œCubase guides you on your music production
journey.â€ [Online]. Available:
https://new.steinberg.
net/cubase/
[16] Z. Wang, Y. Zhang, Y. Zhang, J. Jiang, R. Yang,
J. Zhao, and G. Xia, â€œPianotree vae:
Structured
representation learning for polyphonic music,â€ arXiv
preprint arXiv:2008.07118, 2020.
[17] D. Jeong, T. Kwon, Y. Kim, and J. Nam, â€œGraph neu-
ral network for music score data and modeling expres-
sive piano performance,â€ in International Conference
on Machine Learning.
PMLR, 2019, pp. 3060â€“3070.
[18] H. H. Mao, T. Shin, and G. Cottrell, â€œDeepj: Style-
speciï¬c music generation,â€ in IEEE 12th International
Conference on Semantic Computing.
IEEE, 2018, pp.
377â€“382.
[19] E. S. Koh, S. Dubnov, and D. Wright, â€œRethinking re-
current latent variable model for music composition,â€
in IEEE 20th International Workshop on Multimedia
Signal Processing.
IEEE, 2018, pp. 1â€“6.
[20] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang,
â€œMusegan: Multi-track sequential generative adversar-
ial networks for symbolic music generation and accom-
paniment,â€ in 32nd AAAI Conference on Artiï¬cial In-
telligence, 2018.
[21] F. Lerdahl and R. S. Jackendoff, A Generative Theory
of Tonal Music, reissue, with a new preface.
MIT
press, 1996.
[22] F. Lerdahl, Tonal pitch space.
Oxford University
Press, 2004.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-
sukhin, â€œAttention is all you need,â€ arXiv preprint
arXiv:1706.03762, 2017.
[24] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Man-
zagol, â€œExtracting and composing robust features with
denoising autoencoders,â€ in International Conference
on Machine Learning, 2008, pp. 1096â€“1103.
[25] J. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W.
Teh, â€œSet transformer:
A framework for attention-
based permutation-invariant neural networks,â€ in Inter-
national Conference on Machine Learning.
PMLR,
2019, pp. 3744â€“3753.
[26] Y. Bengio, E. Laufer, G. Alain, and J. Yosinski, â€œDeep
generative stochastic networks trainable by backprop,â€
in International Conference on Machine Learning.
PMLR, 2014, pp. 226â€“234.
[27] S. Russell and P. Norvig, â€œArtiï¬cial intelligence: a
modern approach,â€ 2002.
[28] Z. Wang, D. Wang, Y. Zhang, and G. Xia, â€œLearning in-
terpretable representation for controllable polyphonic
music generation,â€ arXiv preprint arXiv:2008.07122,
2020.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
728

[29] I.
Simon,
A.
Roberts,
C.
Raffel,
J.
Engel,
C. Hawthorne,
and D. Eck,
â€œLearning a latent
space
of
multitrack
measures,â€
arXiv
preprint
arXiv:1806.00195, 2018.
[30] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai,
X. Gu, and G. Xia, â€œPop909: A pop-song dataset
for music arrangement generation,â€ arXiv preprint
arXiv:2008.07142, 2020.
[31] D. Hendrycks and K. Gimpel, â€œGaussian error linear
units (gelus),â€ arXiv preprint arXiv:1606.08415, 2016.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
729
