MELOFORM: GENERATING MELODY WITH MUSICAL FORM BASED
ON EXPERT SYSTEMS AND NEURAL NETWORKS
Peiling Lu1
Xu Tan*1
Botao Yu2
Tao Qin1
Sheng Zhao3
Tie-Yan Liu1
1 Microsoft Research Asia, Beijing, China
2 Nanjing University, Nanjing, China
3 Microsoft Azure Speech, Beijing, China
{peil,xuta,taoqin,szhao,tyliu}@microsoft.com, btyu@smail.nju.edu.cn
https://github.com/microsoft/muzic
ABSTRACT
Human usually composes music by organizing elements
according to the musical form to express music ideas.
However, for neural network-based music generation, it is
difficult to do so due to the lack of labelled data on musi-
cal form. In this paper, we develop MeloForm, a system
that generates melody with musical form using expert sys-
tems and neural networks. Specifically, 1) we design an
expert system to generate a melody by developing musi-
cal elements from motifs to phrases then to sections with
repetitions and variations according to pre-given musical
form; 2) considering the generated melody is lack of mu-
sical richness, we design a Transformer based refinement
model to improve the melody without changing its musical
form. MeloForm enjoys the advantages of precise musical
form control by expert systems and musical richness learn-
ing via neural models. Both subjective and objective exper-
imental evaluations demonstrate that MeloForm generates
melodies with precise musical form control with 97.79%
accuracy, and outperforms baseline systems in terms of
subjective evaluation score by 0.75, 0.50, 0.86 and 0.89
in structure, thematic, richness and overall quality, without
any labelled musical form data. Besides, MeloForm can
support various kinds of forms, such as verse and chorus
form, rondo form, variational form, sonata form, etc. Mu-
sic samples generated by MeloForm are available via this
link 1 , and our code is available via this link 2 .
1. INTRODUCTION
Melody is often composed of hierarchical motifs, phrases
and sections with repetitions and variations given musi-
* Corresponding author: Xu Tan, xuta@microsoft.com
1 https://ai-muzic.github.io/meloform/
2 https://github.com/microsoft/muzic
Â© Peiling Lu, Xu Tan, Botao Yu, Tao Qin, Sheng Zhao, Tie-
Yan Liu. Licensed under a Creative Commons Attribution 4.0 Interna-
tional License (CC BY 4.0). Attribution:
Peiling Lu, Xu Tan, Botao
Yu, Tao Qin, Sheng Zhao, Tie-Yan Liu, ÂªMeloForm: Generating Melody
with Musical Form based on Expert Systems and Neural NetworksÂº, in
Proc. of the 23rd Int. Society for Music Information Retrieval Conf.,
Bengaluru, India, 2022.
cal form [1]. This organized structure provided by mu-
sical form can help better express music ideas. For ex-
ample, verse and chorus form is widely used in popular
music. The repetition of verse and chorus sections helps
emphasize music ideas, while the contrast between verse
and chorus can create more emotional intensity. Automatic
melody generation with pre-given musical form based on
purely data driven technology is difficult due to the lack of
labelled data on musical form. Previous work attempt to
generate melody with structure information, but still suf-
fers from the following issues: 1) They generate melodies
with repetitive patterns implicitly either by learning long-
term dependency [2Â±4] or representing the repetition struc-
ture with same harmony, rhythm patterns, etc [5Â±7]. How-
ever, repetitive patterns are far from the exact musical
form. 2) They generate melodies with bar-level structure
explicitly by learning the relationship between bars [8, 9],
but bar-level structure is still not the exact musical form.
3) They generate melodies with repetitive phrases and sec-
tions either by detecting phrase labels with rules and al-
gorithms [10] or using human labelling [11].
But it is
hard for rules or algorithms to detect precise musical form,
and it costs much for hiring people to label this musical
form data. Furthermore, all of these work aim to generate
melodies with some kind of repetitive patterns, but musical
form is constructed to express music ideas by developing
the hierarchical structural units (i.e., motifs, phrases and
sections) with repetitions and variations. Simply repeat-
ing some fragments, or even phrases and sections, with-
out considering the relationships among these hierarchical
structures, is superficial.
Although it is difficult to collect labeled musical form
data through detection algorithm or human labeling, it is
much easier for expert systems to generate melodies with
musical form. However, experts systems may suffer from
monotonous musicality because of handcraft rules.
On
the other hand, neural networks are capable of creating
melodies with rich expressions by learning the data dis-
tribution, but it is hard to precisely control the musical
form. Considering the complementary characteristics of
these two systems, we come up with a method that can
leverage the advantages and make up for the shortcomings.
567

In this paper, we develop MeloForm, a system that gen-
erates melody with musical form using expert systems and
neural networks. The expert system is designed to generate
synthetic melodies with precise musical form. It develops
the motifs to phrases then to sections, which are arranged
by repetitions and variations according to the pre-given
musical form. The encoder-attention-decoder Transformer
based neural network is introduced to refine melodies gen-
erated by expert systems. To improve musical richness
without changing musical form, we propose the refinement
strategy in phrase level, the conditioning on rhythm and
harmony, and the methods for differentiating sections.
MeloForm enjoys the advantages of expert systems and
neural models and avoids their limitations as following: 1)
Comparing with expert systems, we can generate melodies
with better musical richness. 2) Comparing with the mod-
els that implicitly learn the repetitive patterns, we can gen-
erate melodies explicitly with precise musical form con-
trol. 3) Comparing with the models that depend on bar-
level structures, we construct higher-level phrases and sec-
tions structures. 4) Comparing with the models using de-
tected phrases labels or human labels, we have the labeled
musical form data naturally from expert systems with zero
cost and precise accuracy.
The main contributions of this work are as follows:
â€¢ We develop MeloForm, a system that generates melody
with musical form using expert systems and neural net-
works. This system combines the best of white-box ex-
pert system and black-box neural networks for generat-
ing melodies with precise musical form and rich melodic
expression without any labeled data.
â€¢ Experimental
results
demonstrate
that
MeloForm
achieves precise musical form control with 97.79%
accuracy without any labeled data, and outperforms
baseline systems by 0.75, 0.50, 0.86 and 0.89 averagely
in structure, thematic, richness and overall quality in
subjective evaluation.
â€¢ MeloForm can generate melodies with various kinds of
forms, such as verse and chorus form, rondo form, vari-
ational form, sonata form, etc.
2. RELATED WORK
Automatic melody generation evolves from grammar or
statistical based generation [12Â±17] to deep learning em-
powered generation [2, 3, 6, 8Â±10, 18Â±31]. In this section,
we introduce existing neural networks and expert systems
for melody generation with musical structure.
2.1 Neural Networks for Melody Generation
Generating structured melody has attracted more attention
when modeling long music sequence. Previous work ad-
dress this problem as following: 1) They implicitly learn
the long-term dependency or represent repetitive structure
with same musical elements to generate melodies with
repetitive patterns.
Music Transformer [3] introduces a
relative attention mechanism to capture long-term depen-
dency.
Theme Transformer [4] proposes a novel gated
parallel attention module for generation with theme-based
conditioning. Another work [2] presents a hierarchical re-
current neural network to model the note-beat-bar struc-
ture. Other methods [5Â±7] condition the model with same
musical features (e.g., harmony and rhythm patterns) to
represent the repetitive structure. 2) They explicitly model
the bar-level structure for generating melodies one bar af-
ter another. In [9, 32], the authors leverage the bar related
self-similarity matrix to model the relationship between
bars for guiding the melody generation. MELONS [8] con-
structs a bar-level structure graph for generating melodies
with clear bar-level structures.
3) They collect labeled
musical data by detection algorithms for generating rep-
etitions for melodies. Repetitive patterns from melodies
are detected by music analysis algorithms in [33], while
the boundaries of repetitive phrases are recognized in [10].
All of these works can help realize repetitive patterns for
melody in some degree, but they still cannot model precise
musical form.
2.2 Expert Systems for Melody Generation
Back to 18th century, a system called music dice game is
developed for randomly generating music from precom-
posed options [34]. Recently, much work still investigate
rule-based algorithm compositions for specific purpose.
The author in [35] comes up with a rule-based algorithm
to generate melody note sequence, which is constructed
for comparison with the machine learning based compo-
sitions. dMelodies [36] combines the designed latent fac-
tors to create 2-bar melodies for improving data diversity in
disentanglement learning. However, none of them consider
the rules for generating melodies with musical form. Com-
putoser [37] proposes a hybrid probability/rule based algo-
rithm for music composition, which based on rules about
structure, rhythm, repetition, variations, endings, etc. But
it did not provide the methods about how to arrange these
elements with musical form.
And the method for con-
structing a phrase by multiple different motifs may brings
about divergence from music ideas. In [38], repetition of
phrases is also considered at a rhythmical level and con-
cerning pitch intervals, but the melody in each phrase is
generated one note at a time, which is different from a com-
mon composition process by developing a phrase from a
motif. Comparing with these systems, the expert system in
MeloForm generates melodies by considering the musical
form as the hierarchical structure of motifs, phrases and
sections with repetitions and variations, which can better
express music ideas.
3. METHOD
To combine the advantages of precise musical form control
by expert systems and musical richness learning by neu-
ral networks, we develop MeloForm that is shown in Fig-
ure 2(a), which contains two modules: 1) expert systems
for generating synthetic melodies with musical forms; 2)
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
568

25
17
9
1
î‰¢
î‰¢
î‰¢
î‚¤
î‚£
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤î‚¤
î‚¤
î‚¤
î‚£
î‚£
î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤
î‚£
î‚¤î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤
î‚¤
î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î‚¤î‚¤î‚¤
î‚¤
î‚¤
î‚¤î‚¤î‚¤
î‚¤î‚¤
î
î
î
î
î‰¢
î‰¢
î‰¢
î‰¢î‚ƒî‚„
î‰€
î‡§
î‡§
î‡§
î‡§
Figure 1.
The score of the melody part from ÂªMin-
uet in G MajorÂº by the composer Christian Petzold.
This is an example of a melody with musical form as
A(a1, a1)B(b1, b2). We use capital letters (e.g., A, B) to
label sections, while using lowercase (e.g., a1, b1, b2) for
representing phrases. Different phrases in the same section
are labeled with different numbers.
Transformer based neural networks for refining the gener-
ated melodies from expert systems.
3.1 Melody Generation with Expert System
The designed expert system with purely handcraft rules
is inspired by music theory [1], and is shown in Figure
2(b). Given the musical form with hierarchical structure
of sections and phrases, the expert system firstly gener-
ates the motifs based on chord progression and rhythm
patterns, then develops the generated motifs to phrases.
In the phrase-to-section-to-melody development module,
we arrange the phrases in sections and sections in melody
with repetitions and variations according to the given mu-
sical form to generate the synthetic melody with musical
form. For example in Figure 1, given the musical form
A(a1, a1)B(b1, b2), a 2-bar motif in the blue box is devel-
oped into 8-bar phrase a1. Section A is formed by plac-
ing repeated phrase a1 sequentially with some variations.
The similar process is implemented to form section B, in
which different phrases b1 and b2 are developed from dif-
ferent motifs. Then section A and section B are placed
sequentially for getting the final composition.
3.1.1 Motif Generation
Motif is the smallest structural unit for identifying mu-
sic theme, so we create an initial melody in a few bars
(i.e., usually 1-2 bars) to construct the motif. Before the
generation process, we should define the meta information
(i.e., scale, pitch range, tempo and meter) for guiding the
whole generation procedure. Considering note sequences
are composed of pitches and rhythm patterns, we come up
with rule-based algorithms for chord progression genera-
tion to guide pitch selection and for rhythm pattern gener-
ation to create rhythm patterns for the note sequence in mo-
tif. Specifically, the initial pitch of notes is selected from
tones in corresponding chords. Then we add some embel-
lishing tones 3 to decorate the motif melody. Besides, the
interval between adjacent note pitches is constrained to be
not greater than seven semitones to ensure pitch consis-
tency. The detailed algorithms of chord progression gen-
3 http://openmusictheory.com/embellishingTones.
html
Melody Generation 
with Expert System
Melody Refinement 
with Transformer
Synthetic Melody 
with Musical Form
Refined Melody 
with Musical Form
Musical 
Form
(a) Pipeline of MeloForm.
Chord Progression 
Generation
Rhythm Pattern 
Generation
Musical Form
Motif 
Generation
Motif-to-Phrase 
Development
Phrase-to-Section-to-Melody 
Development
Meta Information
Chord Progression
Rhythm Pattern
Synthetic Melody with Musical 
Form
(b) Detailed process of the expert system.
Context Encoder
Melody Decoder
Attention
[sep]
[sep]
ğ‘¥1
[sep]
[sep]
[sep]
[sep]
<s>
ğ‘¥2
ğ‘¥3
ğ‘¥4
ğ‘¥5
ğ‘¥6
ğ‘¦1
ğ‘¦2
ğ‘¦3
ğ‘¦4
ğ‘¦5
ğ‘¦6
ğ‘¦1
ğ‘¦2
ğ‘¦3
ğ‘¦4
ğ‘¦5
ğ‘¦6
ğ‘1
</s>
ğ‘¦7
[sep]
[sep]
ğ‘¦8
ğ‘¦9
ğ‘¦10 ğ‘¦11 ğ‘¦12
ğ‘1
ğ‘2
ğ‘1
ğ‘1
ğ‘1
ğ‘1
ğ‘1
(c) Architecture of melody refinement neural networks.
The in-
put contains four phrases from a melody with musical form as
A(a1, a1)B(b1, b2). In each phrase, x represents the conditioning infor-
mation (i.e., rhythm, chord and cadence) for each note, while y represents
the melody information (i.e., rhythm and pitch) for each note. [sep] in-
dicates the boundaries between phrases, while <s> and </s> indicates the
beginning and end of phrases.
Figure 2. Architecture of MeloForm.
eration and rhythm pattern generation can be referred in
Supplementary Materials Section 1 and 2.
3.1.2 Motif-to-Phrase Development
A motif is a unit for identifying the music theme, but it is
not a complete music expression. Thus, we need to develop
it into a phrase, which is the smallest structural unit for ex-
pressing complete music ideas. In order to develop motif
into phrase, we introduce three basic categories of devel-
opment strategies: sequence 4 , transformation and ending.
These basic development strategies can be combined with
each other to create compound development strategies. For
example, in Figure 1, the 2-bar motif in the blue box is de-
veloped by sequence (i.e., 3-4 bars), transformation (i.e.,
5-6 bars) and ending (i.e., 7-8 bars) to form 8-bar phrase
a1. The detailed description of each development strategy
can be referred in Supplementary Materials Sections 3.
There may exist multiple different phrases in each sec-
tion, each of which has its own motif. Too much different
motifs in each section may distract listeners from the main
ideas. Thus, we present another way to generate a new
motif similar to already generated one for a new phrase.
Specifically, we can either directly copy the already gen-
erated motif or borrow its rhythm patterns to create a new
motif, which can help build some relationships between
4 https://en.wikipedia.org/wiki/Sequence_
(music)
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
569

phrases in the same section. For example, in Figure 1, the
rhythm patterns from the motif in the orange box of phrase
b2 is borrowed from the motif in the pink box of phrase b1
with a few variations.
3.1.3 Phrase-to-Section-to-Melody Development
After generating all phrases, we need to arrange them
with repetitions and variations to form sections, which are
higher-level structural units. And sections are then ordered
sequentially to finish the composition. As shown in Fig-
ure 1, given musical form as A(a1, a1)B(b1, b2), repeated
phrases a1 with variations are placed in order to form sec-
tion A. Phrase b1 and b2 are placed in order to form section
B. At last, section A and section B are arranged sequen-
tially to get the final composition.
Directly generating new motifs when building different
sections is not always musically expressive, so we intro-
duce another method for establishing not only similar but
also contrastive motifs. To establish the similarity, we can
borrow one of the random fragment from phrases in other
sections. To form a contrast, we can adjust the rhythm pat-
terns and pitch selection in desired section to change the
tension. For example, in Figure 1, the rhythm patterns of
the motif in the pink box from phrase b1 is borrowed from
that of the third bar from phrase a1, which is not the exact
motif for phrase a1. And to form a contrast, the pitch is
selected from higher range for more tension.
3.2 Melody Refinement with Transformer
Melodies generated by the expert system are guaranteed to
have precise musical form, but are lack of musical richness
since they are generated by handcraft rules. Thus, in this
section, we introduce an encoder-attention-decoder Trans-
former that takes the synthetic melodies as input and out-
puts refined melodies with better musical richness. How-
ever, the refinement is challenging since it needs to keep
the musical form unchanged while improving the musi-
cal richness. Due to the lack of controllability of neural
networks, directly refining the whole melody without any
constraints is easy to lose musical form information. To
address this challenge, we describe several design princi-
ples in our Transformer model for refinement:
â€¢ Refining melody phrases by phrases.
Since musical
phrase is the smallest structural unit for a complete mu-
sical expression, it is better to refine the melody phrase
by phrase in an iterative process, instead of refining the
whole melody in a single time that fails to maintain the
musical form from the expert system. However, only re-
fining one phrase at a time is hard to model the repetitive
patterns from similar phrases. Therefore, we should re-
fine similar phrases at each iteration step.
â€¢ Refining phrases by conditioning on rhythm and har-
mony. Directly generating the refined musical phrases
from scratch may lose bottom-level music structure
determined by rhythm patterns and harmony.
Thus,
we should provide explicit condition and control with
rhythm and harmony to guide the refine process.
â€¢ Refining phrases by considering their differentiations
among different sections.
When composers need to
distinguish different sections (e.g., verse and chorus)
in melodies, they usually change pitch distribution or
rhythms patterns for building up contrasted tension. Ac-
cordingly, to maintain such contrast between sections,
we need to consider these differentiations in the refine-
ment process.
Based on these design principles, our model architec-
ture is shown in Figure 2(c). We introduce the implemen-
tations corresponding to each design principle as follows.
3.2.1 Iterative Phrase Refinement
According to the first design principle, we refine the simi-
lar phrases in each iterative step. To train the Transformer
model with iterative refinement capability, we first mask
similar phrases of the melody in the training data and take
the masked melody as the input of the encoder, and gen-
erate original phrases corresponding to the masked posi-
tions with the decoder, like the masked sequence to se-
quence (MASS) task in [39]. After model training, the
Transformer model is used to refine the similar phrases in
synthetic melodies from expert systems in an iterative way.
Specifically, if we want to refine the melody with musical
form as A(a1, a1)B(b1, b2), we can refine phrases a1 for
the first iteration and replace the two original phrases with
the refined versions, and then refine the phrase b2 based on
the refined melody. This iteration step is finished until all
phrases are refined.
3.2.2 Condition with Rhythm and Harmony
The phrases are developed from motif by sequence strat-
egy that results in similar rhythm patterns within some
bars. Besides, harmony plays an important role to con-
trol pitch distribution in phrase development.
Thus, to
maintain these music structures in refinement, we condi-
tion the model with rhythm patterns, chord progression and
cadence to encourage the model to only refine the pitch of
the phrase. Specifically, instead of replacing the masked
phrases with masked symbols in MASS [39], we replace
them with the corresponding rhythm, chord and cadence
symbols, shown as x in Figure 2(c).
3.2.3 Differentiation between Sections
We differentiate the phrases in different sections into as-
pects: 1) Controlling rhythm patterns in the expert system.
Rhythm patterns can be adjusted directly in expert system
by increasing/decreasing note density, prolong/shrink note
length, etc. It is common that larger note density or faster
tempo can bring about more intensity. 2) Controlling the
pitch distribution in the Transformer model. Although ex-
pert systems can control the pitch distribution by selecting
pitch from different ranges, it is hard for neural networks
to control in the same way. Thus, we insert an ÂªAVG-
PITCHÂº token at the beginning of the condition from each
target phrase to represent the average pitch of this phrase,
and another ÂªSPANÂº token to indicate the pitch span (i.e.,
the difference between the maximum pitch with minimum
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
570

Structureâ†‘
Thematicâ†‘
Richnessâ†‘
Overallâ†‘
Dataset
LMD [40]
3.18 (Â±0.64)
3.07 (Â±0.58)
3.14 (Â±0.40)
3.00 (Â±0.57)
POP909 [41]
4.06 (Â±0.49)
3.83 (Â±0.54)
4.00 (Â±0.61)
4.11 (Â±0.46)
Method
Music Transformer [3]
3.00 (Â±0.76)
3.21 (Â±0.74)
2.11 (Â±0.46)
2.32 (Â±0.54)
MELONS [8]
3.18 (Â±0.64)
3.07 (Â±0.58)
2.64 (Â±0.64)
2.89 (Â±0.52)
POP909_lm
2.61 (Â±0.62)
2.93 (Â±0.60)
2.96 (Â±0.45)
2.96 (Â±0.49)
MeloForm
3.68 (Â±0.35)
3.57 (Â±0.36)
3.43 (Â±0.43)
3.61 (Â±0.34)
Table 1. Subjective evaluation results, with mean opinion scores and 95% confidence interval for each metric.
pitch). Higher average pitch and more pitch span can build
up much more tension.
4. EXPERIMENTAL RESULTS
In this section, we first describe the dataset and system con-
figurations. Next, we show the main results compared with
baseline systems. Then we implement some method anal-
ysis to further validate the effectiveness of each design. Fi-
nally we implement some extensions to demonstrate the
scalability of this system.
Music samples generated by
MeloForm are available via this link 1 . Our code and Sup-
plementary materials are available via this link 2 .
4.1 Experiment Setup
Dataset. We utilize the LMD-matched MIDI dataset [40]
in the training stage of the Transformer based neural net-
works for melody refinement. We select melodies in 4/4
time signature, and normalize them to the tonality of ÂªC
majorÂº or ÂªA minorÂº.
At last, we obtain 30,218 MIDI
samples. This dataset contains 471,058 phrases, in which
there exists 100,948 distinct set of similar phrases based on
our phrase boundary detection and similarity calculation,
of which the detailed algorithm can be found in Supple-
mentary Materials Section 4.
System Configurations. The encoder-attention-decoder
Transformer has 4 encoder and decoder layers, both of
which contains 4 attention heads. The hidden size is set
as 256. We use Adam optimizer [42] with the learning rate
is 0.0005. The dropout rate is set as 0.2 during training.
There contains maximum of 4,096 tokens in each batch.
The dataset is randomly splitted into training/valid/test set
with the ratio of 0.8, 0.1, and 0.1, respectively. We use
nucleus sampling [43] with the p set as 0.9.
Subjective Evaluation Metrics. For subjective listening,
we invite 10 participants to evaluate 30 samples. There are
five randomly selected samples from MeloForm and base-
lines described in Section 4.2. For each listener, they are
randomly ordered to avoid perceptual bias and habituation
effect. The rating is based on five-point scale. We define
the following four metrics: 1) Structure: Does this melody
have complete structure with repetitions and variations? 2)
Thematic: Can you figure out the musical theme? 3) Rich-
ness: Is the melody similarly melodious as human compo-
sitions? 4) Overall: Is the overall quality of the melody the
same as human compositions?
4.2 Main Results
We compare MeloForm with the following baselines: 1)
Music Transformer [3], an end-to-end neural network so-
lution which introduces the relative attention to learn long-
term repetitive patterns; 2) MELONS [8], a Linear Trans-
former and structure graph based framework for generat-
ing melody with bar-level structure; 3) POP909_lm, a lan-
guage model implemented by ourself for leveraging the
phrase labels in POP909 [41] to generate melodies given
musical form, whose details can be found in Supplemen-
tary section 6. These baselines share the same system con-
figurations with MeloForm for fair comparison.
Table 1 shows the subjective evaluation results in the
mean opinion scores (MOS) and 95% confidence interval
for these four metrics over all experimental groups. Com-
paring with baseline systems, MeloForm outperforms all
of them in thematicness, structureness, richness and overall
quality. We calculate the controlling accuracy of phrase la-
bels for POP909_lm to verify if same phrase labels results
in similar melodies. The high accuracy of 100% demon-
strates it is capable of generating corresponding repetitive
melodies for same phrase labels. It is worth noticing that
although POP909_lm can realize the repetitions of phrases,
it still fails to provide good listening experience compar-
ing with MeloForm. This demonstrate simply repeating
phrases is still deficient. Besides, the less 95% confidence
interval of MeloForm provides more confidence for its bet-
ter performance. We also compare MeloForm with human
composed data from the LMD-matched MIDI dataset and
POP909. MeloForm shows better performance compared
with LMD-matched MIDI dataset, since the samples from
LMD-matched MIDI dataset are collected from various
kinds of publicly-available sources on the internet, which
is hard to guarantee the production quality. In contrast,
POP909 is constructed by hiring professional musicians
for creating the samples and reviewing the whole genera-
tion process, which ensures the quality to be the same as re-
alistic compositions. The comparing results with POP909
reveal that there still exist gaps between melodies gener-
ated by MeloForm with the human composed ones.
4.3 Method Analysis
Controllability Study. We calculate the controlling ac-
curacy of musical form by calculating the similarity score
between generated melodies from same phrases. The accu-
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
571

0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
vs. Setting 1
vs. Setting 2
vs. Setting 3
vs. Setting 4
vs. Setting 5
vs. Setting 6
Prefer MeloForm
No Preference
Prefer MeloForm in Setting #
Figure 3. Preference distribution for ablation study, which
compares the melodies generated by MeloForm with that
from modified system (i.e., MeloForm in Setting #).
racy of 97.79% indicates MeloForm can generate melodies
with precise musical form control. We also calculate the
accuracy to verify if the pitch distribution is well controlled
by average pitch and pitch span. The accuracy of 92.5% in
average pitch controlling and 99.71% in pitch span control-
ling demonstrate its controllability to differentiate sections
in pitch dimension.
Ablation Study.
We conduct preference tests to verify
the contribution of the components in MeloForm with the
following settings: 1) Setting 1: w/o development strate-
gies from expert systems. Notes are randomly arranged
in expert systems without any development strategies. 2)
Setting 2: w/o expert systems.
Melodies are first gen-
erated for each phrase by neural networks, and then got
copied directly with the given musical form for repetition
in the same phrase. 3) Setting 3: w/o neural networks.
We remove melody refinement neural networks. 4) Setting
4: w/o fine-grained rhythm pattern condition. The condi-
tion of rhythm patterns changed from fine-grained level to
coarse-grained level. 5) Setting 5: w/o refinement strategy.
We directly copy the generate melodies from each phrase
to the following same phrases. 6) Setting 6: w/o section
differentiation. The ÂªAVGPITCHÂº and ÂªSPANÂº tokens are
removed from the beginning of each phrase. The partici-
pants are required to compare the samples from MeloForm
and these settings and give their preference score.
The preference distribution for the above settings is
shown in Figure 3.
We derived some observations in
the following: 1) More preferences over setting 1 and 2
demonstrate the effectiveness of the whole expert system
and the development strategies for building up phrases. 2)
More preferences over setting 3, 4, 5, and 6 validate the
efficacy of the whole neural refinement model, the rhythm
pattern condition, the iterative refinement strategy, and the
section differentiation method.
4.4 Extensions to More Musical Forms
In this section, we illustrate the principles to generate
melodies with four different musical forms (i.e., Verse and
Chorus, Rondo, Variational and Sonata form) based on
MeloForm. The examples from these extensions are shown
in the demo page.
Verse and Chorus Form is widely used in popular music.
It contains two contrasting sections: verse section (i.e., A)
and chorus section (i.e., B), which can be arranged in var-
ious kinds of ways, such as AABAAB, ABA, or AABB.
Users can follow the method in Section 3.1 to build the
verse and chorus sections, and leverage the section differ-
entiation in Section 3.2.3 to change the tension. For ex-
ample, Chorus tends to have more intensive emotional ex-
pression. To achieve this, we can control the average pitch
to higher level or increase the pitch span for this section.
Rondo Form is a musical form where the refrained section
alternates with contrasting sections, such as ABACADA.
Melody of Rondo Form can be generated by leveraging the
method in Section 3.1 to construct the refrained section and
contrasting sections, and arrange them sequentially based
on the given Rondo Form.
Variational Form is a musical form where the main sec-
tion is followed by its variations. We represent Variational
Form as AAâ€²Aâ€²â€² to describe the main section and its varia-
tions. To address the challenge of deriving variations from
the main section, we can treat the main section as a mo-
tif, and use the development strategies in 3.1.2 to develop
this longer motif into another higher-level section. Another
way is to generate melodies with same sections like AAA
by expert systems, and predict the melodies from each sec-
tion in different iteration steps. There are many other ways
for creating the Variational Form, so users are encouraged
to provide their own creative design.
Sonata Form is generally consisted of three sections: an
exposition, a development, and a recapitulation, which can
be represented as ABAâ€² in our system. For the exposition,
we generate two phrases with contrasting motifs, where
the second phrase is a transposition of the first one by con-
trolling the tonality token when refining the second phrase.
The development section is a variation of the exposition,
which can be generated by leveraging the methods for con-
structing variational form. The recapitulation is a repeti-
tion of the exposition, in which the second phrase should
go back to the tonic key by controlling the tonality token
in condition from neural networks.
5. CONCLUSIONS
In this paper, we propose MeloForm, a system to generate
melody with musical form based on expert systems and
neural networks. It combines the advantages from expert
systems to precisely control musical form and neural net-
works to refine melody for better musical richness without
changing musical form. Experimental results demonstrate
MeloForm achieves 97.79% accuracy in musical form con-
trol, and outperforms baseline systems in structure, the-
matic,richness and overall quality in terms of subjective
evaluation. We will release the dataset of the synthetic
melodies generated by our expert system and the refined
version from our Transformer model to facilitate the future
research on music form modeling. Furthermore, we will
explore the generation of intro, outro and bridge, and in-
vestigate the arrangement generation with musical form to
accomplish a complete music composition.
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
572

6. REFERENCES
[1] W. E. Caplin and W. E. Caplin, Analyzing classical
form: an approach for the classroom.
Oxford Uni-
versity Press, 2013.
[2] J. Wu, C. Hu, Y. Wang, X. Hu, and J. Zhu, ÂªA hier-
archical recurrent neural network for symbolic melody
generation,Âº IEEE transactions on cybernetics, vol. 50,
no. 6, pp. 2749Â±2757, 2019.
[3] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer,
I. Simon, C. Hawthorne, A. M. Dai, M. D. Hoff-
man, M. Dinculescu, and D. Eck, ÂªMusic transformer,Âº
arXiv preprint arXiv:1809.04281, 2018.
[4] Y.-J. Shih, S.-L. Wu, F. Zalkow, M. Muller, and Y.-H.
Yang, ÂªTheme transformer: Symbolic music genera-
tion with theme-conditioned transformer,Âº IEEE Trans-
actions on Multimedia, 2022.
[5] K. Chen, W. Zhang, S. Dubnov, G. Xia, and W. Li,
ÂªThe effect of explicit structure encoding of deep neu-
ral networks for symbolic music generation,Âº in 2019
International Workshop on Multilayer Music Repre-
sentation and Processing (MMRP).
IEEE, 2019, pp.
77Â±84.
[6] Z. Ju, P. Lu, X. Tan, R. Wang, C. Zhang, S. Wu,
K. Zhang, X. Li, T. Qin, and T.-Y. Liu, ÂªTelemelody:
Lyric-to-melody generation with a template-based
two-stage method,Âº arXiv preprint arXiv:2109.09617,
2021.
[7] X.
Zhang,
J.
Zhang,
Y.
Qiu,
L.
Wang,
and
J. Zhou,
ÂªStructure-enhanced pop music genera-
tion via harmony-aware learning,Âº arXiv preprint
arXiv:2109.06441, 2021.
[8] Y. Zou, P. Zou, Y. Zhao, K. Zhang, R. Zhang, and
X. Wang, ÂªMelons:
generating melody with long-
term structure using transformers and structure graph,Âº
arXiv preprint arXiv:2110.05020, 2021.
[9] J. Wu, X. Liu, X. Hu, and J. Zhu, ÂªPopmnet: Gener-
ating structured pop music melodies using neural net-
works,Âº Artificial Intelligence, vol. 286, p. 103303,
2020.
[10] S. Dai, Z. Jin, C. Gomes, and R. B. Dannenberg,
ÂªControllable deep melody generation via hierarchi-
cal music structure representation,Âº arXiv preprint
arXiv:2109.00663, 2021.
[11] J. Zhao and G. Xia, ÂªAccomontage: Accompaniment
arrangement via phrase selection and style transfer,Âº
arXiv preprint arXiv:2108.11213, 2021.
[12] H. Young, ÂªA categorial grammar for music and its use
in automatic melody generation,Âº in Proceedings of the
5th ACM SIGPLAN International Workshop on Func-
tional Art, Music, Modeling, and Design, 2017, pp. 1Â±
9.
[13] D. Quick and P. Hudak, ÂªGrammar-based automated
music composition in haskell,Âº in Proceedings of the
first ACM SIGPLAN workshop on Functional art, mu-
sic, modeling & design, 2013, pp. 59Â±70.
[14] M. Kikuchi and Y. Osana, ÂªAutomatic melody gen-
eration considering chord progression by genetic al-
gorithm,Âº in 2014 Sixth World Congress on Nature
and Biologically Inspired Computing (NaBIC 2014).
IEEE, 2014, pp. 190Â±195.
[15] A. Garay Acevedo, ÂªFugue composition with counter-
point melody generation using genetic algorithms,Âº in
International symposium on computer music modeling
and retrieval.
Springer, 2004, pp. 96Â±106.
[16] K. Wakui, Y. Hatori, and Y. Osana, ÂªAutomatic melody
generation considering chord progression using ge-
netic algorithm,Âº IEICE Proceedings Series, vol. 48,
no. B2L-E-7, 2016.
[17] M. Takano and Y. Osana, ÂªAutomatic melody genera-
tion considering userâ€™s evaluation using interactive ge-
netic algorithm,Âº in Asia-Pacific Conference on Simu-
lated Evolution and Learning.
Springer, 2014, pp.
785Â±797.
[18] Z. Guo, M. Dimos, and H. Dorien, ÂªHierarchi-
cal recurrent neural networks for conditional melody
generation with long-term structure,Âº arXiv preprint
arXiv:2102.09794, 2021.
[19] Y. Yu, A. Srivastava, and S. Canales, ÂªConditional
lstm-gan for melody generation from lyrics,Âº ACM
Transactions on Multimedia Computing, Communica-
tions, and Applications (TOMM), vol. 17, no. 1, pp.
1Â±20, 2021.
[20] A. Mishra, K. Tripathi, L. Gupta, and K. P. Singh,
ÂªLong short-term memory recurrent neural network ar-
chitectures for melody generation,Âº in Soft Computing
for Problem Solving.
Springer, 2019, pp. 41Â±55.
[21] Y. Yu, F. HarscoÃ«t, S. Canales, G. Reddy M, S. Tang,
and J. Jiang, ÂªLyrics-conditioned neural melody gen-
eration,Âº in International Conference on Multimedia
Modeling.
Springer, 2020, pp. 709Â±714.
[22] L.-C. Yang, S.-Y. Chou, and Y.-H. Yang, ÂªMidinet:
A convolutional generative adversarial network for
symbolic-domain music generation,Âº arXiv preprint
arXiv:1703.10847, 2017.
[23] S. Li, S. Jang, and Y. Sung, ÂªAutomatic melody com-
position using enhanced gan,Âº Mathematics, vol. 7,
no. 10, p. 883, 2019.
[24] Z. Sheng, K. Song, X. Tan, Y. Ren, W. Ye, S. Zhang,
and T. Qin, ÂªSongmass: Automatic song writing with
pre-training and alignment constraint,Âº in Proceed-
ings of the AAAI Conference on Artificial Intelligence,
vol. 35, no. 15, 2021, pp. 13 798Â±13 805.
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
573

[25] Y.-S. Huang and Y.-H. Yang, ÂªPop music transformer:
Beat-based modeling and generation of expressive pop
piano compositions,Âº in Proceedings of the 28th ACM
International Conference on Multimedia, 2020, pp.
1180Â±1188.
[26] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret,
ÂªTransformers are rnns: Fast autoregressive transform-
ers with linear attention,Âº in International Conference
on Machine Learning.
PMLR, 2020, pp. 5156Â±5165.
[27] W.-Y. Hsiao, J.-Y. Liu, Y.-C. Yeh, and Y.-H. Yang,
ÂªCompound word transformer: Learning to compose
full-song music over dynamic directed hypergraphs,Âº
arXiv preprint arXiv:2101.02402, 2021.
[28] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and
R. Salakhutdinov, ÂªTransformer-xl: Attentive language
models beyond a fixed-length context,Âº arXiv preprint
arXiv:1901.02860, 2019.
[29] X. Wu, C. Wang, and Q. Lei, ÂªTransformer-xl based
music generation with multiple sequences of time-
valued notes,Âº arXiv preprint arXiv:2007.07244, 2020.
[30] C. Zhang, L. Chang, S. Wu, X. Tan, T. Qin, T.-
Y. Liu, and K. Zhang, ÂªRelyme: Improving lyric-to-
melody generation by incorporating lyric-melody rela-
tionships,Âº arXiv preprint arXiv:2207.05688, 2022.
[31] A. Lv, X. Tan, T. Qin, T.-Y. Liu, and R. Yan, ÂªRe-
creation of creations: A new paradigm for lyric-to-
melody generation,Âº arXiv e-prints, pp. arXivÂ±2208,
2022.
[32] H. Jhamtani and T. Berg-Kirkpatrick, ÂªModeling self-
repetition in music generation using generative adver-
sarial networks,Âº in Machine Learning for Music Dis-
covery Workshop, ICML, 2019.
[33] D. Herremans and E. Chew, ÂªMorpheus: automatic
music generation with recurrent pattern constraints and
tension profiles,Âº in Proceedings of IEEE TENCON,-
2016 IEEE Region 10 Conference.
IEEE, 2016, pp.
282Â±285.
[34] S. A. Hedges, ÂªDice music in the eighteenth century,Âº
Music & Letters, vol. 59, no. 2, pp. 180Â±187, 1978.
[35] P. Wiriyachaiporn, K. Chanasit, A. Suchato, P. Pun-
yabukkana, and E. Chuangsuwanich, ÂªAlgorithmic
music composition comparison,Âº in 2018 15th Inter-
national Joint Conference on Computer Science and
Software Engineering (JCSSE).
IEEE, 2018, pp. 1Â±6.
[36] A. Pati, S. Gururani, and A. Lerch, Âªdmelodies: A
music dataset for disentanglement learning,Âº arXiv
preprint arXiv:2007.15067, 2020.
[37] B. Bozhanov, ÂªComputoser-rule-based, probability-
driven algorithmic music composition,Âº arXiv preprint
arXiv:1412.3079, 2014.
[38] A. Elowsson and A. Friberg, ÂªAlgorithmic composition
of popular music,Âº in The 12th international confer-
ence on music perception and cognition and the 8th tri-
ennial conference of the European society for the cog-
nitive sciences of music, 2012, pp. 276Â±285.
[39] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, ÂªMass:
Masked sequence to sequence pre-training for lan-
guage generation,Âº in International Conference on Ma-
chine Learning.
PMLR, 2019, pp. 5926Â±5936.
[40] C. Raffel, Learning-based methods for comparing se-
quences, with applications to audio-to-midi alignment
and matching.
Columbia University, 2016.
[41] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai,
X. Gu, and G. Xia, ÂªPop909: A pop-song dataset
for music arrangement generation,Âº arXiv preprint
arXiv:2008.07142, 2020.
[42] D. P. Kingma and J. Ba, ÂªAdam: A method for stochas-
tic optimization,Âº arXiv preprint arXiv:1412.6980,
2014.
[43] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi,
ÂªThe curious case of neural text degeneration,Âº arXiv
preprint arXiv:1904.09751, 2019.
Proceedings of the 23rd ISMIR Conference, Bengaluru, India, December 4-8, 2022
574
