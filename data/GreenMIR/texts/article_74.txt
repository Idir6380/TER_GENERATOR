SCALE- AND RHYTHM-AWARE MUSICAL NOTE ESTIMATION FOR
VOCAL F0 TRAJECTORIES BASED ON A SEMI-TATUM-SYNCHRONOUS
HIERARCHICAL HIDDEN SEMI-MARKOV MODEL
Ryo Nishikimi1 Eita Nakamura1 Masataka Goto2 Katsutoshi Itoyama1 Kazuyoshi Yoshii1,3
1Graduate School of Informatics, Kyoto University, Japan
3RIKEN AIP, Japan
2National Institute of Advanced Industrial Science and Technology (AIST), Japan
{nishikimi, enakamura, itoyama, yoshii}@sap.ist.i.kyoto-u.ac.jp, m.goto@aist.go.jp
ABSTRACT
This paper presents a statistical method that estimates a se-
quence of musical notes from a vocal F0 trajectory. Since
the onset times and F0s of sung notes are considerably de-
viated from the discrete tatums and pitches indicated in a
musical score, a score model is crucial for improving time-
frequency quantization of the F0s. We thus propose a hier-
archical hidden semi-Markov model (HHSMM) that com-
bines a score model representing the rhythms and pitches
of musical notes with musical scales with an F0 model rep-
resenting time-frequency deviations from a note sequence
speciï¬ed by a score. In the score model, musical scales
are generated stochastically. Note pitches are then gener-
ated according to the scales and note onsets are generated
according to a Markov process deï¬ned on the tatum grid.
In the F0 model, onset deviations, smooth note-to-note F0
transitions, and F0 deviations are generated stochastically
and added to the note sequence. Given an F0 trajectory,
our method estimates the most likely sequence of musical
notes while giving more importance on the score model
than the F0 model. Experimental results showed that the
proposed method outperformed an HMM-based method
having no models of scales and rhythms.
1. INTRODUCTION
Singing voice analysis is important for music information
retrieval because a singing voice usually forms a large part
of the melody line of popular music, and provides much in-
formation about music. Singing voice analysis techniques
such as vocal F0 estimation [1,3,7,9,14] and singing voice
separation [8, 12] have actively been studied and applied
to singer identiï¬cation [10, 22], karaoke generation [19],
query-by-humming [8], and active music listening [6]. To
leverage musical information conveyed by singing voices,
it is helpful to convert a vocal F0 trajectory to a musical
score containing only discrete symbols.
câƒRyo Nishikimi, Eita Nakamura, Masataka Goto, Kat-
sutoshi Itoyama, Kazuyoshi Yoshii.
Licensed under a Creative Com-
mons Attribution 4.0 International License (CC BY 4.0). Attribution:
Ryo Nishikimi, Eita Nakamura, Masataka Goto, Katsutoshi Itoyama,
Kazuyoshi Yoshii. â€œScale- and Rhythm-Aware Musical Note Estimation
for Vocal F0 Trajectories Based on a Semi-Tatum-Synchronous Hierar-
chical Hidden Semi-Markov Modelâ€, 18th International Society for Mu-
sic Information Retrieval Conference, Suzhou, China, 2017.





	

	

	


	

			

 	

	
	

	
Figure 1: The generative process of a vocal F0 trajectory
based on a hierarchical hidden semi-Markov model involv-
ing a score model and an F0 model.
In this study, we tackle musical note estimation for vo-
cal F0 trajectories that tend to have large deviations from
original musical scores. The pitches and onset times of
musical notes in a musical score can take only discrete val-
ues, whereas an F0 trajectory is a continuous signal that
can dynamically and smoothly vary over time. F0 trajecto-
ries are modulated by vibrato and changes smoothly from
one note to another by a portamento. Naive time-frequency
quantization of an F0 trajectory therefore outputs a note se-
quence that often includes statistically-rare chromatic note
progressions with unlikely rhythms.
To solve this problem, we propose a statistical method
of scale- and rhythm-aware musical note estimation based
on integration of a score model describing the process of
generating a note sequence and an F0 model describing the
process of generating an F0 trajectory from the note se-
quence (Fig. 1). In the score model, a sequence of musical
scales (local keys) is determined by a Markov process and
the semitone-level pitch of each note is then determined ac-
cording to both a scale of the note position and the pitch of
a previous note. The onset position of each note on a tatum
grid is determined according to that of a previous note
to make rhythmic structures. In the F0 model, the time-
frequency deviations are added to a step-function-shaped
F0 trajectory corresponding to a musical score given by
the score model. The integrated model is thus formulated
as a hierarchical hidden semi-Markov model (HHSMM).
Given a vocal F0 trajectory with a tatum grid, the scales,
musical notes, and F0 deviations, which are all latent vari-
ables of the proposed model, are jointly estimated by us-
376

ing a Markov chain Monte Carlo algorithm. A key feature
of our method is that musical scales and rhythms work as
self-organizing constraints on time-frequency quantization
of vocal F0 trajectories.
2. RELATED WORK
In this section, we introduce related work on the analysis
of singing voices.
2.1 Vocal F0 Estimation for Music Audio Signals
Estimation of vocal F0 trajectories for music audio signals
has actively been studied [1, 3, 7, 9, 14], and the outputs
of these methods can be used as inputs of our method.
One of the most basic method is subharmonic summation
(SHS) [7] that calculates the sum of the harmonic compo-
nents of each candidate F0. Ikemiya et al. [9] improved
F0 estimation based on SHS and singing voice separation
based on robust principle component analysis (RPCA) [8]
by using the mutual dependency of those two tasks. Sala-
mon et al. [21] estimated contours of the melody F0 can-
didates by calculating a salience function and then recur-
sively removed contours which do not form a melody line
by using the characteristics of each contour. Durrieu et
al. [3] extracted a main melody by representing accompa-
niments with a model inspired by non-negative matrix fac-
torization (NMF) and leading voices with a source-ï¬lter
model. Mauch et al. [14] modiï¬ed the YIN [1] in a prob-
abilistic way so that the modiï¬ed system could determine
multiple candidate fundamental frequencies and then se-
lect one at each frame by using an HMM.
2.2 Musical Note Estimation for Singing Voices
Estimation of musical notes from sung melody has been
a hot research topic [6, 11, 13, 15, 17, 18, 20, 23]. A naive
method is to take the majority of vocal F0s in each interval
of a regular grid [6]. Paiva et al. [17] proposed a cascad-
ing method based on multipitch detection, multipitch tra-
jectory construction, segmentation of multipitch trajectory,
elimination of irrelevant notes, and extraction of notes that
form a main melody. Raphael [18] proposed an HMM-
based method that estimates pitches, rhythms, and tempos
when the number of notes is given. The rhythm and on-
set deviation models used in [18] are similar to those used
in our method. Laaksonen et al. [11] divided audio data
into segments corresponding to scales and notes by focus-
ing on the boundaries of chords given as input, and inde-
pendently estimated the notes based on a score function.
RyynÂ¨anen et al. [20] proposed a method based on a hierar-
chical HMM in order to capture the different kinds of vocal
ï¬‚uctuations (e.g., vibrato and portamento) within one note.
In this model, the transition between pitches is represented
in the upper-level HMM and the transition between the
vocal ï¬‚uctuations is represented in the lower-level HMM.
Molina et al. [15] focused on the hysteresis characteris-
tics of vocal F0s. Nishikimi et al. [16] proposed a method
based on an HHM that represents the generative process of
a vocal F0 trajectory considering the time and frequency
deviations. Yang et al. [23] proposed a method based on
a hierarchical HMM that represents the generative process
of the f0-âˆ†f0 plane. Mauch et al. [13] developed a soft-
ware tool called Tony for extracting pitches. In this tool, a
vocal F0 trajectory is estimated by PYIN [14], and musical
notes are estimated by a modiï¬ed version of RyynÂ¨anenâ€™s
method [20].
3. PROPOSED METHOD
This section explains the proposed method of estimating a
sequence of musical notes from a vocal F0 trajectory. The
method is based on an HHSMM (Fig. 1) that stochastically
generates the F0 trajectory with time-frequency deviations
from a sequence of musical notes depending on musical
scales. The upper part of the proposed model is an HMM
that stochastically generates a sequence of musical notes
according to the scales that are assigned to bars. The lower
part is an HSMM that represents the musical notes and
temporal deviations as latent variables and the frequency
deviations as F0 emission probabilities.
3.1 Problem Speciï¬cation
The problem we tackle is deï¬ned as follows:
Input: A vocal F0 trajectory X= {xt}T
t=1 and 16th-note-
level tatums Y = {(un, vn)}N
n=0,
Output: A sequence of notes Z= {zj=(pj, lj)}J
j=0,
where T is the number of frames in a vocal F0 trajectory, xt
is a log frequency at time t, and N is the number of 16th-
note-level tatums. un âˆˆ{1, . . ., T+1} is the time of tatum
n and the beginning and end of music are represented as
u0 = 1 and uN = T+1, respectively. vn âˆˆ{0, . . ., 15} is
the relative position of tatum n in a bar. J is the number of
musical notes estimated by proposed methods, and the j-th
note zj is represented as a pair consisting of an pitch pj âˆˆ
{1, . . . , K} and a note length lj âˆˆ{1, . . . , L} in the unit of
tatums, where K is the number of kinds of semitone-level
pitches, and pj indicates any one in {Âµ1, . . . , ÂµK}, which
is a set of log frequencies corresponding to semitone-level
pitches. For convenience we introduce the initial note z0
that does not appear in the actual score.
3.2 Probabilistic Modeling of Musical Scores
This section describes the score model constructed with an
HMM that represents rhythms and pitches of musical notes
under musical scales.
3.2.1 Modeling Scale Transitions
Scales are represented as S = {sm}M
m=0, where M de-
notes the number of bars in the musical piece and sm de-
notes the scale at the m-th bar. For convenience, we intro-
duce the initial bar s0 to which the initial note z0 belongs.
Instead of ï¬xing one scale for the whole piece, the scale
is allowed to change at bar lines. Each scale sm takes one
of the 24 values of {C, C#, Â· Â· Â· , B} Ã— {major, minor}. The
latent variables S are described by a Markov chain as
p(s0|Ï€) = Ï€s0,
(1)
p(sm|smâˆ’1, Î¾smâˆ’1) = Î¾smâˆ’1sm,
(2)
where Ï€âˆˆR24
â‰¥0 is a set of initial probabilities and Î¾sâˆˆR24
â‰¥0
is a set of transition probabilities.
Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017
377














	

    




	

 
	
	
	
	
						
Figure 2: Overview of the score model.

	














(a) Temporal deviations

	





(b) Frequency deviations
Figure 3: Deviations in a vocal F0 trajectory.
3.2.2 Modeling Pitch Transitions
The sequence of pitches P is generated by a Markov chain
depending on scales S as follows (Fig. 2):
p(p0|s0, Ï•s0) = Ï•s0p0,
(3)
p(pj|pjâˆ’1, sm, Ïˆsmpjâˆ’1) = Ïˆsmpjâˆ’1pj,
(4)
where Ï•sâˆˆRK
â‰¥0 is a set of initial probabilities, ÏˆspâˆˆRK
â‰¥0
is a set of transition probabilities, and m is the index of
a bar to which the note zj belongs. Moreover, Ï•s0p0 and
Ïˆsmpjâˆ’1pj are deï¬ned as
Ï•s0p0 =
Ë†Ï•Ë†s0deg(p0;s0)
âˆ‘K
p=1 Ë†Ï•Ë†s0deg(p;s0)
,
(5)
Ïˆsmpjâˆ’1pj =
Ë†ÏˆË†smdeg(pjâˆ’1;sm)deg(pj;sm)
âˆ‘K
p=1 Ë†ÏˆË†smdeg(pjâˆ’1;sm)deg(p;sm)
,
(6)
where Ë†sâˆˆ{major,minor} is the mode of scale s and deg(p;s)
âˆˆ{0, . . . , 11} is the degree of pitch p in scale s (deï¬ned as
the relative pitch class of p from the tonic of scale s). Ë†Ï•âˆ—
and Ë†Ïˆâˆ—are the initial and transition probabilities of pitch
classes, given the scales.
3.2.3 Modeling Onset Transitions
Considering the transition between onset positions of adja-
cent notes, the model makes Z have the plausible rhythm.
Let rjâˆ’1âˆˆ{vn}N
n=1 be the onset position of the j-th note
zj. The transition probability is given by
p(rj|rjâˆ’1, Î¶rjâˆ’1) = Î¶rjâˆ’1rj,
(7)
where the distance between rjâˆ’1 and rj indicates the note
value lj of note zj. We assume that r0 = v0 and rJ = vN.
3.3 Probabilistic Modeling of F0 Trajectories
The section describes the F0 model based on an HSMM
that represents the generative process of a vocal F0 trajec-
tory. In our model, the pitches, onsets, and temporal devia-
tions are represented as latent variables, and the frequency
deviations are represented as emission probabilities.












	



Figure 4: The black bold line represents a sequence of the
location parameters of the Cauchy distributions.
3.3.1 Modeling Temporal Deviations
We assume that vocal F0 trajectories include the following
two types of temporal deviations (Fig. 3a):
Onset deviation: the gap between the vocal onset time
and the note onset time.
F0 transitional duration: the time it takes for singing
voices to ï¬nish transitioning from one pitch to the next.
The onset deviations G = {gj}J
j=0 accompanying with
Z are represented as discrete latent variables. Each gj can
take an integer value between âˆ’G and G. As with the onset
position model, gjâˆ’1 denotes the onset deviation of note
zj. We assume that each gj is independently generated by
p(gj|Ï) = Ïgj,
(8)
where Ï âˆˆR2G+1
â‰¥0
is a set of onset deviation probabilities.
We assume that there are no deviations for the onset of the
ï¬rst note and the offset of the last note, i.e., g0 = gJ = 0.
The F0 transitional durations D = {dj}J
j=1 accompa-
nying with Z are also represented as discrete latent vari-
ables. Each dj can take a value from 1 to D. The con-
tinuous transition of vocal F0s between notes zjâˆ’1 and zj
is represented by a slanted line spanning dj frames. We
assume that each dj is independently generated as follows:
p(dj|Î·) = Î·dj,
(9)
where Î· âˆˆRD
â‰¥0 is a set of duration probabilities.
3.3.2 Modeling Frequency Deviations
The vocal F0 trajectory X = {xt}T
t=1 is generated by im-
parting probabilistic frequency deviations to the sequence
of notes to which probabilistic temporal deviations have
already been imparted (Fig. 3b). Assuming that xt is inde-
pendently generated at each frame, the emission probabil-
ity of the j-th note zj is given by
p(xÏ„jâˆ’1:Ï„jâˆ’1|pjâˆ’1, pj, lj, gjâˆ’1, gj, dj, Ë†Âµt, Î»)
=
Ï„jâˆ’1
âˆ
t=Ï„jâˆ’1
{Î´xt,voicedCauchy(xt|Ë†Âµt, Î») + Î´xt,unvoiced}
= epjâˆ’1pjljgjâˆ’1gjdj,
(10)
where xÏ„ â€²:Ï„âˆ’1 indicates xÏ„ â€², . . . , xÏ„âˆ’1, Î» is a scale param-
eter that represents the scale of the frequency deviations, Î´
is Kroneckerâ€™s delta, and Ë†Âµt (Fig. 4) is a location parameter
given by
Ë†Âµt=
{Âµpj âˆ’Âµpjâˆ’1
dj
(tâˆ’Ï„jâˆ’1)+Âµpjâˆ’1 (Ï„jâˆ’1â‰¤t<Ï„j+dj)
Âµpj
(Ï„jâˆ’1+djâ‰¤t<Ï„j)
. (11)
When the onset of note zj+1 is located at the n-th tatum,
Ï„j = un + gj and Ï„jâˆ’1 = unâˆ’lj + gjâˆ’1.
378
Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017










	

 



	


Figure 5: The conï¬guration of the hyperemarameter a
Ë†Ï•
Ë†s .
3.4 Prior Distributions
We put conjugate Dirichlet priors on categorical model pa-
rameters Ï€, Î¾, Ë†Ï•, Ë†Ïˆ, Î¶, Ï, and Î· as follows:
Ï€ âˆ¼Dirichlet
(
aÏ€)
, Î¾s âˆ¼Dirichlet
(
aÎ¾
s
)
,
Ë†Ï•Ë†s âˆ¼Dirichlet
(
a
Ë†Ï•
Ë†s
)
,
Ë†ÏˆË†sdeg(p;s)âˆ¼Dirichlet
(
a
Ë†
Ïˆ
Ë†sdeg(p;s)
)
,
Î¶r âˆ¼Dirichlet
(
aÎ¶
r
)
,
Ï
âˆ¼Dirichlet
(
aÏ)
, Î· âˆ¼Dirichlet
(
aÎ·)
,
(12)
where aÏ€âˆˆR26
+ , aÎ¾
sâˆˆR26
+ , a
Ë†Ï•
Ë†s âˆˆR12
+ , a
Ë†
Ïˆ
Ë†sdeg(p;s)âˆˆR12
+ , aÎ¶
râˆˆR16
+ ,
aÏâˆˆR2G+1
+
, and aÎ·âˆˆRD
+ are hyperparameters. The prob-
ability distribution over the 12 pitch classes under a scale
is estimated using the priors on the initial and transitional
probabilities of those classes. As illustrated in Fig. 5, we
set the hyperparameters a
Ë†Ï•
Ë†s and a
Ë†
Ïˆ
Ë†sdeg(p;s) so that the prob-
ability distributions represent the diatonic scales, respec-
tively. Since the Cauchy distribution does not have a con-
jugate prior, we put a Gamma prior on Î» as
Î» âˆ¼Gamma
(
aÎ»
0, aÎ»
1
)
,
(13)
where aÎ»
0 and aÎ»
1 are shape and rate hyperparameters.
3.5 Bayesian Inference
Given an F0 trajectory X, we aim to calculate the posterior
distribution p(Q, S, Î˜|X), where Q = {P , L, G, D}
(latent variables) and Î˜ = {Ï€, Î¾, Ë†Ï•, Ë†Ïˆ, Î¶, Ï, Î·} (model
parameters). Since this calculation is analytically intractab-
le, we use Markov chain Monte Carlo (MCMC) methods.
To get samples of the latent variables S and Q, forward
ï¬ltering-backward sampling algorithms are used. To get
samples of Î˜ except for Î», a set of parameters with con-
jugate priors, a Gibbs sampling algorithm is used. Since
there is no conjugate prior for the parameter Î», we use
the Metropolis-Hastings (MH) algorithm. Since S and Q
share the sequence of notes Z and are mutually dependent,
each variable is updated as follows:
1. Initialize notes Z with a majority-vote method.
2. Update the sequence of scales S based on given Z.
3. Update Q based on given S.
4. Update the model parameters Î˜.
5. Return to 2.
3.5.1 Inferring Latent Variables S
Given the sequence of notes Z, each sm is sampled in ac-
cordance with the probability given by
Î²S
sm = p(sm|sm+1:M, Z),
(14)
where sm+1:M represents sm+1, . . . , sM. The calculation
of Eq. (14) and sampling of scales S are performed by the
forward ï¬ltering-backward sampling method.
In forward ï¬ltering, we recursively calculate the proba-
bility Î±S
sm as follows:
Î±S
s0 = p(p0, s0) = p(p0|s0)p(s0) = Ï•s0p0Ï€s0,
(15)
Î±S
sm = p(p0:jm+1âˆ’1, sm)
=
jm+1âˆ’1
âˆ
j=jm
Ïˆsmpjâˆ’1pj
âˆ‘
smâˆ’1
Î¾smâˆ’1smÎ±S
smâˆ’1,
(16)
where jm is the index of the ï¬rst note whose onset belongs
to the m-th bar. jm can be calculated from given note val-
ues L.
In backward sampling, Eq. (14) is calculated by using
the values calculated in forward ï¬ltering, and scales are
sampled recursively as follows:
Î²S
sM = p(sM|Z) âˆÎ±S
sM ,
(17)
Î²S
sm = p(sm|sm+1:M, Z) âˆÎ±S
smÎ¾smsm+1.
(18)
3.5.2 Inferring Latent Variables Q
The latent variables Q can be estimated in a way similar
to that in which the latent variables S are inferred.
In
forward ï¬ltering, we recursively calculate the probability
Î±Q
pnln,gndn as follows:
Î±Q
p0l0g0d0 = p(p0|S) = Ï•y0p0,
(19)
Î±Q
pnlngndn= p(x1:Ï„nâˆ’1, pn, ln, gn, dn|S)
=
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
0
(ln>n)
ÏgnÎ·dnÎ¶r0rn
Â· âˆ‘
p0 Ïˆs1p0pnep0pnln0gndnÎ±Q
p0l0g0d0 (ln=n)
âˆ‘
pnâ€²,gnâ€²
min(nâ€²,L)
âˆ‘
lnâ€²
âˆ‘
dnâ€²
ÏgnÎ·dnÎ¶rnâ€²rnÏˆsm(nâ€²)pnâ€²pn
Â· epnâ€²pnlngnâ€²gndnÎ±Q
pnâ€²lnâ€²gnâ€²dnâ€²
(ln<n)
, (20)
where Ï„n = un + gn, nâ€² = n âˆ’ln, and m(nâ€²) is the index
of the bar that the nâ€²-th tatum belongs to. pn, ln, gn, and
dn are the variables of forward messages that correspond
to the note whose offset position is at the n-th tatum un.
Note that these variables are different from j-indexed vari-
ables pj, lj, gj, and dj. Since the onset and offset times
of the note zn = (pn, ln) are respectively the (nâˆ’ln)-th
tatum and the n-th tatum, the probability p(ln) which ap-
pears in the recursive calculation of Eq. (20) is replaced by
p(rn|rnâˆ’ln).
In backward sampling, the posterior distribution of the
latent variables is calculated by using the values calculated
in forward ï¬ltering, and notes and temporal deviations are
sampled recursively as follows:
Î²pNlNgNdN = p(pN, lN, gN, dN|X, S) âˆÎ±Q
pNlNgNdN ,
Î²pnâ€²lnâ€²gnâ€²dnâ€²
= p(pnâ€², lnâ€², gnâ€², dnâ€²|pn:N, ln:N, gn:N, dn:N, X)
âˆ
ï£±
ï£´
ï£²
ï£´
ï£³
0
(ln>n)
epnâ€²pnlngnâ€²gndnÏˆsm(nâ€²)pnâ€²pn
Â· Î¶rnâ€²rnÏgnÎ·dnÎ±Q
pnâ€²lnâ€²gnâ€²dnâ€²
(ln â‰¤n)
.
(21)
3.5.3 Learning Model Parameters Î˜
The posterior distributions of the model parameters with
the prior distributions are calculated using S and Q ob-
tained in the backward sampling steps, and these parame-
ters are sampled according to the posterior distributions as
follows:
Ï€âˆ¼Dirichlet
(
aÏ€+bÏ€)
, Î¾sâˆ¼Dirichlet
(
aÎ¾
s+bÎ¾
s
)
,
(22)
Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017
379

Ë†Ï•Ë†sâˆ¼Dirichlet
(
a
Ë†Ï•
Ë†s +b
Ë†Ï•
Ë†s
)
,
(23)
Ë†ÏˆË†sdeg(p;s)âˆ¼Dirichlet
(
a
Ë†
Ïˆ
Ë†sdeg(p;s)+b
Ë†
Ïˆ
Ë†sdeg(p;s)
)
,
(24)
Î¶r âˆ¼Dirichlet
(
aÎ¶
r+bÎ¶
r
)
,
(25)
Ï âˆ¼Dirichlet
(
aÏ+bÏ)
, Î·âˆ¼Dirichlet
(
aÎ·+bÎ·)
,
(26)
where bÏ€âˆˆR26
â‰¥0 is a unit vector whose s0-th element is 1.
bÎ¾
sâˆˆR26
â‰¥0 is a vector whose sâ€²-th element indicates the num-
ber of transitions between adjacent scales s and sâ€² in the se-
quence of latent variables Y . bÏâˆˆR2G+1
â‰¥0
is a vector whose
g-th element indicates the number of vocal onset devia-
tions of g in sampled Q, and bÎ·âˆˆRD
â‰¥0 is a vector whose
d-th element represents the number of F0 transitional du-
rations of d in sampled Q. bÎ¶
râˆˆR16
â‰¥0 is a vector whose râ€²-th
element represents the number of transitions between ad-
jacent note onset positions r and râ€² in R= {rj}J
j=0 that
can be calculated from the note values L sampled in back-
ward sampling. Regarding the vector b
Ë†Ï•
Ë†s âˆˆR12
>0, when the
scale of the initial bar and the pitch of the initial note are
s0 = s and p0 = p, the value of the element b
Ë†Ï•
Ë†sdeg(p;s)
is 1, and the other elements are 0. Regarding the vector
b
Ë†
Ïˆ
Ë†sdeg(p;s)âˆˆR12
â‰¥0, the value of b
Ë†
Ïˆ
Ë†sdeg(p;s)deg(pâ€²;s) is increased
by one when there is a transition from a pitch p to a pitch
pâ€² under a scale s in the sampled latent variables.
To apply the MH sampling to the parameter Î», we deï¬ne
a random-walk proposal distribution as follows:
q(Î»âˆ—|Î») = Gamma(Î³Î», Î³),
(27)
where Î»âˆ—is a proposal, Î» is the current sample, and Î³ is
a hyperparameter. The proposal Î»âˆ—is accepted as the next
sample according to the probability given by
A(Î»âˆ—, Î») = min
{L(Î»âˆ—)q(Î»|Î»âˆ—)
L(Î»)q(Î»âˆ—|Î»)
}
,
(28)
where L(Î») is the complete joint likelihood of Î» given by
L(Î») = Gamma
(
Î»|aÎ»
0, aÎ»
1
)
J
âˆ
j=1
epjâˆ’1pjljgjâˆ’1gjdj,
(29)
{pj, lj, gj, dj}J
j=0 are the values sampled in the backward
sampling. The value of Î» is updated by Î»âˆ—only when the
value of A(Î»âˆ—, Î») is larger than a random number sampled
from the uniform distribution U(0, 1).
3.6 Viterbi Decoding
The sequence of latent variables S and Q are estimated
with the Viterbi algorithm with the model parameters that
maximize the joint distribution p(X, Q, S, Î˜|Î¦) in the
learning process. As in the inference of latent variables, we
initialize Z by the majority-vote method, S is estimated
based on Z, and then Q is estimated depending on the S
estimated in the previous step.
In the Viterbi decoding on scales S, the value Ï‰S
s is
recursively calculated as follows:
Ï‰S
s0= ln Ï•s0k0+ ln Ï€s0,
(30)
Ï‰S
sm=
jm+1âˆ’1
âˆ‘
j=jm
ln Ïˆsmpjâˆ’1pj+max
smâˆ’1
{
ln Î¾smâˆ’1sm+Ï‰S
smâˆ’1
}
.(31)
In the recursive calculation of Ï‰S
s , the previous state smâˆ’1
that maximizes the value of Ï‰S
sm is memorized as cS
sm, and
the scales S are recursively estimated as follows:
sM = arg max
sM
Î±S
sM ,
smâˆ’1 = cS
sm.
(32)
In the Viterbi decoding on variables Q, the value Ï‰Q
plgd
is recursively calculated as follows:
Ï‰Q
p0l0g0d0 = wÏ• ln Ï•s0p0,
(33)
Ï‰Q
pnlngndn
=
ï£±
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£³
âˆ’inf
(ln>n)
wÏ ln Ïgn+wÎ· ln Î·dn+wÎ¶ ln Î¶rnr0
+ maxp0
{
wÏˆ ln Ïˆs1p0pn
+ we ln ep0pnln0gndn + Ï‰Q
p0l0g0d0
}
(ln=n)
wÏ ln Ïgn+wÎ· ln Î·dn+wÎ¶ ln Î¶rnrnâ€²
+ max(pnâ€²,lnâ€²,gnâ€²,dnâ€²)
{
wÏˆ ln Ïˆsm(nâ€²)pnâ€²pn
+ we ln epnâ€²pnlngnâ€²gndn
+ Ï‰Q
pnâ€²lnâ€²gnâ€²dnâ€²
}
(ln<n)
,
(34)
where wÏ•, wÏˆ, wÏ, wÎ·, wÎ¶, and we are the weight pa-
rameters that control the balance between probabilities. In
the recursive calculation of Ï‰Q
plgd, the previous states pnâ€²,
lnâ€², gnâ€², and dnâ€² which maximize the value of Ï‰Q
pnlngndn
are memorized as cQ
pnlngndn, and the variables Q are recur-
sively estimated as follows:
(pN, lN, gN, dN) =
arg max
pN,lN,gN,dN
Î±Q
pNlNgNdN ,
(35)
(pnâ€², lnâ€², gnâ€², dnâ€²) = cQ
pnlngndn.
(36)
4. EVALUATION
We report comparative experiments conducted to evaluate
the performance of the proposed method in musical note
estimation from vocal F0 trajectories.
4.1 Experimental Conditions
Among the 100 pieces of popular music in the RWC mu-
sic database [5], we used 63 pieces that do not include
32nd notes, triplets, harmonizing parts, and overlaps of ad-
jacent notes, which the proposed method cannot deal with.
The input F0 trajectories were obtained from the annota-
tion data [4] or automatically estimated by using the state-
of-the-art melody extraction method proposed in [9]. The
annotation data contain unvoiced regions and the estima-
tion data do not. The tatum times and onset positions were
obtained from the annotation data.
The Bayesian inference and Viterbi decoding were in-
dependently conducted for each song. The onset transi-
tion probabilities were learned in advance from a corpus
of rock music [2] without Bayesian learning. The hyper-
parameters were aÏ€=1, aÎ¾
s=1, aÎ¶
r=1, aÏ = aÎ· = aÎ»
0 =
aÎ»
1 = Î³ =1, where 1 and 1 respectively represent the ma-
trix and vector whose elements are all ones. The elements
of a
Ë†Ï•
Ë†s and a
Ë†
Ïˆ
Ë†sdeg(p;s) corresponding to musical notes on the
scale of Ë†s were 10 and the others were 1. The weight pa-
rameters of the Viterbi algorithm were empirically set as
wÏ• = wÏˆ = 29.4, wÏ = 2.4, wÎ· = 2.9, wÎ¶ = 48.5, and
we = 3.8. To obtain musically-consistent sequences of
380
Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017

Model
Input F0s
Tatum level Note level
Proposed
Ground-truth 72.4 Â± 1.7 28.1 Â± 2.1
method
Estimated
68.7 Â± 1.3
30.7 Â± 1.8
With
Ground-truth 71.5 Â± 1.6
26.3 Â± 2.1
only rhythms
Estimated
67.7 Â± 1.3
29.1 Â± 1.8
With
Ground-truth 67.8 Â± 1.6
10.6 Â± 1.2
only scales
Estimated
65.6 Â± 1.2
13.8 Â± 1.1
Without scales Ground-truth 67.2 Â± 1.5
9.8 Â± 1.2
& rhythms
Estimated
64.6 Â± 1.2
12.9 Â± 1.1
Majority vote
Ground-truth 54.1 Â± 1.5
20.1 Â± 1.4
Estimated
61.0 Â± 1.4
22.0 Â± 1.5
HMM [16]
Estimated
68.0 Â± 1.2
14.8 Â± 1.3
Table 1: Average matching rates [%] and their standard
errors in tatum and note levels.
musical notes, we put more emphasis on the score model
than the F0 model.
For comparison, we tested the majority-vote method as
a baseline and the latest conventional method based on
a semi-beat-synchronous HMM [16]. Since the conven-
tional method cannot deal with unvoiced regions in a vocal
F0 trajectory given as input, we only tested the method
for the estimation data. To evaluate the effectiveness of
the score model, we tested four versions of the proposed
method; a method that does not consider scales (scale tran-
sition probabilities) and rhythms (onset transition proba-
bilities), a method considering only scales, a method con-
sidering only rhythms, the full method considering both
scales and rhythms. To accelerate the inference, the search
range of pitches was limited around the pitches estimated
by the majority-vote method.
To evaluate the performance of each method, we cal-
culated tatum-level and note-level matching rates by com-
paring the estimated sequences of musical notes with the
ground-truth data. The tatum-level matching rate is the rate
of the number of tatum units whose pitches were estimated
correctly to the total number of tatum units whose pitches
exist in the ground-truth scores.
The note-level match-
ing rate is the rate of the number of musical notes whose
pitches, onsets, and offsets were estimated correctly to the
total number of musical notes in the ground-truth scores.
If adjacent notes in the ground-truth scores have the same
pitch or are connected by a tie, those notes were regarded
as a single note. Since the compared method [16] outputs
a pitch in a 16th-note-wise manner, a sequence of the same
pitches was regarded as a single note.
4.2 Experimental Results
The experimental results are shown in Table 11 . The pro-
posed method outperformed the majority-vote method and
the conventional method in terms of both measures. Com-
paring the tatum-level matching rates obtained by the four
versions of the proposed method, we conï¬rmed that the
score model improved the performance of musical note
estimation. The use of the onset transition probabilities
1 The results of music note estimation by the proposed method are
available online: http://sap.ist.i.kyoto-u.ac.jp/members/nishikimi/demo/
ismir2017/

	

	


Figure 6: Musical scores estimated from a ground-truth F0
trajectory by the proposed method and its variant without
scale and rhythm constraints.
(rhythm constraints) was found to be more effective than
that of the scale transition probabilities (scale constraints).
Although the tatum-level matching rate obtained the pro-
posed method (68.7%) was close to that obtained by the
conventional method (68.0%), the note-level matching rate
obtained the proposed method (30.7%) was better than that
obtained by the conventional method (14.8%), This is a re-
markable advantage of the proposed HHSMM that can di-
rectly represent both the pitches and durations (onsets and
offsets) of musical notes on symbolic musical scores, not
on continuous-time piano rolls.
Examples of estimated musical scores are illustrated in
Fig. 6. The proposed method yielded the almost accurate
musical score except that some notes were merged. To cor-
rectly recognize two adjacent notes with the same pitch, it
is necessary to refer to original singing voices or music au-
dio signals. The score estimated without considering the
score model, on the other hand, included a lot of wrong
notes that were inconsistent with music theory. This result
also shows the effectiveness of using the score model as
musical constraints on musical note estimation.
5. CONCLUSION
This paper presented a statistical method for musical note
estimation from a vocal F0 trajectory. Our method is based
on an HHSMM that combines a score model (HMM) rep-
resenting the generative process of a musical score from
musical scales with an F0 model (HSMM) representing
the generative process of a vocal F0 trajectory with time-
frequency deviation from the musical score. We conï¬rmed
that the proposed method can yield more musically-consistent
sequences of musical notes.
One of the most interesting directions of this research is
to use the proposed model as a musically-meaningful prior
distribution on a vocal F0 trajectory in vocal F0 estimation
for music audio signals. We plan to integrate the proposed
â€œlanguageâ€ model that generates an F0 trajectory from a
musical score with an acoustic model that generates a spec-
trogram from the F0 trajectory in a hierarchical Bayesian
manner. This enables us to jointly learn the vocal F0 tra-
jectory and musical score from music audio signals. Joint
estimation of beat times and F0s is worth investigating to
overcome the problem of estimation-error accumulation in
the cascaded estimation approach.
Acknowledgement: This study was partially supported by JSPS
KAKENHI Grant Numbers 26700020, 16H01744, and 16J05486
and JST ACCEL No. JPMJAC1602.
Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017
381

6. REFERENCES
[1] A. de CheveignÂ´e and H. Kawahara. YIN, a fundamen-
tal frequency estimator for speech and music. The Jour-
nal of the Acoustical Society of America, 111(4):1917â€“
1930, 2002.
[2] T. De Clercq and D. Temperley. A corpus analysis of
rock harmony. Popular Music, 30(01):47â€“70, 2011.
[3] J.-L. Durrieu, G. Richard, B. David, and C. FÂ´evotte.
Source/ï¬lter model for unsupervised main melody ex-
traction from polyphonic audio signals. IEEE Trans-
actions on Audio, Speech, and Language Processing,
18(3):564â€“575, 2010.
[4] M. Goto. Aist annotation for the RWC music database.
In The 7th International Conference on Music Infor-
mation Retrieval (ISMIR 2006), pages 359â€“360, 2006.
[5] M. Goto, H. Hashiguchi, T. Nishimura, and R. Oka.
RWC music database: Popular, classical and jazz mu-
sic databases. In The 3rd International Conference on
Music Information Retrieval (ISMIR 2002), pages 287â€“
288, 2002.
[6] M. Goto, K. Yoshii, H. Fujihara, M. Mauch, and
T Nakano. Songle: A web service for active music
listening improved by user contributions. In Proc. of
the 12th International Society for Music Information
Retrieval Conference (ISMIR 2011), pages 311â€“316,
2011.
[7] Dik J. Hermes. Measurement of pitch by subharmonic
summation. The journal of the acoustical society of
America, 83(1):257â€“264, 1988.
[8] P.-S.
Huang,
S.
D.
Chen,
P.
Smaragdis,
and
M. Hasegawa-Johnson. Singing-voice separation from
monaural recordings using robust principal compo-
nent analysis. In 2012 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP
2012), pages 57â€“60, 2012.
[9] Y. Ikemiya, K. Yoshii, and K. Itoyama. Singing voice
analysis and editing based on mutually dependent F0
estimation and source separation. In 2015 IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2015), pages 574â€“578, 2015.
[10] Y. E. Kim and B. Whitman. Singer identiï¬cation in
popular music recordings using voice coding features.
In 3rd International Conference on Music Information
Retrieval (ISMIR 2002), volume 13, page 17, 2002.
[11] A. Laaksonen. Automatic melody transcription based
on chord transcription. In Proc. of the 15th Interna-
tional Society for Music Information Retrieval (ISMIR
2014), pages 119â€“124, 2014.
[12] Y. Li and D. Wang. Separation of singing voice from
music accompaniment for monaural recordings. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 15(4):1475â€“1487, 2007.
[13] M. Mauch, C. Cannam, R. Bittner, G. Fazekas, J Sala-
mon, J. Dai, J. Bello, and S Dixon. Computer-aided
melody note transcription using the Tony software: Ac-
curacy and efï¬ciency. In Proc. of the 1st International
Conference on Technologies for Music Notation and
Representation (TENOR 2015), pages 23â€“30, 2015.
[14] M. Mauch and S. Dixon. pYIN: A fundamental fre-
quency estimator using probabilistic threshold distri-
butions. In 2014 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP
2014), pages 659â€“663, 2014.
[15] E. Molina, L. J. TardÂ´on, A. M. Barbancho, and I. Bar-
bancho. Sipth: Singing transcription based on hystere-
sis deï¬ned on the pitch-time curve. IEEE/ACM Trans-
actions on Audio, Speech and Language Processing
(TASLP), 23(2):252â€“263, 2015.
[16] R. Nishikimi, E. Nakamura, K. Itoyama, and K Yoshii.
Musical note estimation for f0 trajectories of singing
voices based on a bayesian semi-beat-synchronous
hmm. In Proc. of the 17th International Society for Mu-
sic Information Retrieval Conference (ISMIR 2026),
pages 461â€“467, 2016.
[17] R. P. Paiva, T. Mendes, and A. Cardoso. On the detec-
tion of melody notes in polyphonic audio. In 6th Inter-
national Conference on Music Information Retrieval
(ISMIR 2005), pages 175â€“182, 2005.
[18] C. Raphael. A graphical model for recognizing sung
melodies. In 6th International Conference on Music
Information Retrieval (ISMIR 2005), pages 658â€“663,
2005.
[19] M. RyynÂ¨anen, T. Virtanen, J. Paulus, and A. Kla-
puri. Accompaniment separation and karaoke appli-
cation based on automatic melody transcription. In
2008 IEEE International Conference on Multimedia
and Expo, pages 1417â€“1420, 2008.
[20] M. P. RyynÂ¨anen and A. P. Klapuri. Automatic tran-
scription of melody, bass line, and chords in poly-
phonic music. Computer Music Journal, 32(3):72â€“86,
2008.
[21] J. Salamon and E. GÂ´omez. Melody extraction from
polyphonic music signals using pitch contour charac-
teristics. IEEE Transactions on Audio, Speech, and
Language Processing, 20(6):1759â€“1770, 2012.
[22] W.-H. Tsai and H.-M. Wang. Automatic singer recog-
nition of popular music recordings via estimation and
modeling of solo vocal signals. IEEE Transactions on
Audio, Speech, and Language Processing, 14(1):330â€“
341, 2006.
[23] L. Yang, A. Maezawa, J. B. L. Smith, and E. Chew.
Probabilistic transcription of sung melody using a pitch
dynamic model. In 2017 IEEE International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP 2017), pages 301â€“305, 2017.
382
Proceedings of the 18th ISMIR Conference, Suzhou, China, October 23-27, 2017
