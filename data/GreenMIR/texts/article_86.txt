ON THE PERFORMANCE OF OPTICAL MUSIC RECOGNITION IN THE
ABSENCE OF SPECIFIC TRAINING DATA
Juan C. Martinez-Sevilla1
Adrian Rosello1
David Rizo1,2
Jorge Calvo-Zaragoza1
1 U. I. for Computing Research, University of Alicante, Spain
2 Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana, Spain
adrian.rosello@ua.es, {jcmartinez, drizo, jcalvo}@dlsi.ua.es
ABSTRACT
Optical Music Recognition (OMR) has become a popu-
lar technology to retrieve information present in musical
scores in conjunction with the increasing improvement of
Deep Learning techniques, which represent the state-of-
the-art in the ﬁeld. However, its effectiveness is limited
to cases where the target collection is similar in musical
context and graphical appearance to the available train-
ing examples. To address this limitation, researchers have
resorted to labeling examples for speciﬁc neural models,
which is time-consuming and raises questions about us-
ability. In this study, we propose a holistic and comprehen-
sive study for dealing with new music collections in OMR,
including extensive experiments to identify key aspects to
have in mind that lead to better performance ratios. We
resort to collections written in Mensural notation as a spe-
ciﬁc use case, comprising 5 different corpora of training
domains and up to 15 test collections. Our experiments
report many interesting insights that will be important to
create a manual of best practices when dealing with new
collections in OMR systems.
1. INTRODUCTION
Manual sheet music transcription is a tedious process,
prone to errors, and generally requires professionals with
precise knowledge of the type of notation and/or music at
issue. The alternative to this manual digitization of con-
tent is to resort to cutting-edge technology based on artiﬁ-
cial intelligence, which performs an automated reading of
documents. This technology is known as Optical Music
Recognition (OMR).
OMR has been an active research area for decades [1],
although the ﬁeld progressed slowly [2].
Recently, the
use of modern machine learning techniques, namely Deep
Learning, has led to a paradigm shift that has partially un-
locked this situation [3,4]. Indeed, it has been shown that
© J. C. Martinez-Sevilla, A. Rosello, D. Rizo and J. Calvo-
Zaragoza. Licensed under a Creative Commons Attribution 4.0 Interna-
tional License (CC BY 4.0). Attribution:
J. C. Martinez-Sevilla, A.
Rosello, D. Rizo and J. Calvo-Zaragoza, “On the Performance of Optical
Music Recognition in the Absence of Speciﬁc Training Data”, in Proc. of
the 24th Int. Society for Music Information Retrieval Conf., Milan, Italy,
2023.
current OMR technologies, despite the fact that they are
not yet fully mature, are usually a better alternative than
performing the entire transcription by hand [5].
Concerning the machine learning methods, the related
literature reports that the models provide sufﬁcient preci-
sion when the collections to be transcribed are from the
same graphic and content domain as the corpus used to
train them. This, however, makes it difﬁcult to transfer
technology to new collections, since it is not always possi-
ble, desirable, or efﬁcient to invest resources in annotating
a small portion of the target collection. Although it is naive
to assume the availability of training sets from the same
domain as a given target collection, in the current data era
we can assume to have at least a series of labeled collec-
tions, even with different graphic and musical character-
istics. This, of course, can and should be used to improve
the efﬁciency of ﬁtting OMR models to new collections for
which we do not have speciﬁc training sets.
In this paper, we report on a case study focused on Men-
sural notation to answer questions about the transferability
of OMR models to new music collections. To our best
knowledge, this work constitutes the ﬁrst to analyze this
issue in the ﬁeld. We consider Mensural notation as the
structuring experimental body because the OMR technol-
ogy can be considered mature for this notation. Also, we
have a signiﬁcant number of labeled and unlabeled collec-
tions in this notation, which allows us to carry out an ex-
haustive study that is expected to lead to more generaliz-
able conclusions. Speciﬁcally, we consider 5 labeled col-
lections that will be used as training sets, along with their
possible combinations, and up to 15 unlabeled collections
as target.
The rest of the paper is structured as it follows: in Sec-
tion 2, we provide some background to the topic; in Sec-
tion 3, we present our methodology to analyze the question
at issue; the experimental setup is described in Section 4,
while the results and analysis are given in Section 5; ﬁ-
nally, we conclude the paper in Section 6, while pointing
out some interesting avenues for future work.
2. BACKGROUND
Recent advances in artiﬁcial intelligence, with extensive
use of Deep Learning (DL) technologies, resulted in about
successful approaches to OMR. Speciﬁcally, a holistic ap-
319

proach, also known as end-to-end formulation, which has
been dominating the state of the art in other applications
such as text or speech recognition [6, 7], is currently con-
sidered the reference model in OMR. The related literature
includes many successful solutions of this type [8–10]. In
this work, we resort to this approach as representative of
the state of the art based on DL.
However, as introduced above, there is still no computa-
tional approach for creating a universal OMR system; i.e,
one that is capable of dealing with any kind of collection.
The underlying issue is an overly unsolved challenge in
artiﬁcial intelligence [11]: DL works well if the problem
is statistically regular and there is abundant training data
to adequately and representatively learn such regularity.
This is, unfortunately, quite difﬁcult to expect when deal-
ing with ancient documents. Instead of trying to solve the
underlying problem of machine learning, we take a more
practical path to provide a series of best practices to tackle
the situation of target collections in the absence of speciﬁc
training data successfully.
It is important to highlight that, in the OMR literature,
there are very few works dedicated to studying the prac-
tical aspects of the technology. Pugin and Crawford [12]
estimated through a quantitative evaluation the suitability
of using the Aruspix machine-learning-based OMR sys-
tem on a real collection. Furthermore, Alfaro-Contreras et
al. [5] analyzed the beneﬁts of using OMR in cases where
the accuracy of the system was not perfect. Our work fur-
ther contributes to this barely explored line of practical as-
pects for the application of OMR to real-world scenarios
from the perspective of the available training data.
3. METHODOLOGY
The focus of the work is essentially experimental.
We
want to be able to answer speciﬁc questions about how
to approach the generation of generalizable OMR mod-
els. Our objective is to reduce the uncertainty when facing
the recognition of collections for which there is no speciﬁc
training set.
To answer these questions, we will consider as a starting
point the availability of N training sets that, even depict-
ing the same musical notation (Mensural notation), differ
in graphic characteristics. This will allow drawing more
interesting conclusions about the synergy of using a het-
erogeneous set of training collections. To cover all possi-
bilities, we create models from all possible combinations
of these sets (2N −1 possibilities). Each of these possibil-
ities will be directly evaluated on M test sets (not seen in
any training case), also showing heterogeneous character-
istics.
As previously mentioned, we will consider a deep end-
to-end model as representative of the state of the art in
OMR. Below we explain in more detail how this model
works.
3.1 Learning framework
For the task, a Convolutional Recurrent Neural Network
(CRNN) scheme is proposed for the end-to-end optical
music transcription pipeline. The CRNN architecture con-
sists of a block of convolutional layers that learns the rele-
vant features from the input image (single staff), followed
by a group of recurrent stages that model the temporal de-
pendencies of the feature-learning block. Finally, a fully-
connected network with a softmax activation is used to re-
trieve the posteriogram, which is decoded to obtain the pre-
dicted musical symbols. 1
The Connectionist Temporal Classiﬁcation (CTC) [13]
training procedure is used to train the CRNN model using
unsegmented sequential data. The training set T consists
of pairs of single musical staff images xi and their corre-
sponding symbol sequence zi in a symbol vocabulary Σ,
with 261 units corresponding to the number of different
symbols among the training sets. To use CTC as an end-
to-end sequence labeling framework, an additional "blank"
symbol is included in the vocabulary Σ′.
Formally, let T ⊂X × Σ∗be a set of data where an
image xi ∈X of a single staff is related to symbol se-
quence zi =
 zi1, zi2, . . . , zi|zi|

∈Σ∗, where Σ repre-
sents the symbol vocabulary used for encoding the music
score. Note that the use of CTC to model the transcrip-
tion task as an end-to-end sequence labeling framework
requires the inclusion of an additional “blank” symbol in
the Σ vocabulary, i.e., Σ′ = Σ ∪{blank}.
At prediction, for a given musical staff image input
xi ∈X, the model outputs a posteriogram pi ∈R|Σ′|×K,
where K represents the number of frames given by the
recurrent stage. Finally, the predicted sequence ˆzi is ob-
tained resorting to a greedy policy that retrieves the most
probable symbol per frame in pi, later a subsequent map-
ping function merges consecutive repeated symbols and re-
moves blank labels.
4. EXPERIMENTAL SETUP
In this section, we present our choices for the experimental
design. First, we describe the considered evaluation met-
ric. Then, we give more implementation details of the deep
learning model. Finally, we present and describe the col-
lections selected as train and target sets.
4.1 Evaluation
To evaluate the performance of the OMR model, we resort
to the Symbol Error Rate (SER). This is computed as the
average number of elementary editing operations (inser-
tions, deletions, or substitutions) required to convert pre-
diction ˆzi into reference zi, normalized by the length of
the latter.
In general, we are interested in computing the amount
of effort it would take for a person to correct the remaining
errors in the system. Since computing this human effort
1 Understanding
musical
symbol
as
the
conjunction
of
glyph:position, i.e., note_half:L2 (a glyph note_half
present in the second staff line).
Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023
320

does not scale well in practice (it consumes huge amounts
of resources), we believe that this metric is suitable to mea-
sure the transcription correctness. In addition, it is a metric
that has been commonly applied in previous works on this
subject (cf. Section 2).
4.2 Neural model conﬁguration
The CRNN topology is based on the one used in the re-
search [14], where the authors adopt a 4 convolutional
layer block with batch normalization, Leaky ReLu activa-
tion, and max-pooling down-sampling. The feature maps
extracted from the convolutional block are fed into two
Bidirectional Long Short-Time Memory layers with 256
hidden units each and a dropout value of d = 50% fol-
lowed by a fully-connected network with |Σ′| units.
The models were trained with a batch size of 16
elements—note that in experiments where multiple train-
ing sets were used all the generated batches in the train-
ing process were balanced so the net didn’t adjust to a
certain corpus. The ADAM optimizer [15] was consid-
ered and a ﬁxed learning rate of 10−3. We iterate for 300
epochs, keeping the weights that minimize the SER met-
ric in the validation partition with an early stopping policy
of 30 epochs. Finally, all experiments were run using the
Python language (v. 3.8.13) with the PyTorch framework
(v. 1.13.0) on a single NVIDIA GeForce RTX 4090 card
with 24GB of GPU memory.
4.3 Datasets
A set of 20 different white Mensural notation works has
been collected for this work, consisting of pairs of staff
images and their transcription into sequences of musical
symbols. The pieces have been selected looking for diverse
cases concerning printers or copyists, layouts, authors, the
period in history, and extension. 2
4.3.1 Training Datasets
For training, 4 different datasets were chosen from real col-
lections, trying to cover as much variability as possible.
When facing a new transcription project, it is usual that
no training collection is similar or big enough for build-
ing a model to obtain reliable results from the automatic
recognition process. In this scenario, the creation of syn-
thetic training data from scratch is a valid alternative that
will be evaluated in the work with the PRIMENS dataset.
Therefore, we will add this synthetic collection to the set
of training sets, resulting in 5 different collections. These
training collections are described below.
• CAPITAN. The Capitan dataset contains 100 handwrit-
ten pages of ca. 17th-century manuscripts in late white
Mensural notation extracted from the work with signa-
ture B59.850 in the Catedral del Pilar in Zaragoza [16].
• SEILS. The SEILS dataset contains 151 printed pages
of the “Il Lauro Secco” collection corresponding to an
2 The whole set,
along with a comprehensive description of
the contents, can be found at https://grfia.dlsi.ua.es/
polifonia/ismir2023.html.
anthology of 16th-century Italian madrigals in white
Mensural notation [17].
• GUATEMALA.
The Guatemala dataset presents 383
handwritten pages from a polyphonic choir book, part
of a larger collection held at the “Archivo Histórico Ar-
quidiocesano de Guatemala” [18].
• MOTTECTA.
This dataset corresponds to the work
“Mottecta (Mottecta Francisci Guerreri, que partim
quaternis partim quinis alia senis alia octonis concinun-
tur vocibus, liber secundus dataset)”, authored by Fran-
cisco Guerrero in the 16th-century and edited by Gi-
acomo Vincenti in the 17th-century. This 297-printed
mensural pages corpus has been obtained from the col-
lection of mensural books of the Biblioteca Digital His-
pánica. 3
• PRIMENS. The Printed Images of Mensural Staves
(PrIMenS) dataset is a synthetic corpus that tries to
resemble low-quality real scans of printed mensural
sources. It has been built from works composed by Agri-
cola, Frye, and Ockeghem available in the Josquin Re-
search Project 4 . Given polyphonic scores encoded in
**kern [19] format, each voice is separated into a single
ﬁle. In order to increase the variability, the original clefs
are modiﬁed according to the instrument annotation in
the voice. To obtain single staves, the whole work has
been divided into a random number of measures from
3 to 18, and the resulting ﬁles have been converted into
**mens [20] format. The corresponding agnostic encod-
ing has been generated following the method described
in [17]. The images have been obtained using the digi-
tal engraver Verovio [21] by applying random values to
all the options in the allowed ranges. Finally, those im-
ages have been distorted to simulate real printed image
scans by using a random sequence of graphical ﬁlters
with the GraphicsMagick Image Processing. Addition-
ally, this real-image simulation process has been com-
plemented by composing randomly damaged old paper
textures with distorted images.
To better understand the differences that might appear
among these corpora, we provide a staff example from
each corpus in Fig. 1.
4.3.2 Target Datasets
For the task of testing the suitability of each model, 15
datasets have been chosen. These corpora have been care-
fully and speciﬁcally labeled for this work, and are sum-
marized in Table 1 and Fig. 2.
The printed sets have been extracted from the publicly
available collection of Mensural books in the Biblioteca
Digital Hispánica. 5
The handwritten collections are ob-
tained from archive of Catedral del Pilar in Zaragoza [16].
3 bdh.bne.es/bnesearch/detalle/bdh0000008932
4 https://josquin.stanford.edu/
(accessed
September
1st, 2022).
5 https://www.bne.es/es/catalogos/
biblioteca-digital-hispanica (accessed March 7th, 2023)
Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023
321

(a) CAPITAN
(b) SEILS
(c) GUATEMALA
(d) MOTTECTA
(e) PRIMENS
Figure 1:
Samples of staves of the different training
datasets employed.
Name (ID)
Number of staves
Printer
Amorosa (Amo)
224
H. of G. Scoto
Chansons (Cha)
173
A. Le Roy, R. Ballard
Dolci (Dol)
170
H. of G. Scoto
Lamentationes (Lam)
528
G. G. Carlino
Madrigali (Mad)
201
G. Scotto
Magniﬁcat (Mag)
1361
Antonio Gardano
Missarum (Mis)
489
H. of G. Scoto
MusicaNova (Mus)
874
Antonio Gardano
Orlande (Orl)
259
A. Le Roy, R. Ballard
Responsoria (Res)
666
G. G. Carlino
Sacrarum (Sac)
460
Antonio Gardano
Villanelle (Vil)
59
G. G. Carlino
B3.28 (B3)
60
Handwritten
B50.747 (B50)
80
Handwritten
B53.781 (B53)
32
Handwritten
Table 1: Features of the different target collections consid-
ered in this work.
5. RESULTS
Given the number of training corpora (5), the test datasets
(15), and the number of experiments (31), we are able to
report up to 465 different SER results. This enables us to
properly summarize the experimentation, extracting mean-
ingful learnings that will be used to state the best practices
to deal with training data on new projects. The analysis of
the results follows. The extended raw results of each exper-
iment are attached to this document in the supplementary
material.
5.1 Importance of size and variability
In order to understand which is the best training set selec-
tion strategy when facing a new unseen collection, all the
possible combinations of the datasets available for training
have been evaluated against the different target sets.
Figure 2: Image examples from the selected corpora as
test partition. The images follow a left-right-top-bottom
order concerning the list order from Table 1.
The more training sets we include in the combination
the greater the number of staves of that combined training
set will be. To evaluate which factor is more important, ei-
ther the variability, given by the number of different train-
ing sets included in each combination, or the size as the
total number of staves to train, we have plotted in Fig. 3
the summary statistics of the SER obtained by each trained
model over all the target collections.
In general, the best behavior has been obtained when
merging all the available training corpora. This ﬁrst out-
come may seem obvious, but due to the variability of the
training datasets and some of the test works, it was not il-
logical to expect otherwise. From this result, the fact to
be explained is why it performs the best, either due to the
size of the training set in terms of the number of staves or
the generality the model encompasses due to the training
corpora of different natures included.
The plot shows that, although adding more training cor-
pus does not worsen the results, it is not a determining fac-
tor. In general, good results are generally obtained with
combinations of at least 3 training sets. However, a com-
bination of just two corpora (i.e. CS) yields a good per-
formance both in mean and dispersion that denotes its ro-
bustness. These two corpora are complementary from the
graphical point of view and seem to be representative of
both printed sources (SEILS) and handwritten manuscripts
(Capitan). When applying 3-corpora training set combi-
nations, the results are equivalent: CGS experiment com-
pared with the GMP, wherein the combination of the ﬁrst
two handwritten corpora and one printed appear compared
to the collection of one handwritten and two printed train-
ing sets. From these evidences, it can be deduced that the
variability of training sets is relevant for better overall per-
formance.
If we focus on the size of the training collection, i.e.,
the total number of staves used for training, the plot shows
that it is not as important as the variability for the ﬁnal
performance. For example, experiment CMS, having less
than 4 000 staves, brings better results than experiment GP
with over 8 000 samples for training.
To conﬁrm the size is not all that matters, Fig. 4 illus-
trates the results reported by calculating the number of ex-
periments where the SER is minimized in any of the target
datasets, taking into account the number of datasets used
Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023
322

0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
 0
 2000
 4000
 6000
 8000
 10000
 12000
SER
Accumulated number of staves in training corpora
C
S
P
M
G
CG
GP
GM
GS
CP
CM
CS
MP
PS
MS
CGP
CGM
CGS
GMP
GPS
GMS
CMP
CPS
CMS
MPS
CGMP
CGPS
CGMS
GMPS
CMPS
CGMPS
#1
#2
#3
#4
#5
Figure 3: The boxplot shows different statistical SER ﬁgures (min, Q1, mean, Q3, max) over the test corpora using a
different combination of training corpora. The colors shown in the right bar represent the number of training corpora used
in each experiment. The labels are the initials of the corpora included in each training set: C: Capitan, S: SEILS, M:
Mottecta, G: Guatemala, P: PrIMenS.
0%
25%
50%
75%
100%
% of no. of collections with lowest errors
Number of corpora
0.0%
6.2%
31.2%
31.2%
31.2%
#1
#2
#3
#4
#5
Figure 4: Percentage of experiments that minimize the
SER value for any of the available test corpora.
to train. It can be noticed that trained experiments with
sizes 3, 4, and 5 report a value of 31.2%. Aside from the
value itself, what this aspect exposes is that the size of your
dataset at a given point is no longer a critical factor for the
transcription quality.
5.2 The complexity of a corpus
The average SER values for all experiments on each target
dataset are plotted in Fig. 5. The main noticeable aspect is
the difference between Q1 and Q3 (the colored box ends)
in the diverse corpora. This substantial contrast in disper-
sion is what we named “The complexity of a corpus”. The
plot shows that, as expected, the performance depends on
the precise selection of the combination of training corpora
to use. The maximum SER values are obtained when the
training data is built from just one dataset.
In general, the worst results in the graph are obtained
for handwritten target works (those named with the preﬁx
“B”) because, intrinsically, they are more difﬁcult to deal
with and need a higher variability in the number of training
corpora of handwritten works.
0%
20%
40%
60%
80%
100%
SER
Test Corpora
Amo
Cha
Dol
Lam
Mad
Mag
Mis
Mus
Orl
Res
Sac
Vil
B3
B50
B53
Figure 5: The boxplot shows different statistical SER ﬁg-
ures over all experiments made in each one of the testing
corpora.
5.3 The importance of leveraging the availability of
training corpora
Figure 6 shows the results of the experiments that use each
speciﬁc training corpus compared to the experiments that
do not use it. The image presents the casuistry when hav-
ing to choose either adding new samples from a different
dataset or continue increasing the size of existing labeled
samples. As the image reveals, every dataset available for
the train, no matter the type—printed or handwritten, real
or synthetic—should be included. It is worth mentioning,
that the relevance of adding a new corpus is more notice-
able than others. For example, referring to the Capitan cor-
pus, if we compare the experiments CMP – MP, CPS – PS,
Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023
323

0%
5%
10%
15%
20%
25%
30%
C - Capitan
S - SEILS
P - PrIMenS
M - Mottecta
G - Guatemala
SER Analogous Diﬀerence
Corpus
CGP
CGM
CGS
CMP
CPS
CMS
CGMP
CGPS
CGMS
CMPS
CGMPS
GP
GM
GS
MP
PS
MS
GMP
GPS
GMS
MPS
GMPS
CGS
GPS
GMS
CPS
CMS
MPS
CGPS
CGMS
GMPS
CMPS
CGMPS
CG
GP
GM
CP
CM
MP
CGP
CGM
GMP
CMP
CGMP
CGP
GMP
GPS
CMP
CPS
MPS
CGMP
CGPS
GMPS
CMPS
CGMPS
CG
GM
GS
CM
CS
MS
CGM
CGS
GMS
CMS
CGMS
CGM
GMP
GMS
CMP
CMS
MPS
CGMP
CGMS
GMPS
CMPS
CGMPS
CG
GP
GS
CP
CS
PS
CGP
CGS
GPS
CPS
CGPS
CGP
CGM
CGS
GMP
GPS
GMS
CGMP
CGPS
CGMS
GMPS
CGMPS
CP
CM
CS
MP
PS
MS
CMP
CPS
CMS
MPS
CMPS
Figure 6: Comparison between all experiments containing (green) and not containing (blue) each training set.
and CMS – MS, we can observe this phenomenon: because
of the variability that Capitan adds to the training set, the
improvement is noticeable. Therefore, a new corpus seems
to generally improve the model performance, as outlined in
Fig. 5.
But not only adding a different corpus helps to improve,
as the key is to be aware of what is missing in terms of
graphical variability in the available training data to build
a more robust model. An interesting piece of evidence in
the plot that shows how to proceed when this happens is
to notice that even a synthetic corpus helps in improving
the overall results when it complements the available orig-
inal training data. Note the reduction in SER when adding
PrIMenS, that synthetically simulates printed sources, to
complement two other handwritten datasets (Capitan and
Guatemala).
5.4 Lessons learned
In order to summarize and establish a set of best practices
to improve the generalization performance of OMR sys-
tems in the absence of speciﬁc training data, we will intro-
duce some questions and answers related to the knowledge
acquired from the experimental outcomes.
• Which is the best choice to transcribe a new collec-
tion? In general, one must use all the available training
corpora even if some of them are quite different from the
target collection.
• Is it better to have fewer collections with a high num-
ber of samples or more collections with fewer sam-
ples each? It is preferable to have more variability even
at the cost of a smaller sample set.
• How important is it to be aware of the collection to
transcribe for selecting the right corpora to train the
model? It is indeed relevant, and depending on the difﬁ-
culty (for example, whether or not it is handwritten) the
differences in performance can be very varied.
• Does the introduction of a synthetic corpus improve
the performance? Yes, the introduction of a reliable
synthetic collection adds size and variability to the train-
ing data, enabling better performance rates.
We consider that these answers can be used as general
rules of thumbs, although of course in certain cases they
may not hold.
6. CONCLUSIONS
OMR promises to make written music collections more ac-
cessible and browsable by automatically recognizing the
symbolic content from their images.
However, modern
technologies are based on machine learning with deep neu-
ral networks, which typically causes unpredictable perfor-
mance when processing a collection for which no speciﬁc
training data is available. In this work, we have studied
this issue using a large number of training and test col-
lections depicting Mensural notation. This extensive study
has been developed considering a state-of-the-art model as
representative of the ability to transfer knowledge between
collections with dissimilar characteristics.
Our experiments allowed us to analyze various phenom-
ena related to the synergies created between different train-
ing collections, the importance of choosing a good recog-
nition trained model to alleviate the uncertainty about per-
formance in a new collection, as well as a series of gen-
eral good practices on how to proceed for training general
OMR models.
As future work, we want to keep on in this line of inves-
tigating practical aspects of OMR systems that have a di-
rect impact on particular use cases. For example, we want
to extend the case study to the scenario of transfer learning
and ﬁne-tuning, where a (limited) amount of training data
from a new collection can be assumed. Also, it is interest-
ing to analyze the nature of the errors made by the different
OMR models, as well as to have a more precise estimate
of the impact of the different errors on the amount of effort
required during the post-editing correction process.
Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023
324

7. ACKNOWLEDGMENT
This paper is part of the I+D+i TED2021-130776A-
I00
(PolifonIA)
project,
funded
by
MCIN/AEI
/10.13039/501100011033 and European Union NextGen-
erationEU/PRTR.
8. REFERENCES
[1] D. Bainbridge and T. Bell, “The challenge of optical
music recognition,” Computers and the Humanities,
vol. 35, pp. 95–121, 2001.
[2] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. Marcal,
C. Guedes, and J. S. Cardoso, “Optical music recog-
nition: state-of-the-art and open issues,” International
Journal of Multimedia Information Retrieval, vol. 1,
pp. 173–190, 2012.
[3] A. Pacha, K.-Y. Choi, B. Coüasnon, Y. Ricquebourg,
R. Zanibbi, and H. Eidenberger, “Handwritten music
object detection: Open issues and baseline results,” in
2018 13th IAPR International Workshop on Document
Analysis Systems (DAS).
IEEE, 2018, pp. 163–168.
[4] J. Calvo-Zaragoza, J. Hajic Jr, and A. Pacha, “Under-
standing optical music recognition,” ACM Computing
Surveys (CSUR), vol. 53, no. 4, pp. 1–35, 2020.
[5] M. Alfaro-Contreras, D. Rizo, J. M. Inesta, and
J. Calvo-Zaragoza, “OMR-assisted transcription:
a
case study with early prints,” in Proceedings of the
22nd International Society for Music Information Re-
trieval Conference.
Online: ISMIR, Nov. 2021, pp.
35–41.
[6] A. Chowdhury and L. Vig, “An efﬁcient end-to-end
neural model for handwritten text recognition,” in
British Machine Vision Conference 2018, BMVC 2018,
Newcastle, UK, September 3-6, 2018.
BMVA Press,
2018, p. 202.
[7] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar,
P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao,
E. Gonina, N. Jaitly, B. Li, J. Chorowski, and
M. Bacchiani, “State-of-the-art speech recognition
with sequence-to-sequence models,” in 2018 IEEE In-
ternational Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP), 2018, pp. 4774–4778.
[8] J. Calvo-Zaragoza, A. H. Toselli, and E. Vidal,
“Handwritten music recognition for mensural notation
with convolutional recurrent neural networks,” Pattern
Recognition Letters, vol. 128, pp. 115–121, 2019.
[9] P. Torras, A. Baró, L. Kang, and A. Fornés, “On the
integration of language models into sequence to se-
quence architectures for handwritten music recogni-
tion,” in Proceedings of the 22nd International Soci-
ety for Music Information Retrieval Conference, ISMIR
2021, Online, November 7-12, 2021.
[10] M. Alfaro-Contreras, A. Ríos-Vila, J. J. Valero-Mas,
J. M. Iñesta, and J. Calvo-Zaragoza, “Decoupling mu-
sic notation to improve end-to-end optical music recog-
nition,” Pattern Recognition Letters, vol. 158, pp. 157–
163, 2022.
[11] Y. Bengio, Y. Lecun, and G. Hinton, “Deep learning
for AI,” Communications of the ACM, vol. 64, no. 7,
pp. 58–65, 2021.
[12] L. Pugin and T. Crawford, “Evaluating OMR on the
early music online collection,” in Proceedings of the
14th International Society for Music Information Re-
trieval Conference, ISMIR 2013, Curitiba, Brazil,
November 4-8, 2013, A. de Souza Britto Jr., F. Gouyon,
and S. Dixon, Eds., 2013, pp. 439–444.
[13] A. Graves, S. Fernández, F. J. Gomez, and J. Schmid-
huber, “Connectionist temporal classiﬁcation:
la-
belling unsegmented sequence data with recurrent neu-
ral networks,” in Proceedings of the Twenty-Third In-
ternational Conference on Machine Learning, (ICML
2006), Pittsburgh, Pennsylvania, USA, June 25-29,
2006, 2006, pp. 369–376.
[14] J. Calvo-Zaragoza, A. Toselli, and E. Vidal, “Hand-
written music recognition for mensural notation with
convolutional recurrent neural networks,”
Pattern
Recognition Letters, vol. 128, 08 2019.
[15] D. P. Kingma and J. Ba, “Adam:
A Method for
Stochastic Optimization,” in 3rd Int. Conf. on Learning
Representations, Y. Bengio and Y. LeCun, Eds., San
Diego, USA, 2015.
[16] J. Calvo-Zaragoza, D. Rizo, and J. M. I. Quereda, “Two
(note) heads are better than one: Pen-based multimodal
interaction with music scores,” in Proceedings of the
17th International Society for Music Information Re-
trieval Conference, ISMIR 2016, New York City, United
States, August 7-11, 2016, M. I. Mandel, J. Devaney,
D. Turnbull, and G. Tzanetakis, Eds., 2016, pp. 509–
514.
[17] E. Parada-Cabaleiro, A. Batliner, and B. Schuller, “A
diplomatic edition of il lauro secco: Ground truth for
omr of white mensural notation,” 10 2019.
[18] M. E. Thomae, J. E. Cumming, and I. Fujinaga, “Digi-
tization of choirbooks in guatemala,” in Proceedings of
the 9th International Conference on Digital Libraries
for Musicology, ser. DLfM ’22.
New York, NY, USA:
Association for Computing Machinery, 2022, p. 19–26.
[19] D. Huron, “Humdrum and Kern: Selective Feature En-
coding BT - Beyond MIDI: The handbook of musi-
cal codes,” in Beyond MIDI: The handbook of musical
codes.
Cambridge, MA, USA: MIT Press, jan 1997,
pp. 375–401.
[20] D. Rizo, N. Pascual-León, and C. S. Sapp, “White
mensural manual encoding: from humdrum to mei,”
Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023
325

Cuadernos de Investigación Musical, no. 6, pp. 373–
393, 2018.
[21] L. Pugin, R. Zitellini, and P. Roland, “Verovio: A li-
brary for engraving MEI music notation into SVG,”
in Proceedings of the 15th International Society for
Music Information Retrieval Conference, ISMIR 2014,
Taipei, Taiwan, October 27-31, 2014, H. Wang,
Y. Yang, and J. H. Lee, Eds., 2014, pp. 107–112.
Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023
326
