LEARNING INTERPRETABLE REPRESENTATION FOR
CONTROLLABLE POLYPHONIC MUSIC GENERATION
Ziyu Wang
Dingsu Wang
Yixiao Zhang
Gus Xia
Music X Lab, Computer Science Department, NYU Shanghai
{ziyu.wang, dingsu.wang, yixiao.zhang, gxia}@nyu.edu
ABSTRACT
While deep generative models have become the leading
methods for algorithmic composition, it remains a chal-
lenging problem to control the generation process because
the latent variables of most deep-learning models lack
good interpretability. Inspired by the content-style disen-
tanglement idea, we design a novel architecture, under the
VAE framework, that effectively learns two interpretable
latent factors of polyphonic music: chord and texture. The
current model focuses on learning 8-beat long piano com-
position segments. We show that such chord-texture dis-
entanglement provides a controllable generation pathway
leading to a wide spectrum of applications, including com-
positional style transfer, texture variation, and accompani-
ment arrangement. Both objective and subjective evalua-
tions show that our method achieves a successful disentan-
glement and high quality controlled music generation.1
1. INTRODUCTION
With the development of artiﬁcial neural networks, deep
learning has become one of the most popular techniques
for automated music generation. In particular, we see re-
current and attention-based models being able to gener-
ate creative and human-like music without heavily hand-
crafted rules [1–3]. However, the main drawback of these
deep generative models is that they behave like “black
boxes”, and it is difﬁcult to interpret the musical meaning
of their internal latent variables [4]. Consequently, it re-
mains a challenging task to control the generation process
(i.e., to guide the music ﬂow by manipulating the high-
level compositional factors such as melody contour, ac-
companiment texture, style, etc.). This limitation restricts
the application scenario of the powerful deep generative
models.
In this paper, we improve the model interpretability
for music generation via constrained representation learn-
ing. Inspired by the content-style disentanglement idea [5],
1 Code and demos can be accessed via https://github.com/
ZZWaang/polyphonic-chord-texture-disentanglement
c⃝Z. Wang, D. Wang, Y. Zhang, G. Xia. Licensed under
a Creative Commons Attribution 4.0 International License (CC BY 4.0).
Attribution:
Z. Wang, D. Wang, Y. Zhang, G. Xia, “Learning inter-
pretable representation for controllable polyphonic music generation”, in
Proc. of the 21st Int. Society for Music Information Retrieval Conf.,
Montréal, Canada, 2020.
we enforce the model to learn two fundamental factors
of polyphonic music: chord (content) and texture (style).
The former refers to the representation of the underlying
chord progression, and the latter includes chord arrange-
ment, rhythmic pattern, and melody contour. The current
design focuses on learning 8-beat long piano composition
segments under a variational autoencoder (VAE) frame-
work.
The core of the model design lies in the encoder. We
incorporate the encoder with two inductive biases for a
successful chord-texture disentanglement. The former
applies a rule-based chord recognizer and embeds the in-
formation into the ﬁrst half of the latent representation.
The latter regards music as 2-D images and uses a chord-
invariant convolutional network to extract the texture infor-
mation, storing it into the second half of the latent repre-
sentation. As for the decoder, we adopt the design from
PianoTree VAE [6], an architecture that can reconstruct
polyphonic music from the latent representation in a hi-
erarchical manner.
We further show that the interpretable representations
are general-purpose, empowering a wide spectrum of
controllable music generation. In this study, we explore
the following three scenarios:
Task 1: Compositional style transfer by swapping the
chord and texture factors of different pieces of music,
which can help us re-harmonize or re-arrange a music
piece following the style of another piece.
Task 2: Texture variation by sampling the texture fac-
tor while keeping the chord factor, which is analogous
to the creation of “Theme and Variations” form of com-
position.
Task 3: Accompaniment arrangement by predicting
the texture factor given the melody using a downstream
encoder-decoder generative model.
In sum, the contributions of our paper are as follows:
• We design a representation disentanglement method for
polyphonic music, which learns two interpretable fac-
tors: chord and texture.
• We show that the interpretable factors are general-
purpose features for controllable music generation,
which
reduces
the
necessity
to
design
heavily-
engineered control-speciﬁc model architectures. As far
as we know, this is the ﬁrst attempt to explicitly con-

trol the compositional texture feature for symbolic poly-
phonic music generation.
• We demonstrate that control methods are effective and
the quality of generated music is high. Some style trans-
ferred pieces are rated even higher than the original ones
composed by humans.
2. RELATED WORK
We review two techniques of automated music generation
related to our paper: controlled generation (in Section 2.1)
and representation disentanglement (in Section 2.2). For
a more general review of deep music generation, we refer
readers to [7,8].
2.1 Controlled Music Generation
Most existing learning-based methods regard controlled
music generation a conditional estimation problem. That
is, to model p(music|control), in which both music and
control are usually time-series features. Another approach
that is closely related to conditional estimation is to ﬁrst
learn the joint distribution p(music, control) and later on
force the value of control during the generation process.
The above two methods have been used in various tasks,
including generating chords based on the melody [9], cre-
ating the melody based on the chords [10,11], completing
the counterparts or accompaniment based on the melody or
chord [3,12–16], and producing the audio waveform based
on timbre features [17,18].
However, many abstract music factors, such as texture
and melody contour, could hardly be explicitly coded by
labels. Even if such labels are provided, the control still
does not allow continuous manipulation, such as sampling
and interpolation. Consequently, it remains a challenging
task to control music by more abstract factors without com-
plex heuristics [19].
2.2 Music Representation Disentanglement
Learning disentangled representations is an ideal solution
to the problem above, since: 1) representation learning
embeds discrete music and control sequences into a con-
tinuous latent space, and 2) disentanglement techniques
can further decompose the latent space into interpretable
subparts that correspond to abstract music factors. Recent
studies show that VAEs [20,21] are in general an effective
framework to learn the representations of discrete music
sequences, and the key to a successful disentanglement is
to incorporate proper inductive biases into the representa-
tion learning models [22].
Under a VAE framework, an inductive bias can be real-
ized in various forms, including constraining the encoder
[23–25], constraining the decoder [26], imposing multitask
loss functions [27, 28], and enforcing transformation in-
variant results during the learning process [29, 30]. This
study is based on our previous work Deep Music Anal-
ogy [27] in which we disentangle pitch and rhythm factors
for monophonic segments. We extend this idea to poly-
phonic composition while the model design is more similar
to [24].
3. MODEL
In this section, we introduce the model design and data
representation in detail. The goal is to learn the represen-
tations of 8-beat long piano compositions (with 1
4 beat as
the shortest unit) and disentangle the representations into
two interpretable factors: chord and texture.
Figure 1: The model diagram.
Figure 1 shows the overall architecture of the model.
It adopts a VAE framework and contains four parts: 1) a
chord encoder, 2) a chord decoder, 3) a texture encoder,
and 4) a PianoTree decoder. The chord encoder and chord
decoder can be seen as a standalone VAE which extracts
the latent chord representation zchd. On the other hand,
the texture encoder aims to extract the texture represen-
tation ztxt using a chord-invariant convolutional mapping.
Finally, the PianoTree decoder takes in both zchd and ztxt
and outputs the original music in a tree-structured data for-
mat.
3.1 Chord Encoder
The chord encoder ﬁrst applies rule-based methods [31,32]
to extract the chord progression under one-beat resolu-
tion. Each extracted chord progression is a 36 by 8 ma-
trix, where each column denotes a chord of one beat. Each
chord is a 36-D vector consisting of three parts: a 12-D
one-hot vector for the pitch class of the root, a 12-D one-
hot vector for the bass, and a 12-D multi-hot chroma vec-
tor.
The chord progression is then fed into a bi-directional
GRU encoder [21], and the last hidden states on both ends
of the GRU are concatenated and used to approximate the
posterior distribution of zchd. Following the assumption
of a standard VAE, zchd has a standard Gaussian prior and
follows an isotropic Gaussian posterior.
Note that although the chord progression here is ex-
tracted using algorithms, it can also be provided by ex-
ternal labels, in which case the whole model becomes a
conditional VAE [33].

3.2 Chord Decoder
The chord decoder reconstructs the chord progression from
zchd using another bi-directional GRU. The reconstruction
loss of a chord progression is computed as a summation of
8 beat-wise chord loss using cross entropy functions [34].
For each beat, the chord loss is deﬁned as the product of
three parts: 1) the root loss, 2) the bass loss, and 3) the
chroma loss. The root and bass are both considered 12-
way categorical distributions and the chroma is regarded
as 12 independent Bernoulli distributions.
3.3 Texture Encoder
The input of the texture encoder is an 8-beat segment of
polyphonic piece represented by an image-like data for-
mat slightly modiﬁed from the piano-roll [14]. Each 8-
beat segment is represented by a 128 by 32 matrix, where
each row corresponds to a MIDI pitch and each column
corresponds to 1
4 beat. The data entry at (p, t) records the
duration of the note if there is a note onset, and zero other-
wise.
The texture encoder aims to learn a chord-invariant rep-
resentation of texture by leveraging both the translation in-
variance property of convolution and the blurry effect of
max-pooling layers [35].
We use a convolutional layer
with kernel size 12 × 4 and stride 1 × 4, which is followed
by a ReLU activation [36] and max-pooling with kernel
size 4×1 and stride 4×1. The convolutional layer has one
input channel and 10 output channels. The convolutional
layer design aims at extracting a blurry “concept sketch”
of the polyphonic texture which contains minimum infor-
mation of the underlying chord. Ideally, when such blurry
sketch is combined with speciﬁc chord representation, the
decoder can identify its concrete pitches in a musical way.
The output of the convolutional layer is then fed into
a bi-directional GRU encoder to extract the texture repre-
sentation ztxt, similar to how we encode zchd introduced in
Section 3.1.
3.4 PianoTree Decoder
The PianoTree decoder takes the concatenation of zchd and
ztxt as input and decodes the music segment using the same
decoder structure invented in PianoTree VAE [6], a hierar-
chical model structure for polyphonic representation learn-
ing.
The decoder works as follows.
First, it generates
32 frame-wise hidden states (one for each 1
4 beat) using a
GRU layer. Then, each frame-wise hidden state is further
decoded into the embeddings of individual notes using an-
other GRU layer. Finally, the pitch and duration for each
note are reconstructed from the note embedding using a
fully-connected layer and a GRU layer, respectively. For
more detailed derivation and model design, we refer the
readers to [6].
3.5 Training Objective
Let x denote the input music piece and c = f(x) denote
the chord progression extracted by algorithm f(·). We as-
sume standard Gaussian priors of p(zchd) and p(ztxt), and
denote the output posteriors of chord encoder and texture
encoder by qφ(zchd|c), qψ(ztxt|x), the output distributions
of chord decoder and PianoTree decoder by pρ(c|zchd) and
pθ(x|zchd, ztxt). The objective of the model is:
L(φ, ψ, ρ, θ; x) =
−Ezchd∼qφ
ztxt∼qψ

log pρ(c|zchd) + log pθ(x|zchd, ztxt)

+ KL(qφ||p(zchd)) + KL(qψ||p(ztxt)).
(1)
4. CONTROLLED MUSIC GENERATION
In this section, we show some controlled generation exam-
ples of the three tasks mentioned in the introduction.
4.1 Compositional Style Transfer
By regarding chord progression content and texture style,
we can achieve compositional style transfer by swapping
the texture representations of different pieces. Figure 2
shows the transferred results ((c) & (d)) based on two 16-
bar samples ((a) & (b)) in the test set by swapping ztxt every
2 bars (without overlap).2
We see that such long-term style transfer is successful:
The generated segment (c) follows the chord progression
of (b) while mimicking the texture of (a), while (d) follows
the chord progression of (a) while mimicking the texture
of (b). As shown in the marked scores, the style transfer
is effective. E.g., the cut-offs, melody contours, and the
shape of the left-hand accompaniment are all preserved.
4.2 Texture Variation by Sampling
We can make variations of texture by sampling from ztxt
while keeping zchd.
Here, we investigate two sampling
strategies: sampling from the posterior qψ(ztxt|x), and
sampling from the prior p(ztxt).
Sampling from the posterior distribution qψ(ztxt|x)
yields reasonable variations as shown in Figure 3a. The
variations of the right-hand melody can be seen as an
improvisation following the chord progression and the
melody. On the contrary, there is only small variation in
the left-hand part, showing that the model regards the left-
hand accompaniment as the dominant feature of texture.
Sampling from the prior distribution p(ztxt) changes the
texture completely. Figure 3b shows a series of examples
of prior sampling under the same chord progression C-Am-
F-G. The resulting generations follow exactly the chord
progression but with new textures.
4.3 Accompaniment Arrangement
We use a downstream predictive model to achieve accom-
paniment arrangement. For this task, we provide extra vo-
cal melody tracks paired with the piano samples, and the
model learns to generate 16-bar piano accompaniment con-
ditioned on melody in a supervised fashion.
We encode the music every 2 bars (without overlap)
into latent representations. For the accompaniment, we
use the proposed model to compute the latent chord and
texture representation, denoted by zchd = [z(1)
chd, ..., z(4)
chd]
and ztxt = [z(1)
txt , ..., z(4)
txt ].
For the melody, we use the
2 The presented excerpts are converted from MIDI by the authors. The
chord labels are inferred from the original/generated samples.

(a) A real piece.
(b) The other real piece.
(c) The generated piece by combining ztxt from (a) and zchd from (b).
(d) The generated piece by combining ztxt from (b) and zchd from (a).
Figure 2: An example of compositional style transfer of 16-bar-long samples when k = 2.
EC2-VAE [27] to compute the latent pitch and rhythm rep-
resentations, denoted by zp = [z(1)
p , ..., z(4)
p ] and zr =
[z(1)
r
, ..., z(4)
r
]. Then, we adopt a vanilla Transformer [37]
to model p(ztxt, zchd|zp, zr), in which the encoder takes in
the condition and the decoder’s input is a shifted right ver-
sion [zchd, ztxt]. Both encoder and decoder inputs are incor-
porated with a positional encoding indicating the time po-
sitions and a learned factor embedding indicating the rep-
resentation type (i.e., pitch, rhythm, chord or texture).
Figure 4 shows an example of accompaniment arrange-
ment, where the ﬁrst staff shows the melody and the sec-
ond staff shows the piano accompaniment. In this case, the
whole melody, together with the complete chord progres-
sion and the ﬁrst 2 bars of accompaniment are given. The
chord conditioning is done by forcing the decoded chord

(a) An example of posterior sampling of ztxt of the ﬁrst 8 bars of the segment (a) in Figure 2
(b) An example of prior sampling of ztxt under given chord progression C-Am-F-G. Each two-bar segment is independently sampled,
having different texture.
Figure 3: Examples of texture variations via posterior sampling and prior sampling.
Figure 4: An example of accompaniment arrangement conditioned on melody, chord progression, and ﬁrst 2 bars of
accompaniment.
representation to match the given input during inference
time. (A similar trick is used in [15].) From Figure 4, we
see that the model predicts a similar texture to the given
accompaniment. Moreover, it ﬁlls in a secondary melody
line as a transition when the lead melody is rest.
Note that the arrangement can be generated in a ﬂexi-
ble way by conditioning on different sets of latent factors.
Much longer examples and more conditioning settings are
available on our github page.
5. EXPERIMENTS
5.1 Dataset and Training
We train our model on the POP909 dataset [38], which
contains about 1K MIDI ﬁles of pop songs (including
paired vocal melody and piano accompaniment). We fur-
ther extract the chord annotations using [31, 32]. We only
keep the pieces with 2
4 and 4
4 meters and cut them into
8-beat music segments (so that each data sample in our
experiment contains 32 time steps under 16th note resolu-
tion). In all, we have 66K samples. We randomly split
the dataset (at song-level) into training set (90%) and test
set (10%). All training samples are further augmented by
transposing to all 12 keys.
In our experiment, the VAE model uses 256, 512, and
512 hidden dimensions for the GRUs in chord encoder,
chord decoder and texture encoder respectively. The latent
dimension of zchd and ztxt are both 256. The model size of
the PianoTree decoder is the same as the implementation
in the original paper [6]. The transformer model has the
following size: hidden dimension = 256, number of layers
= 4 and number of heads = 8.
For both models, we use Adam optimizer [39] with a
scheduled learning rate from 1e-3 to 1e-5. Moreover, for
the VAE model, we use KL-annealing [40], i.e. setting a
weight parameter for the KL-divergence loss starting from
0 to 0.1. We set batch size to be 128 and the training con-
verges within 6 epochs. For the downstream transformer
model, we use 12K warm-up steps for learning rate up-
date [41]. We use the same batch size and the model con-
verges within 40 epochs.
5.2 Objective Measurement
When zchd and ztxt are well disentangled, small variations
over the note pitches of the original music should lead to a
larger change on zchd, while variations of rhythm will inﬂu-
ence more on ztxt. Following this assumption, we adopt a
disentanglement evaluation via data augmentation method
used in [42] and further developed in [27].
We deﬁne Fi as the operation of transposing all the
notes by i semitones, and use the L1-norm to measure the
change of latent z after augmentation. Figure 5a shows a
comparison between Σ|∆zchd| and Σ|∆ztxt| when we apply
Fi to all the music pieces in the test set (where i ∈[1, 12]).
It is conspicuous that when augmenting pitch in a small
range, the change of zchd is much larger than the change of
ztxt. At the same time, the change of ztxt gets higher as the

augmentation scale increases. Similar to the result in [27],
the change of zchd reﬂects human pitch perception as zchd is
very sensitive to a tritone transposition, and least sensitive
for a perfect octave.
(a) A comparison between ∆zchd, ∆ztxt after pitch transposition
on all notes.
(b) A comparison among ∆zchd, ∆ztxt after beat-wise pitch trans-
position and texture augmentation with different probabilities.
Figure 5: Results of objective measurement.
We further deﬁne Pi as the function to randomly trans-
pose all the notes in one beat either up or down one semi-
tone under a certain probability i, and Ri as the function
to randomly reduce the note duration by half. Figure 5b
shows a comparison between Σ|∆zchd| and Σ|∆ztxt| when
we apply Pi and Ri to all the music pieces in our test set
(where i ∈[0.1, 1.0]).
For each value of i in the ﬁgure 5b, the ﬁrst and sec-
ond bars demonstrate Σ|∆zchd| and Σ|∆ztxt| caused by Pi
function, while the third bar indicates Σ|∆ztxt| caused by
Ri function. (We did not show Σ|∆zchd| caused by Ri
since they are all zero.) It again proves that the chord repre-
sentation is more sensitive than texture representation un-
der pitch variations, and conversely, texture representation
is more sensitive than chord representation under rhythm
variations.
5.3 Subjective Evaluation
Besides objective measurement, we conduct a survey to
evaluate the musical quality of compositional style transfer
(see Section 4.1). Each subject listens to ten 2-bar pieces
with different chord progressions, each paired with 5 style-
transfer versions generated by swapping the texture repre-
sentation with a random sample from the test set. In other
words, each subject evaluates 10 groups of samples, each
of which contains 6 versions of textures (1 from the orig-
inal piece and 5 from other pieces) under the same chord
progression. Both the order of groups and the sample order
within each group are randomized. After listening to each
sample, the subjects rate them based on a 5-point scale
from 1 (very low) to 5 (very high) according to three crite-
ria: creativity, naturalness and musicality.
Figure 6: Subjective evaluation results. Here “TFRed: xth
largest” denotes the xth (largest) order statistic of the trans-
ferred segments.
A total of 36 subjects (26 females and 10 males) partic-
ipated in the survey. Figure 6 shows the comparison result
among the original pieces (indicated by the orange bars)
and the transferred pieces in terms of their mean and order
statistics. The heights of bars represent averaged ratings
across the subjects and the error bars represent the conﬁ-
dence intervals computed via paired t-test [43]. The result
shows if we randomly transfer a piece’s texture 5 times, the
best result is signiﬁcantly better than the original version
(with p-value < 0.005), and there are only marginal differ-
ences between the second-largest statistics and the original
(with p-value > 0.05) in terms of creativity and musical-
ity. We also see that on average the transferred results are
still rated lower than the original ones. How to automati-
cally decide the quality of a transferred result is considered
a future work.
6. CONCLUSION AND FUTURE WORK
In conclusion, we contributed an effective algorithm to dis-
entangle polyphonic music representation into two inter-
pretable factors, chord and texture, under a VAE frame-
work. Such interpretable representations serve as an intu-
itive human-computer co-creation interface, by which we
can precisely manipulate individual factors to control the
ﬂow of the generated music. In this paper, we demon-
strated three ways to interact with the model, including
compositional style transfer via swapping the latent codes,
texture variation by sampling from the latent distribu-
tion, accompaniment arrangement using downstream con-
ditional prediction, and there are potentially many more.
We hope this work can shed light on the ﬁeld of con-
trollable algorithmic composition in general, especially on
the paradox between model complexity and model inter-
pretability.
We acknowledge that the learned music factors are still
very basic. In the future, we plan to extract more abstract
and longer-range features using hierarchical models. We
also plan to explore more ways to control the music gener-
ation for practical usage.

7. REFERENCES
[1] K. Chen, W. Zhang, S. Dubnov, G. Xia, and W. Li,
“The effect of explicit structure encoding of deep neu-
ral networks for symbolic music generation,” in 2019
International Workshop on Multilayer Music Repre-
sentation and Processing (MMRP).
IEEE, 2019, pp.
77–84.
[2] C. A. H. et al., “Music transformer: Generating mu-
sic with long-term structure,” in 7th International Con-
ference on Learning Representations (ICLR), New Or-
leans, LA, USA, 2019.
[3] Y.-S. Huang and Y.-H. Yang, “Pop music transformer:
Generating music with rhythm and harmony,” arXiv
preprint arXiv:2002.00212, 2020.
[4] J.-P. Briot and F. Pachet, “Deep learning for music gen-
eration: challenges and directions,” Neural Computing
and Applications, vol. 32, no. 4, pp. 981–993, 2020.
[5] S. Dai, Z. Zhang, and G. G. Xia, “Music style transfer:
A position paper,” arXiv preprint arXiv:1803.06841,
2018.
[6] Z. Wang, Y. Zhang, Y. Zhang, J. Jiang, R. Yang,
J. Zhao, and G. Xia, “Pianotree vae: Structured repre-
sentation learning for polyphonic music,” in Proceed-
ings of 21st International Conference on Music Infor-
mation Retrieval (ISMIR), virtual conference, 2020.
[7] J.-P. Briot, G. Hadjeres, and F.-D. Pachet, “Deep learn-
ing techniques for music generation–a survey,” arXiv
preprint arXiv:1709.01620, 2017.
[8] J.-P. Briot, “From artiﬁcial neural networks to deep
learning for music generation–history, concepts and
trends,” arXiv preprint arXiv:2004.03586, 2020.
[9] I. Simon, D. Morris, and S. Basu, “Mysong: automatic
accompaniment generation for vocal melodies,” in Pro-
ceedings of the SIGCHI conference on human factors
in computing systems, 2008, pp. 725–734.
[10] L.-C. Yang, S.-Y. Chou, and Y.-H. Yang, “Midinet:
A convolutional generative adversarial network for
symbolic-domain music generation,” arXiv preprint
arXiv:1703.10847, 2017.
[11] K. Chen, W. Zhang, S. Dubnov, G. Xia, and W. Li,
“The effect of explicit structure encoding of deep neu-
ral networks for symbolic music generation,” in 2019
International Workshop on Multilayer Music Repre-
sentation and Processing (MMRP).
IEEE, 2019, pp.
77–84.
[12] G. Hadjeres, F. Pachet, and F. Nielsen, “Deepbach: a
steerable model for bach chorales generation,” in Pro-
ceedings of the 34th International Conference on Ma-
chine Learning-Volume 70.
JMLR. org, 2017, pp.
1362–1371.
[13] H. Zhu, Q. Liu, N. J. Yuan, C. Qin, J. Li, K. Zhang,
G. Zhou, F. Wei, Y. Xu, and E. Chen, “Xiaoice band:
A melody and arrangement generation framework for
pop music,” in Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery &
Data Mining, 2018, pp. 2837–2846.
[14] H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang,
“Musegan: Multi-track sequential generative adversar-
ial networks for symbolic music generation and accom-
paniment,” in Thirty-Second AAAI Conference on Arti-
ﬁcial Intelligence, 2018.
[15] C. Donahue, H. H. Mao, Y. E. Li, G. W. Cot-
trell, and J. McAuley, “Lakhnes: Improving multi-
instrumental music generation with cross-domain pre-
training,” arXiv preprint arXiv:1907.04868, 2019.
[16] I.
Simon,
A.
Roberts,
C.
Raffel,
J.
Engel,
C. Hawthorne,
and D. Eck,
“Learning a latent
space
of
multitrack
measures,”
arXiv
preprint
arXiv:1806.00195, 2018.
[17] S. Huang, Q. Li, C. Anil, X. Bao, S. Oore, and
R. B. Grosse, “Timbretron: A wavenet (cyclegan (cqt
(audio))) pipeline for musical timbre transfer,” arXiv
preprint arXiv:1811.09620, 2018.
[18] O. Kwon, I. Jang, C. Ahn, and H.-G. Kang, “Emotional
speech synthesis based on style embedded tacotron2
framework,” in 2019 34th International Technical Con-
ference on Circuits/Systems, Computers and Commu-
nications (ITC-CSCC).
IEEE, 2019, pp. 1–4.
[19] S. Lattner, M. Grachten, and G. Widmer, “Imposing
higher-level structure in polyphonic music generation
using convolutional restricted boltzmann machines and
constraints,” arXiv preprint arXiv:1612.04742, 2016.
[20] D. P. Kingma and M. Welling, “Auto-encoding varia-
tional bayes,” arXiv preprint arXiv:1312.6114, 2013.
[21] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and
D. Eck, “A hierarchical latent vector model for learn-
ing long-term structure in music,” arXiv preprint
arXiv:1803.05428, 2018.
[22] F.
Locatello,
S.
Bauer,
M.
Lucic,
G.
Rätsch,
S. Gelly, B. Schölkopf, and O. Bachem, “Challeng-
ing common assumptions in the unsupervised learn-
ing of disentangled representations,” arXiv preprint
arXiv:1811.12359, 2018.
[23] Y.-J. Luo, K. Agres, and D. Herremans, “Learn-
ing
disentangled
representations
of
timbre
and
pitch for musical instrument sounds using gaussian
mixture variational autoencoders,”
arXiv preprint
arXiv:1906.08152, 2019.
[24] Y. Wu, T. Carsault, E. Nakamura, and K. Yoshii,
“Semi-supervised
neural
chord
estimation
based
on a variational autoencoder with discrete labels
and continuous textures of chords,” arXiv preprint
arXiv:2005.07091, 2020.

[25] T. Akama, “Controlling symbolic music generation
based on concept learning from domain knowledge.”
in ISMIR, 2019, pp. 816–823.
[26] K. Choi and K. Cho, “Deep unsupervised drum tran-
scription,” arXiv preprint arXiv:1906.03697, 2019.
[27] R. Yang, D. Wang, Z. Wang, T. Chen, J. Jiang, and
G. Xia, “Deep music analogy via latent representation
disentanglement,” arXiv preprint arXiv:1906.03626,
2019.
[28] G. Brunner, A. Konrad, Y. Wang, and R. Wattenhofer,
“Midi-vae: Modeling dynamics and instrumentation
of music with applications to style transfer,” arXiv
preprint arXiv:1809.07600, 2018.
[29] S. Lattner, M. Dörﬂer, and A. Arzt, “Learning complex
basis functions for invariant representations of audio,”
arXiv preprint arXiv:1907.05982, 2019.
[30] M. F. Mathieu, J. J. Zhao, J. Zhao, A. Ramesh,
P. Sprechmann, and Y. LeCun, “Disentangling factors
of variation in deep representation using adversarial
training,” in Advances in neural information process-
ing systems, 2016, pp. 5040–5048.
[31] B. Pardo and W. P. Birmingham, “Algorithms for
chordal analysis,” Computer Music Journal, vol. 26,
no. 2, pp. 27–49, 2002.
[32] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,
O. Nieto, D. Liang, D. P. Ellis, and C. C. Raffel,
“mir_eval: A transparent implementation of common
mir metrics,” in In Proceedings of the 15th Interna-
tional Society for Music Information Retrieval Confer-
ence, ISMIR.
Citeseer, 2014.
[33] X. Yan,
J. Yang,
K. Sohn,
and H. Lee,
“At-
tribute2image: Conditional image generation from vi-
sual attributes,” in European Conference on Computer
Vision.
Springer, 2016, pp. 776–791.
[34] P.-T. De Boer, D. P. Kroese, S. Mannor, and R. Y. Ru-
binstein, “A tutorial on the cross-entropy method,” An-
nals of operations research, vol. 134, no. 1, pp. 19–67,
2005.
[35] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Ima-
genet classiﬁcation with deep convolutional neural net-
works,” in Advances in neural information processing
systems, 2012, pp. 1097–1105.
[36] V. Nair and G. E. Hinton, “Rectiﬁed linear units im-
prove restricted boltzmann machines,” in Proceedings
of the 27th international conference on machine learn-
ing (ICML-10), 2010, pp. 807–814.
[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,
“Attention is all you need,” in Advances in neural in-
formation processing systems, 2017, pp. 5998–6008.
[38] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai,
X. Gu, and G. Xia, “Pop909: A pop-song dataset for
music arrangement generation,” in Proceedings of 21st
International Conference on Music Information Re-
trieval (ISMIR), virtual conference, 2020.
[39] D. P. Kingma and J. Ba, “Adam: A method for stochas-
tic optimization,” arXiv preprint arXiv:1412.6980,
2014.
[40] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai,
R. Jozefowicz, and S. Bengio, “Generating sen-
tences from a continuous space,” arXiv preprint
arXiv:1511.06349, 2015.
[41] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng,
C. Xing, H. Zhang, Y. Lan, L. Wang, and T.-Y. Liu,
“On layer normalization in the transformer architec-
ture,” arXiv preprint arXiv:2002.04745, 2020.
[42] H. Kim and A. Mnih, “Disentangling by factorising,”
arXiv preprint arXiv:1802.05983, 2018.
[43] H. Hsu and P. A. Lachenbruch, “Paired t test,” Ency-
clopedia of Biostatistics, vol. 6, 2005.
