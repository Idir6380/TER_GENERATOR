LEARNING LONG-TERM MUSIC REPRESENTATIONS VIA
HIERARCHICAL CONTEXTUAL CONSTRAINTS
Shiqi Wei1,2
Gus Xia2
1 School of Data Science, Fudan University
2 Music X Lab, Computer Science Department, New York University Shanghai
sqwei19@fudan.edu.cn, gxia@nyu.edu
ABSTRACT
Learning symbolic music representations, especially dis-
entangled representations with probabilistic interpreta-
tions, has been shown to beneÔ¨Åt both music understanding
and generation. However, most models are only applica-
ble to short-term music, while learning long-term music
representations remains a challenging task. We have seen
several studies attempting to learn hierarchical representa-
tions directly in an end-to-end manner, but these models
have not been able to achieve the desired results and the
training process is not stable. In this paper, we propose
a novel approach to learn long-term symbolic music rep-
resentations through contextual constraints. First, we use
contrastive learning to pre-train a long-term representation
by constraining its difference from the short-term represen-
tation (extracted by an off-the-shelf model). Then, we Ô¨Åne-
tune the long-term representation by a hierarchical predic-
tion model such that a good long-term representation (e.g.,
an 8-bar representation) can reconstruct the corresponding
short-term ones (e.g., the 2-bar representations within the
8-bar range). Experiments show that our method stabilizes
the training and the Ô¨Åne-tuning steps. In addition, the de-
signed contextual constraints beneÔ¨Åt both reconstruction
and disentanglement, signiÔ¨Åcantly outperforming the base-
lines.
1. INTRODUCTION
Deep music representation learning have been proven to
be a powerful tool for high-quality symbolic music gen-
eration [1]. The learned representations can be directly
fed into downstream predictive models such as LSTMs [2]
and Transformers [3] to achieve more coherent results than
note-based or event-based generation [4‚Äì6] Furthermore,
when a representation learning model has a probability in-
terpretation, the representation can then be easily interpo-
lated or resampled to create new music pieces. Recently,
several studies further disentangle music representations
into interpretable factors (such as pitch, rhythm, chord and
texture) to achieve a more controllable and interactive mu-
sic generation [5,7,8]. For example, we can keep the pitch
¬© Shiqi Wei, Gus Xia. Licensed under a Creative Commons
Attribution 4.0 International License (CC BY 4.0). Attribution:
Shiqi
Wei, Gus Xia, ‚ÄúLearning long-term music representations via hierarchi-
cal contextual constraints‚Äù, in Proc. of the 22nd Int. Society for Music
Information Retrieval Conf., Online, 2021.
factor of a melody while resampling its rhythm factor to
achieve theme variation. We can also interpolate the pitch
factor for a smooth music morphing [7].
Despite the above mentioned progress [9‚Äì11], most ex-
isting work applies only to short music segments with a
length of several beats, while learning long-term repre-
sentations remains a challenging task. In particular, stud-
ies have shown that even for monophonic melodies, "Ô¨Çat"
model designs (e.g., using long-range sequential encoders)
have difÔ¨Åculty remembering a complete music phrase at
once.
Some other studies have attempted to solve this
problem by building another layer of hierarchy on top
of short-range Ô¨Çat models, learning short-term and long-
term representations simultaneously in an end-to-end man-
ner [1, 12]. However, as the model expressivity increases
with the number of layers, models also become much more
difÔ¨Åcult to train.
We argue that the main problem with current methods
is the lack of proper inductive bias, and in this paper we
propose a new method for learning long-term, phrase-level
symbolic music representations through contextual con-
straints. The method consists of two stages pre-training
and Ô¨Åne-tuning, with two steps in each stage. In the pre-
training stage, we Ô¨Årst adopt EC2-VAE [7] to learn bar-
level, disentangled latent pitch and rhythm representations.
Then, we apply the same model to learn phrase-level repre-
sentation but with contrastive losses to constrain the differ-
ence between phrase-level and bar-level representations. It
is indeed difÔ¨Åcult to learn phrase-level representations di-
rectly using bar-level models, but the additional contrastive
constraint can serve as a useful inductive bias to help Ô¨Ånd a
reasonable solution that can subsequently be improved by
Ô¨Åne-tuning. During the Ô¨Åne-tuning stage, we replace the
pre-trained decoder with a hierarchical prediction model
that forces the phrase-level representation to reconstruct
the bar-level ones. This is achieved by Ô¨Årst tuning only
the new hierarchical decoder (while Ô¨Åxing the pre-trained
encoder) and then tuning the whole network. During these
two steps, structured contrastive loss is applied to stabilize
the learning process.
Experiments show that the proposed method signiÔ¨Å-
cantly outperforms the baselines and successfully learns
disentangled pitch and rhythm representations for 8-bar
long phrases (32 beats in 4/4 meter) without increasing the
latent dimensionality. To our knowledge, this is also the
Ô¨Årst generative model that achieves phrase-level compo-
sition style transfer, latent factor interpolation, and theme
738

variation. In sum, our contributions are as follows:
‚Ä¢ We demonstrate the importance of structured contextual
constraints in learning long-term disentangled represen-
tations. Our approach only requires reasonable amount
of data to train and could learn compact latent represen-
tation.
‚Ä¢ We show that the proposed Structured InfoNCE loss ef-
fectively expresses the contextual constraints, stabilizes
the training of long-range models and helps the model
converge faster.
‚Ä¢ Our model achieves phrase-level music style transfer, la-
tent factor interpolation, and theme variation.
2. RELATED WORK
We review two realms of research related to our work
on long-term music-representation learning: contrastive
learning, which is the main method to stabilize the training
process, and hierarchical music modeling, which is related
to our Ô¨Åne-tuning model.
2.1 Contrastive Learning
Contrastive learning (CL) is an efÔ¨Åcient method in self-
supervised learning [13‚Äì15], serving as regularization to
latent representations.
For example, NCE-based con-
trastive losses [16,17] have been widely used and achieved
good results in natural language processing. Contrastive
predictive coding (CPC) [18] and Deep Infomax (DIM)
[19] explore the relation between minimizing a contrastive
learning loss and maximizing a lower bound of the mu-
tual information. In DIM, global feature is connected with
local feature to learn more abstract and informative repre-
sentations.
2.2 Hierarchical Music Representation Learning
The hierarchical nature of music has been studied for a
long time [20‚Äì23].
Recently, we see some efforts on
learning long-term music representations using hierarchi-
cal modeling [12, 24, 25]. The basic idea is that since a
Ô¨Çat model design can only effectively learn shot-term rep-
resentations, we can stack more layers on top of the short-
term representations module for long-term representations.
Existing works include MusicVAE [1], Music Transformer
VAE [12], Jukebox [26], etc. However, experiments show
that unless we have a huge amount of data, the model is
in general very difÔ¨Åcult to train. In this study, we pro-
vide a two-stage algorithm with contrastive loss as a better
learning strategy. Also, no model so far has achieved dis-
entanglement for long-term representation as done in this
study.
3. METHODOLOGY
In this section, we introduce our algorithm in detail. Con-
ceptually, it consists of two stages, each with two steps.The
Ô¨Årst stage is pre-training:
‚Ä¢ In step 1, we simply adopt EC2-VAE [7], an exist-
ing music representation disentanglement model, to
extract short-term pitch and rhythm representations.
‚Ä¢ In step 2, we build Long-EC2-VAE, a long-term ver-
sion of the model and train it with an extra contextual
constraint using the proposed Structured InfoNCE
loss. Intuitively, this loss prevents the learned long-
term representations from deviating too far from cor-
responding well-trained short-term representations.
The second stage is Ô¨Åne-tuning, in which we build a hierar-
chical representation-learning model by combining the en-
coder of Long-EC2-VAE with a hierarchical decoder. We
name this model after Hierarchical-EC2-VAE.
‚Ä¢ In step 1, we only train the hierarchical decoder to
ensure the predictive power of the long-term repre-
sentation.
‚Ä¢ In step 2, we train the whole hierarchical network for
a better long-term pitch-rhythm disentanglement.
3.1 Pre-training by Contrastive Learning
The model of the pre-training stage, Long-EC2-VAE, is
shown in Figure 1. It is built upon an off-the-shelf music
representation model, EC2-VAE [7], which can effectively
disentangle pitch and rhythm factors for short music seg-
ments by cutting the latent representation into two parts
and pairing one part with a local rhythm decoder. In Fig-
Figure 1: The model architecture of Long-EC2-VAE in
the pre-training stage, where the right-hand-side is short-
term model with parameter Ô¨Åxed and the left-hand-side
is the long-term model.
The dotted lines denote con-
trastive losses, whose weighting matrices are joined opti-
mized with the parameters on the left-hand-side networks.
ure 1, the right-hand-side part is a literal copy of the EC2-
VAE encoder (with parameters Ô¨Åxed) to extract short-term
representations, while the left-hand-side part is a simple
adaptation of EC2-VAE for long-term music by lengthen-
ing its temporal receptive Ô¨Åeld. Note that the left part alone
is not able to learn long-term representations, and our goal
is to assist it using contrastive learning. Formally, the loss
function of Long-EC2-VAE is:
L = LLong-EC2-VAE + LStructured InfoNCE,
(1)
where LLong-EC2-VAE is the same as in the original EC2-
VAE model (which contains the KL loss, the rhythm loss
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
739

Figure 2: The model architecture of Hierarchical-EC2-VAE in the Ô¨Åne-tuning stage. The training follows two steps.
and the overall reconstruction loss). The Structured In-
foNCE loss expresses the contextual constraint. It is de-
veloped from InfoNCE [18] loss, and it is structured since
the compared representation pairs are extracted from mu-
sic segments of different length, one is long term and the
other is short term. Formally:
LStructured InfoNCE =
‚àíln
exp

zT
L,fW ÀÜz+
S,f/œÑ

exp

zT
L,fW ÀÜz+
S,f/œÑ

+ PK
i=1 exp

zT
L,fW ÀÜz‚àí
S,f/œÑ
 ,
(2)
where zL,f and W are the normalized long-term represen-
tations and weighing matrix we need to learn. f = {p, r}
indicates whether it is the pitch or rhythm factor. Like-
wise, we use ÀÜzS,f to denote the short-term representations
extracted by right-hand-side model. K and œÑ are hyper-
parameters. K is the amount negative samples and œÑ is the
temperature parameter.
In speciÔ¨Åc, the short-term melodies are half as long as
long-term ones. The positive samples ÀÜz+
S,f are in the cases
that the corresponding short-term melody is a part of the
long-term melody and f takes the same value as in zL,f,
while the negative samples are not in this case. Also, the
long-term and short-term representations share the same
dimensionality.
3.2 Fine-tuning with Hierarchical Generation
Figure 2 shows the architecture of the Ô¨Åne-tuning model,
Hierarchical-EC2-VAE, where the two subÔ¨Ågures illustrate
the two training steps. Here, the encoder design is the same
as in the Long-EC2-VAE model, while the decoder is a hi-
erarchical predictive model with three layers. The Ô¨Årst two
layers are new designed and the last layer is an aggregation
of several EC2-VAE decoders sharing the same parame-
ters. Given the disentangled long-term (phrase-level) rep-
resentations, it Ô¨Årst decodes intermediate-level representa-
tions, then decodes bar-level representations, and Ô¨Ånally
reconstructs concrete rhythm and music tokens.
Compared to the phrase-level representation, the tem-
poral receptive Ô¨Åelds of the intermediate-level represen-
tations all shrink to a half, but at the same time their
number doubles in order to cover the same range of mu-
sic. The same relationship holds between intermediate and
bar-level representations.
In particular, a phrase means
8 bar (in 4/4 meter, 32 beats) in our design, so that
the intermediate-level and bar-level mean 4-bar and 2-bar
melody segments (a length which the original EC2-VAE
model can handle), respectively. All levels of latent repre-
sentations share the same dimensionality.
In the Ô¨Årst step of training (Figure 2(a)), the encoder is
a literal copy from the Long-EC2-VAE model and we only
train the hierarchical decoder. Formally, the loss function
is:
Lstep1 = LHierarchical‚àíEC2-VAE Decoder + LInfoNCE,
(3)
where the Ô¨Årst term refers to the reconstruction losses
adopted from the EC2-VAE model, and the second term
is deÔ¨Åned as:
LInfoNCE =
‚àíln
exp

zT
l,fW ÀÜz+
l,f/œÑ

exp

zT
l,fW ÀÜz+
l,f/œÑ

+ PK
i=1 exp

zT
l,fW ÀÜz‚àí
l,f/œÑ
 ,
(4)
where zl,f are the normalized hierarchical representations
we need to learn with l = {intermediate, bar} indicating
the level of representation and other notations follow the
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
740

same meaning as in Eq.(2). Here, both positive ÀÜz+
l,f and
negative ÀÜz‚àí
l,f samples are normalized representations com-
puted from a pre-trained EC2-VAE, in which the positive
samples are in the cases that the ÀÜz+
l,f and zl,f are computed
based on the same music segment and have the same value
of l and f, while ÀÜz‚àí
l,f are not in this case.
After the Ô¨Årst step achieves a reasonable accuracy, we
proceed to step 2 (Figure 2(b)), unfreezing the encoder
and training the whole hierarchical representation-learning
model with:
Lstep2 = Lstep1 + LStructured InfoNCE
+ Œ≤LKL phrase,
(5)
where the Ô¨Årst two terms are deÔ¨Åned in Eq. (3) and Eq. (2)
respectively. LKL phrase is KL divergence to only regular-
ize the phrase-level representations by a normal distribu-
tion. The value Œ≤ controls the degree of KL divergence
penalty.
4. EXPERIMENTS
4.1 Dataset and data format
We train our model on Nottingham Database [27] and
POP909 database [28].
Our dataset contains 2154
melodies (at song level) in total. We randomly split these
pieces into 2 subsets: 90% songs for training and 10%
songs for test. The data format is designed as the same
as in [7] in which 4 bar or 8 bar melodies are formalized
as sequences of 130-dimensional one-hot embedding vec-
tors and 16-beat and 32-beat rhythm pattern is represented
by a sequence of 3-dimensional one-hot embedding vec-
tors. Each vector in the melody sequence denotes a 1
4-beat
unit. The Ô¨Årst 128 dimensions of this vector denote 128
MIDI-format pitches from 0 to 127, the 129th dimension
is the holding state for longer note duration, and the last
dimension is kept for rest. The three dimensions of rhythm
pattern vectors represent the onset of any pitch, a holding
state, and rest, respectively.
4.2 Implementation Details
All of our models are trained using Adam optimizer [29]
with a scheduled learning rate from 1e-3 to 1e-5. The batch
size is 128 in the pre-training stage and is 64 in the Ô¨Åne-
tuning stage. We do normalization on representations in
Eq.(2) and (4) to make the training process more stable.
The representations fed into decoders are original repre-
sentation without normalization.
4.2.1 Pre-training
In the pre-training stage, we simply adopt the structure of
EC2-VAE [7] to model 4 bar and 8 bar EC2-VAE. Each
model comprises an encoder with a bi-directional GRU
layer, a rhythm decoder with a GRU layer, and a global
decoder with a GRU layer. We set the hidden dimension of
the GRU in the encoder and decoders to 2048. The latent
dimension is 128 for disentangled pitch representations
and 128 for disentangled rhythm representations for each
range model. For LStructured InfoNCE depicted in Eq. (2), we
set K to 512 and œÑ to 1. The positive samples for Eq. (2)
and Eq. (4) are the representations of 1-4th, 3-6th and 5-8th
bar from well-trained 4 bar EC2-VAE. Actually, even when
training the 4-bar EC2-VAE (right-hand side of Figure 1),
we use a similar constrastive loss as in Eq. (2) where the
positive samples are representations of 1-2th, 2-3th, 3-4th
bar from well-trained 2 bar (original) EC2-VAE [7] .
4.2.2 Fine-tuning
Hierarchical-EC2-VAE model consists of a long-term (8
bar) EC2-VAE encoder, 4 GRU layers, and an aggregation
of 2 bar EC2-VAE decoders. We Ô¨Årst train the hierarchical
model with Ô¨Åxed 8 bar EC2-VAE encoder from pre-trained
stage for around 25 epochs. Then we train the whole model
without Ô¨Åxing parameters. We set the hidden dimension of
4 GRU layers to 1024. We set K to 256 and œÑ to 1 for both
Structured InfoNCE loss and InfoNCE loss and set Œ≤ to 0.1
in Eq. (5).
4.3 Objective Evaluation
We objectively evaluate the model in terms of reconstruc-
tion accuracy, training stability, and disentanglement.
4.3.1 Reconstruction Accuracy
Table 1 shows that the reconstruction accuracy of the pro-
posed models (2nd an 3rd rows) signiÔ¨Åcantly outperform
the baseline, a vanilla EC2-VAE applied to 8-bar melody
(Ô¨Årst row). The last two rows show the results of two ab-
lation settings of Hierarchical-EC2-VAE: one without the
contrastive loss and the other without Ô¨Årst Ô¨Åxing the pa-
rameters of encoder and directly train the model end-to-
end. We see that the proposed Structured InfoNCE or In-
foNCE losses play a vital role for an accurate reconstruc-
tion and the two-step training strategy improves the result
marginally.
Method
Recon.Acc Rhythm Recon.Acc
Baseline
0.772
0.847
Long-EC2-VAE
0.992
0.995
H-EC2-VAE(ours)
0.997
0.995
H-EC2-VAE(w-o CL)
0.584
0.599
H-EC2-VAE(w-o Ô¨Åxed)
0.991
0.989
Table 1: A comparison on reconstruction accuracy of dif-
ferent models.
4.3.2 Training stability
Figure 3: Experimental results of overall reconstruction
and rhythm accuracy on the test set.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
741

Comparing the accuracy curves of the proposed Long-
EC2-VAE with the baseline as illustrated in Figure 3, we
Ô¨Ånd that the proposed Long-EC2-VAE converges more
quickly during training. This indicates that the proposed
training strategy leads to a better initialization and makes
the performance of the model Ô¨Çuctuate less during training.
4.3.3 Disentanglement Evaluation
We evaluate the disentanglement performance of models
using a disentanglement evaluation method adopted in [7]
and [30]. The method randomly transposes all the notes
of the input data by i(i ‚àà[1, 12]) semitones while keep-
ing the rhythm and underlying chord unchanged and then
measures the variation of disentangled representations.We
denote Œ£|‚àÜzp| and Œ£|‚àÜzr| as the variation of zp and zr .
1
2
3
4
5
 6
7
8
9
10
11
12
0
50
100
150
200
250
Pitch
Rhythm
(a) Baseline model (8 bar)
1
2
3
4
5
 6
7
8
9
10
11
12
0
50
100
150
200
250
Pitch
Rhythm
(b) Long-EC2-VAE model (8 bar)
1
2
3
4
5
 6
7
8
9
10
11
12
0
50
100
150
200
250
Pitch
Rhythm
(c) Hirarchical-EC2-VAE model (8 bar)
Figure 4: The comparison between Œ£|‚àÜzp| and Œ£|‚àÜzr| af-
ter transposition. The numbers show the pitch augmented
by 12 semitones in each sub-Ô¨Ågure from left to right.
As shown in Figure 4, values of Œ£|‚àÜzp| of the pro-
posed Hierarchical-EC2-VAE are relatively high while
Œ£|‚àÜzr| maintains in a signiÔ¨Åcantly low level. This indi-
cates that the pitch and rhythm representations of the pro-
posed Hierarchical-EC2-VAE are well-disentangled as the
change of notes has a tiny impact on zr. Similarly, we can
intuitively Ô¨Ånd in the Ô¨Ågure that the disentanglement per-
formance of the proposed Hierarchical-EC2-VAE is much
better than the baseline and also outperforms the proposed
Long-EC2-VAE.
4.4 Music generative examples
In this section, we show some music generation results
by manipulating the disentangled phrase-level pitch and
rhythm representations in three different ways: style trans-
fer via swapping the representation, rhythm morphing via
interpolating the representation, and theme variation via
representation posterior sampling.
4.4.1 Phrase-level composition style transfer
We cross-swap the disentangled pitch and rhythm factors
zp and zr of two 8-bar melodies A and B and then ob-
tain generative pieces C and D. The results are shown in
Figure 5, in which we see that both of the two generative
pieces perfectly inherit target rhythm patterns. Besides,
these generative melodies vary slightly from the source
melody and these variations tend to sound creative, i.e. the
appearance of embellished notes.
(a) Melody A
(b) Melody B
(c) Generated by ùëßùëùfrom A and ùëßùëüfrom B
(d) Generated by ùëßùëùfrom B and ùëßùëüfrom A
Figure 5: Style transfer examples by hierarchical-EC2-
VAE model.
4.4.2 Latent zr interpolation
We interpolate rhythm representations zr of two phrases
using SLERP [31] while keeping the pitch and chord un-
changed. The interpolated latent representations can then
be ‚Äúre-synthesized‚Äù using Hierarchical-EC2-VAE.
As shown in Figure 6, we interpolate zr of the piece A
and B with different SLERP weights. The results exhibit a
surprising sense of coherence of pitch and rhythm in gener-
ative melodies, even in the transition between consecutive
bars.This implies that a longer-term representation is also
adept at modeling short-term generation and even contains
more global harmonic information than a short-term repre-
sentation.
4.4.3 Theme variation
We can also achieve theme variation by adding a Gaus-
sian noise to zr while keeping zp unchanged . As a sam-
ple shown in Figure 7, we Ô¨Ånd that as the variance of the
noise grows larger, the pitch and rhythm of the generative
melody are still reasonable smooth, implying that the long-
term representations contain the coherence of contextual
information and can ‚Äúcontrol‚Äù the generation process.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
742

(a) Source Melody A
(b) Target rhythm B
(c) Interpolated on  ùëßr from A to B with more weight on A
(d) Interpolated on  ùëßr from A to B with more weight on B
Figure 6: Interpolation examples.
(a) Source Melody A
(b) Rhythm representation Posterior sampling with ùúé2 = 0.8
(c) Rhythm representation Posterior sampling with ùúé2 = 1.3
Figure 7: Rhythm representation posterior sampling ex-
amples.
4.5 Subjective Evaluation
One may wonder what are the advantages of learning long-
term representations since we can always generate the mu-
sic bar by bar using short-term models and just concate-
nate the generated samples together. One merit lies in the
coherency in controlled music generation. For example,
when sampling the long-term rhythm representation, the
overall rhythm pattern of a phrase is controlled as an or-
ganic whole, while individually sampling the rhythm of
different bar may easily lose the rhythm coherency. To bet-
ter illustrate this idea,we conduct a survey on theme vari-
ation (as in Section 4.4.3) to compare the performance of
the proposed 8-bar Hierarchical-EC2-VAE and baseline 2-
bar EC2-VAE.
4.5.1 Survey ConÔ¨Åguration
In our survey,
each subject is given 5 groups of
pieces.
Each group contains three 8 bar pieces:
a
human-composed piece from Nottingham dataset and 2
theme variations generated by a 2-bar EC2-VAE and
Hierarchical-EC2-VAE, respectively. In each group, the
generated pieces use zp of the human-composed piece and
the sampled zr.
Each subject listens to Ô¨Åve randomly arranged groups
in turn and is required to rate each melody ranging from
1 (very low) to 5 (very high) according to three aspects:
creativity, naturalness (how human-like the composition
is) and overall musicality.
4.5.2 Results
A total of 29 subjects ( 18 females and 11 males) partici-
pated in the survey. Experimental results depicted in Fig-
ure 8 demonstrate that people prefer melodies generated
by the proposed Hierarchical-EC2-VAE to those generated
by the 2 bar EC2-VAE [7], implying the effects of a long-
term coherence learned by our model. The heights of bars
represent means of the ratings and the error bars represent
the MSEs computed via within-subject ANOVA [32]. The
results show that our model performs signiÔ¨Åcantly better
than the 2 bar EC2-VAE in terms of all three dimensions(p
< 0.05). Besides, the qualities of melodies generated by the
proposed Hierarchical-EC2-VAE reach a competitive stan-
dard compared to the human-composed pieces, especially
in creativity.
Figure 8: The results of the subjective evaluation.
5. CONCLUSION
In conclusion, we contribute a pipeline of algorithms to
learn long-term and disentangled music representations.
The main novelty lies in the proposed two inductive biases
which constrain the long-term representations using con-
textual information. The Ô¨Årst one requires long-term rep-
resentation to be not too different from the short-term ones
which represent a part of the long-term sequence, and we
demonstrate contrastive loss is well-suited for such rough
constraint. The second inductive bias is that a good long-
term representation should be able to reconstruct the cor-
responding short-term ones, and we use a hierarchical pre-
dictive model to realize this constraint. Unlike most hier-
archical models, our purpose is not prediction for its own
sake, but rather to leverage the prediction power to learn a
well-disentangled long-term representation. Experimental
results show that our approach is quite successful, capa-
ble of disentangling pitch and rhythmic factors for phrase-
level (32 beats) melody without increasing the dimension-
ality of latent representation compared to bar-level models.
Moreover, the learned representations enable high-quality
phrase-level style transfer via representation swapping and
theme variation by representation interpolation and poste-
rior sampling.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
743

6. REFERENCES
[1] A. Roberts, J. H. Engel, C. Raffel, C. Hawthorne, and
D. Eck, ‚ÄúA hierarchical latent vector model for learn-
ing long-term structure in music,‚Äù in Proceedings of
the 35th International Conference on Machine Learn-
ing, Stockholm, Sweden, 2018, pp. 4361‚Äì4370.
[2] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term
memory,‚Äù Neural Comput., vol. 9, no. 8, pp. 1735‚Äì
1780, 1997.
[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,
‚ÄúAttention is all you need,‚Äù in Advances in Neural
Information Processing Systems 30: Annual Confer-
ence on Neural Information Processing Systems, Long
Beach, CA, USA, 2017, pp. 5998‚Äì6008.
[4] K. Chen, G. Xia, and S. Dubnov, ‚ÄúContinuous melody
generation via disentangled short-term representations
and structural conditions,‚Äù in 14th International Con-
ference on Semantic Computing, San Diego, CA, USA,
2020, pp. 128‚Äì135.
[5] Z. Wang, D. Wang, Y. Zhang, and G. Xia, ‚ÄúLearning in-
terpretable representation for controllable polyphonic
music generation,‚Äù in Proceedings of the 21th Interna-
tional Society for Music Information Retrieval Confer-
ence, Montreal, Canada, 2020, pp. 662‚Äì669.
[6] A. Pati, A. Lerch, and G. Hadjeres, ‚ÄúLearning to
traverse latent spaces for musical score inpainting,‚Äù
in Proceedings of the 20th International Society for
Music Information Retrieval Conference, Delft, The
Netherlands, 2019, pp. 343‚Äì351.
[7] R. Yang, D. Wang, Z. Wang, T. Chen, J. Jiang, and
G. Xia, ‚ÄúDeep music analogy via latent representation
disentanglement,‚Äù in Proceedings of the 20th Interna-
tional Society for Music Information Retrieval Confer-
ence, Delft, The Netherlands, 2019, pp. 596‚Äì603.
[8] Y. Huang and Y. Yang, ‚ÄúPop music transformer: Beat-
based modeling and generation of expressive pop pi-
ano compositions,‚Äù in MM ‚Äô20: The 28th ACM Interna-
tional Conference on Multimedia, Virtual Event / Seat-
tle, WA, USA, 2020, pp. 1180‚Äì1188.
[9] G. Brunner, A. Konrad, Y. Wang, and R. Wattenhofer,
‚ÄúMIDI-VAE: modeling dynamics and instrumentation
of music with applications to style transfer,‚Äù in Pro-
ceedings of the 19th International Society for Music In-
formation Retrieval Conference, Paris, France, 2018,
pp. 747‚Äì754.
[10] T. Akama, ‚ÄúControlling symbolic music generation
based on concept learning from domain knowledge,‚Äù
in Proceedings of the 20th International Society for
Music Information Retrieval Conference, Delft, The
Netherlands, 2019, pp. 816‚Äì823.
[11] Y. Luo, K. Agres, and D. Herremans, ‚ÄúLearning dis-
entangled representations of timbre and pitch for mu-
sical instrument sounds using gaussian mixture varia-
tional autoencoders,‚Äù in Proceedings of the 20th Inter-
national Society for Music Information Retrieval Con-
ference, Delft, The Netherlands, 2019, pp. 746‚Äì753.
[12] J. Jiang, G. Xia, D. B. Carlton, C. N. Anderson, and
R. H. Miyakawa, ‚ÄúTransformer VAE: A hierarchical
model for structure-aware and interpretable music rep-
resentation learning,‚Äù in International Conference on
Acoustics, Speech and Signal Processing, Barcelona,
Spain, 2020, pp. 516‚Äì520.
[13] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton,
‚ÄúA simple framework for contrastive learning of vi-
sual representations,‚Äù in Proceedings of the 37th Inter-
national Conference on Machine Learning, 2020, pp.
1597‚Äì1607.
[14] K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick,
‚ÄúMomentum contrast for unsupervised visual repre-
sentation learning,‚Äù in Conference on Computer Vision
and Pattern Recognition, Seattle, WA, USA, 2020, pp.
9726‚Äì9735.
[15] X. Chen, H. Fan, R. B. Girshick, and K. He, ‚ÄúIm-
proved baselines with momentum contrastive learn-
ing,‚Äù CoRR, vol. abs/2003.04297, 2020.
[16] M. Gutmann and A. Hyv√§rinen, ‚ÄúNoise-contrastive es-
timation: A new estimation principle for unnormalized
statistical models,‚Äù in Proceedings of the Thirteenth In-
ternational Conference on ArtiÔ¨Åcial Intelligence and
Statistics, ser. JMLR Proceedings, vol. 9, 2010, pp.
297‚Äì304.
[17] A. Mnih and K. Kavukcuoglu, ‚ÄúLearning word em-
beddings efÔ¨Åciently with noise-contrastive estimation,‚Äù
in Advances in Neural Information Processing Sys-
tems 26: 27th Annual Conference on Neural Informa-
tion Processing Systems, Lake Tahoe, Nevada, United
States, 2013, pp. 2265‚Äì2273.
[18] A. van den Oord, Y. Li, and O. Vinyals, ‚ÄúRepre-
sentation learning with contrastive predictive coding,‚Äù
CoRR, vol. abs/1807.03748, 2018.
[19] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon,
K. Grewal, P. Bachman, A. Trischler, and Y. Ben-
gio, ‚ÄúLearning deep representations by mutual infor-
mation estimation and maximization,‚Äù in 7th Interna-
tional Conference on Learning Representations,New
Orleans, LA, 2019.
[20] F. Lerdahl and R. Jackendoff, ‚ÄúA generative theory of
tonal music,‚Äù Journal of Aesthetics and Art Criticism,
vol. 9, no. 1, pp. 72‚Äì73, 1996.
[21] W. Rothstein, O. Jonas, and J. Rothgeb, ‚ÄúIntroduction
to the theory of heinrich schenker: The nature of the
musical work of art,‚Äù Journal of Music Theory, vol. 27,
no. 2, 1983.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
744

[22] M. Hamanaka, K. Hirata, and S. Tojo, ‚ÄúImplementing
"a generative theory of tonal music",‚Äù Journal of New
Music Research, vol. 35, no. 4, pp. pp. 249‚Äì277, 2006.
[23] A. Marsden, ‚ÄúSchenkerian analysis by computer: A
proof of concept,‚Äù Journal of New Music Research,
vol. 39, no. 3, pp. 269‚Äì289, 2010.
[24] H. H. Tan and D. Herremans, ‚ÄúMusic fadernets: Con-
trollable music generation based on high-level features
via low-level feature modelling,‚Äù in Proceedings of the
21th International Society for Music Information Re-
trieval Conference, Montreal, Canada, 2020, pp. 109‚Äì
116.
[25] G. Hadjeres and L. Crestel, ‚ÄúVector quantized con-
trastive predictive coding for template-based music
generation,‚Äù CoRR, vol. abs/2004.10120, 2020.
[26] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,
and I. Sutskever, ‚ÄúJukebox: A generative model for
music,‚Äù CoRR, vol. abs/2005.00341, 2020.
[27] E. Foxley, ‚ÄúNottingham database,‚Äù 2011.
[28] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai,
and G. Xia, ‚ÄúPOP909: A pop-song dataset for music
arrangement generation,‚Äù in Proceedings of the 21th
International Society for Music Information Retrieval
Conference, Montreal, Canada, 2020, pp. 38‚Äì45.
[29] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochas-
tic optimization,‚Äù in 3rd International Conference on
Learning Representations, Y. Bengio and Y. LeCun,
Eds., 2015.
[30] H. Kim and A. Mnih, ‚ÄúDisentangling by factorising,‚Äù
in Proceedings of the 35th International Conference on
Machine Learning, 2018, pp. 2654‚Äì2663.
[31] S. Hollasch, ‚ÄúAdvanced animation and rendering tech-
niques: By alan watt and mark watt, acm press,‚Äù Com-
puters & Graphics, vol. 18, no. 2, p. 249, 1994.
[32] H. Scheff√©, ‚ÄúThe analysis of variance,‚Äù in Architectural
Institute of Japan, 1999.
Proceedings of the 22nd ISMIR Conference, Online, November 7-12, 2021
745
