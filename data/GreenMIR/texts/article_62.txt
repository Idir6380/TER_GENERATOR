StructureNet: INDUCING STRUCTURE IN GENERATED MELODIES
Gabriele Medeot1
Srikanth Cherla1
Katerina Kosta1
Matt McVicar1
Samer Abdallah1
Marco Selvi1
Ed Newton-Rex1
Kevin Webster2
1Jukedeck Ltd., London, United Kingdom
2Imperial College London, London, United Kingdom
{gabriele, srikanth, katerina, matt, samer, marco, ed}@jukedeck.com
kevin.webster@imperial.ac.uk
ABSTRACT
We present the StructureNet - a recurrent neural network
for inducing structure in machine-generated compositions.
This model resides in a musical structure space and works
in tandem with a probabilistic music generation model as
a modifying agent. It favourably biases the probabilities of
those notes that result in the occurrence of structural ele-
ments it has learnt from a dataset. It is extremely ﬂexible
in that it is able to work with any such probabilistic model,
it works well when training data is limited, and the types
of structure it can be made to induce are highly customis-
able. We demonstrate through our experiments on a sub-
set of the Nottingham dataset that melodies generated by a
recurrent neural network based melody model are indeed
more structured in the presence of the StructureNet.
1. INTRODUCTION
Automated generation of symbolic music using comput-
ers involves the application of computer algorithms to the
creation of novel musical scores. The natural predisposi-
tion of computers to quickly enumerate and choose from
a large set of compositional alternatives makes them suit-
able candidates for discovering novelty in the vast space
of musical possibilities that could be daunting to a human
composer. Leveraging computing power for this purpose
has the potential to aid and accelerate the creative process,
thus lowering the bar for composition and democratising
it. So-called machine-generated music has been a subject
of steady interest since the pioneering work of a few mu-
sically inclined information theorists [5, 8]. This interest
has surged during the past decade or so within academia
and especially outside it with the rise of certain industry
players (such as Jukedeck 1 and the Magenta project 2 ).
1 https://www.jukedeck.com/
2 https://magenta.tensorflow.org/
c⃝Gabriele Medeot, Srikanth Cherla, Katerina Kosta, Matt
McVicar, Samer Abdallah, Marco Selvi, Ed Newton-Rex, Kevin Webster.
Licensed under a Creative Commons Attribution 4.0 International Li-
cense (CC BY 4.0). Attribution: Gabriele Medeot, Srikanth Cherla, Ka-
terina Kosta, Matt McVicar, Samer Abdallah, Marco Selvi, Ed Newton-
Rex, Kevin Webster. “StructureNet: Inducing Structure in Generated
Melodies”, 19th International Society for Music Information Retrieval
Conference, Paris, France, 2018.
The roughly seven decade-long history of machine-
generated symbolic music has seen the application of a
plethora of algorithms to varying degrees of success [8].
With the increasing digitisation of musical scores, those
relying on machine learning have gained importance in re-
cent times. The relatively successful approaches among
these have been Probabilistic Grammars [9], (Hidden)
Markov models [19, 21], and Connectionist architectures
[2,18]. The latter in particular have proven to be highly ef-
fective at representing musical information and modelling
long-term dependencies which are crucial to generating
good-quality music [3].
This paper addresses the issue of long-term structure in
machine-generated symbolic monophonic music. Struc-
ture is a key aspect of music composed by humans that
plays a crucial role in giving a piece of music a sense of
overall coherence and intentionality. It appears in a piece
as a collection of musical patterns, variations of these pat-
terns, literal or motivic repeats and transformations of sec-
tions of music that have occurred earlier in the same piece.
Hampshire underlines that a piece can be conceived as a
work of art if and only if the listener’s mind is actively
tracing the structure of the work using her own natural im-
agery and musical memory [7, p. 16].
Here we introduce StructureNet - a recurrent neu-
ral network that induces structure in machine-generated
melodies. It learns about structure from a dataset consist-
ing of structural elements and their occurrence statistics,
which is created using a structure-tagging algorithm from
an existing dataset of melodies. Once trained, StructureNet
works in tandem with a melody model which generates a
probability distribution over a set of musical notes. Given
the melody model’s prediction at any given time during
generation, StructureNet uses the structural elements im-
plied by the melody so far to alter the prediction, leading
to a more structured melody in the future. Our experiments
reveal that music generated with StructureNet contains sig-
niﬁcantly better structure, even when it is trained on a rel-
atively small dataset. We provide musical examples that
highlight this fact.
The next section introduces relevant state-of-the-art.
Some preliminaries and a description of StructureNet fol-
low in Sections 3 and 4 respectively. Based on the results
presented in Section 5, we summarise our ﬁndings and sug-
gest potential future work in Section 6.
725

2. RELATED WORK
In order to repeat verbatim or with variations sections that
have occurred previously in a piece of machine-generated
music, i.e. to induce structure in it, the model must be able
to encode and recall in some way what has happened in the
past. This can be achieved in a variety of ways. In a ﬁrst
instance, improving structure simply involves making the
generation model more powerful. An example of this is the
RNN-RBM [2] that was enhanced purely by replacing its
components - the Recurrent Neural Network (RNN) by a
Long-Short Term Memory (LSTM) Network to improve its
temporal memory, making it the LSTM-RTRBM [16], and
the Restricted Boltzmann Machine (RBM) by a Deep Be-
lief Network (DBN) to improve its output layer, making it
the RNN-DBN [10]. Similarly, it was demonstrated in [4]
that connectionist models outperform Markov models in
modelling melodic sequences. Closely related to these is a
musically informed improvement that enriches the feature
encoding to include those features that have the potential to
add more information about structure [6,19]. Along simi-
lar lines, the Magenta Project proposed two neural network
architectures to model higher-level structure in music - the
Lookback RNN and Attention RNN [25]. While the for-
mer augments the model’s feature vector with information
about notes from previous measures, repeat information
and metrical location, the latter adopts an attention-based
mechanism [1] wherein a weighted sum of the model’s out-
puts in the previous n locations is used in addition to its
current state to make better predictions. Such approaches
address the overall quality of music, of which high-level
structure is just one aspect. Moreover, the improvements
afforded by the former kind are highly dependent on the
training loss, which does not explicitly take into account
structure of the kind observed in music. So while an im-
provement in the model or feature representation does tend
to improve the overall quality of music in a piece, improve-
ment is often observed over short time-spans and not nec-
essarily in the higher-level structure.
Alternatively, one can explicitly addresses the issue of
high-level structure in machine-generated compositions.
One simple solution involves dividing the generation task
between multiple models. The MELONET system [13],
whose goal is to produce variations of a given melodic
theme, achieves structural coherence by dividing the ef-
fort between two mutually interacting neural networks op-
erating at different time-scales. The ﬁrst network learns
to recognise musical structure while the second network
predicts the musical notes. Similarly, Todd [24] proposed
two cascaded networks that allow the explicit representa-
tion of structure in a hierarchy. The ﬁrst network gener-
ates a sequence of plans which correspond to descriptions
of melodic chunks, and the second a sequence of notes
given a plan. More recently, Roig et al. [22] devised a
system in which melodic and rhythmic patterns existing
in the dataset are concatenated according to statistically
governed rules to form new patterns that are not too dis-
tant from those occurring in the dataset. In the system
known as MorpheuS [11] music generation is formulated
as a combinatorial optimisation problem in which a tem-
plate of musical structure acts as a hard-constraint, and
solved using a meta-heuristic search algorithm known as
Variable Neighbourhood Search. Patterns contained in the
dataset of pieces are discovered using an existing pattern-
detection algorithm [17]. In a similar vein, [20] control the
generation of chord sequences and melodies using steer-
able constraints Markov chains. Lattner et al. [14] adopt a
similar approach where a Convolutional Restricted Boltz-
mann Machine is combined with a constraint optimisation
technique to constrain the music sampled from the C-RBM
according to the musical structure of a given template.
3. BACKGROUND
StructureNet is a Recurrent Neural Network (RNN) that
operates in the space of musical structure and learns se-
quences of features that denote the presence or absence
of repeats at a point in time and their type, if present.
Here we give an overview of the Long Short-Term Mem-
ory (LSTM) RNN that underlies StructureNet and the def-
inition of structural repeats that we rely on.
3.1 Long Short-Term Memory
The RNN is a type of neural network for modelling se-
quences and its basic architecture consists of an input layer,
a hidden layer and an output layer. The state of its hidden
layer acts as a memory of the past information it encoun-
ters while traversing a sequence. At each location in the se-
quence, the RNN makes use of both the input and the state
of its hidden layer from the previous location to predict an
output. Here we use a special case of the RNN known as
the Long-Short Term Memory (LSTM) network [12] that,
owing to the presence of purpose-built memory cells to
augment its hidden layer, boasts a greater temporal mem-
ory than the standard RNN. Given an input vector xt at
sequence location t, the output of the LSTM ht−1 and its
memory cell ct−1 (collectively, its state) from the previous
location, the output of the LSTM layer ht is computed and
further propagated into another layer of a larger model.
3.2 Modelling Melodies and Structure Elements
The output layer of the note-based (as opposed to frame-
based) melody model in the present work contains two
groups of softmax units. Each group of softmax units mod-
els a single probability distribution over a set of mutually
exclusive possibilities. The ﬁrst of these denotes the musi-
cal pitch of the note, and the second its duration. Given the
output of the LSTM layer ht at any given location t in the
sequence, this is transformed into two independent proba-
bility distributions pt and dt that together make up the out-
put layer of the network. From these two distributions, the
probability of a certain note (with pitch and duration) can
be obtained simply by multiplying the probabilities of its
corresponding pitch and duration respectively. Note that
the output layer of StructureNet contains three groups of
softmax units. Although these represent different quanti-
ties that deﬁne aspects of structure (explained in detail in
726
Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018

Figure 1. A 16-measure melody generated by our LSTM
melody model together with the StructureNet. A selection
of repeats in this melody are as follows: measures 9-12 are
a duration-interval repeat of measures 5-8, as are measures
13-14 of measures 9-10; and measures 15 and 16 are both
duration repeats of measure 12.
Sections 4.1 and 4.2), the manner in which these are com-
bined to generate the probabilities of structural elements is
identical to the melody model. Also note that the choice of
the LSTM as the melody model is arbitrary and it can be
replaced by any other probabilistic prediction model.
3.3 A Deﬁnition of Structure
There are various types of structure present in music.
Composers use techniques such as instrumental variation,
changes and repeats in timbre, and dynamics to induce a
feeling of familiarity in the listener. In the present work,
however, we focus on the score-level repeat information.
In a score, perhaps the two most obvious types of repeat
are of (1) duration (rhythmic), and (2) pitches (melodic).
A duration repeat is a section of the melody, the du-
rations of whose notes are the same as those of a previ-
ous section. Examples of duration repeats can be found in
the melody of Figure 1. These are determined purely by
the sequences of crotchets and quavers contained in these
measures. When it comes to pitch, it is helpful to think
of these repeats in terms of intervals rather than absolute
pitch. The interval between two notes can be deﬁned in a
number of ways, but in this work we use the scale degree
distance between notes. For instance, in the key of C ma-
jor, the scale degree between a C note and subsequent E
note would be the same as the scale degree between a D
note and subsequent F note. Given this deﬁnition of an in-
terval, a duration-interval repeat is a section of the melody
that holds the same relationship to a previous section as
a duration repeat, and additionally the intervals between
whose consecutive notes are the same as those between the
consecutive notes of that previous section. Figure 1 also
illustrates duration-interval repeats. In the present work,
we consider duration repeats as well as repeats of both du-
rations and intervals. Purely interval repeats were found to
be very few in our chosen dataset and were thus ignored.
4. StructureNet
StructureNet is only able to produce structural repeat in-
formation that biases the predictions of an accompanying
music (in the present case melody) model. In Section 4.2
we will outline a methodology whereby it modiﬁes the
probability of notes that the melody model produces, thus
encouraging structure but not enforcing it. Crucially, this
means that the structure network is able to suggest repeats
of certain types, but if the melody network assigns very
low probability to notes that would form these repeats, it
is free to “override” the structure network’s suggestions in
a probabilistic and ﬂexible manner. The speciﬁcs of how
StructureNet achieves these goals is outlined in the remain-
der of this section, beginning with the type of structure we
capture and how we identify it.
4.1 A Dataset of Structure
StructureNet operates in a space of musical structure. In
order to train the model, we ﬁrst create this structure
dataset by processing a dataset of melodies with a musical
repeat-detection algorithm. The algorithm encodes each
melody into a sequence of binary feature vectors in the
semi-quaver temporal resolution (although this resolution
is not a strict requirement: if the dataset contains no notes
shorter than a quaver, one may use a quaver as the minimal
resolution). The feature vector itself is a concatenation of
three one-hot sub-vectors. The ﬁrst is given by
[f, d, ditr, dint]
wherein each bit of the ﬁrst sub-vector indicates which of
four categories a given frame of music belongs to. These
are (1) f - free music, (2) d - duration repeat, (3) ditr -
duration-interval repeat with transposition and (4) dint -
duration-interval repeat without transposition. The only
distinction between the two types of duration-interval re-
peats is that in the case of ditr the section to which the
frame belongs is a transposed version of the original sec-
tion whereas in the case of dint the section to which the
frame belongs is at the same musical pitch as the original
section. The free music bit f indicates that the frame is a
part of a section that is not a repeat of any previous section
of the melody. The second one-hot sub-vector is given by
[f, l0.5, l0.75, l1.0, l1.5, l2.0, l3.0, l4.0, l8.0, l16.0]
and contains bits that indicate the lookback, i.e. the dis-
tance (in crotchets) between the original section and the
section containing the current frame, if the section con-
taining the current frame is a repeat of the original sec-
tion.
If it is not a repeat, the free music bit f is on.
Note that the value of the free music bits in both sub-
vectors is identical and hence uses the same notation. Also
note that the choice of the set of lookbacks is completely
open to change, and highlights another ﬂexible aspect of
the model; it may even be possible to learn the optimal
set of lookbacks for a given dataset. Finally, the third 8-
dimensional one-hot vector φ encodes the location of a
frame via its beat strength β and its measure strength ρ.
The beat strength [15] encodes the strength of each met-
rical location in a measure. In a measure divided into 16
semi-quaver beats (as in the present work), its values are
β = [0, 4, 3, 4, 2, 4, 3, 4, 1, 4, 3, 4, 2, 4, 3, 4]. The measure
Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018
727

strength extends the notion of beat strength to a sequence
of measures. The strengths associated with beats in a mea-
sure are associated with measures in a piece, beginning
with the ﬁrst measure. In the present work, we choose a
cycle of 8 measures that correspond to the following se-
quence of measure strengths ρ = [0, 3, 2, 3, 1, 3, 2, 3]. In
both cases, a lower value indicates a higher strength. Our
encoding is deﬁned as
φt =
(
ρ(mod(t, 8))
if mod(t, 16) = 0
max(ρ) + β(mod(t, 16))
otherwise
(1)
Note that just as the beat strength, the measure cycle dura-
tion for the measure length can also be varied as desired.
StructureNet models the vector that is a concatenation
of these three sub-vectors as three groups of softmax units
in its output layer. As noted earlier in Section 3.2, the
manner in which one combines the probability distribu-
tions represented by these two groups of softmax units (for
instance, a duration-interval repeat of lookback 8.0, or an
interval repeat of lookback 1.5) is by multiplying the cor-
responding probabilities one from each group.
The repeat-detection algorithm works by ﬁrst convert-
ing a sequence of notes into two strings - one correspond-
ing to durations and the other to intervals. In each of these
strings, it then uses a string matching algorithm to ﬁnd
substrings that repeat. Single-note repeats are trivial and
thus discarded, and only those repeats that correspond to
the above listed lookbacks are retained. Any note that is
longer than 2 measures is split into multiple notes of the
same pitch to limit the number of characters required to
represent the piece as a string. Then the list of duration re-
peats are ﬁltered such that only the longest repeats remain
and all overlapping and shorter repeats are discarded. At
this stage, the duration-interval repeats are nothing but du-
ration repeats with coinciding interval repeats. So from the
list of interval repeats only those are retained that coincide
exactly with the current list of duration repeats with the
same lookbacks. These are tagged as duration-interval re-
peats, replacing the corresponding duration repeats to give
the ﬁnal list of duration repeats and duration-interval re-
peats. While it is indeed possible to look for other types
of repeats, we limit ourselves in this paper to the above as
it is sufﬁcient to demonstrate the efﬁcacy of StructureNet.
This also highlights the ﬂexibility of the model wherein
one may change the type of repeats detected and also cus-
tomise the number of lookbacks as needed.
4.2 Inﬂuencing Event Probabilities
Once trained on the above described structure dataset,
StructureNet is then put to use with the probabilistic
melody prediction model Mm. At time t (that is, given
the history of notes generated up to time t), the model Mm
predicts a probability distribution Pt over a set of notes N.
At the same time, given the history of repeats generated so
far, the structure model Ms predicts a probability distribu-
tion Qt over a set of possible repeats Π, which includes an
element πf, representing ‘free music’. Each note ν ∈N
can be consistent with a subset Πν
t of these repeats, which
will always include πf, meaning that every note is consis-
tent with ‘free music’.
StructureNet inﬂuences the prediction Pt by modifying
the probability of each note according to the probabilities
of the repeats with which it is consistent. Let φt : N×Π →
{0, 1} be a function such that φt(ν, π) = 1 when note ν is
consistent with repeat π at time t and 0 otherwise. In terms
of this we can express Πν
t as {π ∈Π|φt(ν, π) = 1}, and
further deﬁne N π
t = {ν ∈N|φt(ν, π) = 1}, which is the
set of notes consistent with π. Each note ν is then assigned
a weight
Wt(ν) = Pt(ν)
X
π∈Πν
t
Qt(π)
µπ
t
,
(2)
where µπ
t = P
ν∈Nπ
t Pt(ν). In this way, the relative prob-
ability of a note ν is increased when it is consistent with
repeat(s) to which Ms has assigned high probability.
It is important to note that Mm and Ms operate at
different temporal resolutions—note-level and semiquaver
frame-level respectively—and that this difference becomes
signiﬁcant here. Suppose note ν is of duration ∆ν = τνδ,
where δ is the frame duration and τν is the number of
frames occupied by ν. Ideally, in order to get an accurate
estimate of the joint probability of the note ν and the repeat
π, one should consider the probability that Ms assigns to
τν consecutive frames of π. This would be expressed as
Wt(ν) = Pt(ν)
X
π∈Πν
t
τ−1
Y
k=0
Qt+k(π)
µπ
t+k
.
(3)
However, we found in our experiments that the single-step
approximation (2) works well in practice and is less com-
putationally intensive than (3).
Next, the weight distribution Wt is normalised to obtain
a probability distribution Rt:
Rt(ν) =
Wt(ν)
P
ν∈N Wt(ν).
(4)
We may now sample a note νt from this distribution and
update the internal state of the melodic model Mm with
this observation.
It remains to update the state of the structure model Ms
with some observed repeat. The note νt sampled at time t
could be associated with any of the repeats that were con-
sistent with it. We choose one by sampling πt from a dis-
tribution St over Πνt
t deﬁned as
St(π) =
Qt(π)
P
π′∈Πνt
t Qt(π′).
(5)
At this point the two models are misaligned due to the dif-
ferent time-scales they operate in, with Mm being τ semi-
quaver frames ahead of Ms. Since each update of the state
of Ms takes it ahead by just one semi-quaver frame, it is
necessary to update Ms τ times repeatedly with the same
structure vector so that it is once again aligned with Mm.
At the end of the process described above, we have a
melody note sampled from our melody model that has been
728
Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018

inﬂuenced by StructureNet. StructureNet has also updated
its own state according to the sampled note and is ready to
inﬂuence the choice of next note.
5. EXPERIMENTS
We demonstrate the efﬁcacy of StructureNet on a well-
known dataset of melodies by comparing statistics over
several musical quantities computed both on the dataset
and compositions generated by the melody model alone
and the melody and structure models combined. The re-
sults show that the presence of StructureNet leads to music
that is more structured and closer in the statistics to the
dataset. We also share the generated music to allow the
reader to her or himself be the judge of our claims.
5.1 Dataset
We evaluate StructureNet on the cleaned Nottingham folk
melody dataset that was released by the Jukedeck Research
Team [23]. This publicly available dataset facilitates repro-
ducibility. We carry out our experiments on the subset of
450 4/4 time-signature pieces out of the 1, 548 contained
in it. Each piece of the dataset was truncated to its ﬁrst
16 measures and transposed into the Key of C, and all up-
beats at the beginning of each piece were removed prior to
training. We used 20% (90 segments) of the data as the val-
idation and the rest (360 segments) for training the models.
StructureNet is also trained on the same dataset following
the application of the repeat-tagging algorithm. However,
one must note that this is not a requirement and a different
dataset may be used for learning structure and could poten-
tially lead to interesting results. StructureNet successfully
induces structure in the generated melodies despite the few
examples contained in the training data.
5.2 Training methodology
As mentioned earlier, both the structure network and the
melody network are LSTMs and contain a single hidden
layer. A Bayesian Optimisation based method was em-
ployed to carry out model selection. In the case of the
melody model, the single best outcome of the grid search
was used. As for StructureNet, ten models with the same
best set of hyperparameters as determined by the model
selection step, and different initial conditions, were trained
and used in tandem with the melody model. This was done
in order to be able to compute conﬁdence intervals in the
ﬁgures. The hidden layer size of each network was varied
between 50 and 1000 in steps of 50 during model selection,
which led to ns
hid = 950 in the former and nm
hid = 250 in
the latter. Early stopping was used as a regulariser, such
that the training was stopped and the best models thus far
retrieved after no improvement in the validation cost for
25 epochs. The models were trained using the ADAM op-
timiser with an initial learning rate ηinit = 0.001, and pa-
rameters β1 = 0.9, β2 = 0.999 and ϵ = 10−8.
5.3 Evaluation
Our hypothesis is two-fold: (1) that repeat-related statis-
tics computed over the melodies generated with Struc-
tureNet are closer to those over the dataset melodies than
those over melodies generated without StructureNet, and
(2) that non repeat-related statistics do not differ between
the melodies generated by the melody model with and
without StructureNet.
This would demonstrate that the
use of StructureNet leads to more structured melodies than
are generated by the melody model on its own, and that
are musically at least as similar to the original data as the
melody model alone achieves. The statistics are:
1. Repeat Count: Number of repeats corresponding to
various lookback values (in crotchets).
2. Repeat Duration: Number of repeats of various du-
rations (in crotchets).
3. Repeat Onsets: Number of repeats beginning at
various locations (in crotchets) in a piece.
4. Pitch, start time and duration distributions: Oc-
currence statistics of pitches, start times in measure,
and durations.
The ﬁrst three are repeat-related statistics and the rest
are not. A histogram of each is ﬁrst computed per col-
lection of melodies (dataset, generations with and with-
out StructureNet), and then normalised by the count of
melodies in the collection to generate a probability distri-
bution (as the counts vary between the different collections
of melodies). The KL-Divergences (KLD) κdata,SN and
κdata,NoSN between the distribution pairs (dataset, Struc-
tureNet) and (dataset, No StructureNet) respectively high-
light the effect of introducing StructureNet (Table 1). Ide-
ally, among the structure-related distributions, we would
want κdata,SN < κdata,NoSN. And among the non-repeat-
related distributions, we wish for κdata,SN ≤κdata,NoSN.
κdata,NoSN
κdata,SN
Repeat Count (D)
0.0356 ± 0.0022
0.0069 ± 0.0043
Repeat Duration (D)
0.1071 ± 0.0047
0.0389 ± 0.0168
Repeat Onset (D)
0.0844 ± 0.0038
0.0357 ± 0.0094
Repeat Count (DI)
0.0511 ± 0.0049
0.0173 ± 0.0095
Repeat Duration (DI)
0.2402 ± 0.0069
0.0634 ± 0.0352
Repeat Onset (DI)
0.1209 ± 0.0073
0.0639 ± 0.0194
Repeat Count (all)
0.0483 ± 0.0035
0.0083 ± 0.0045
Repeat Duration (all)
0.0996 ± 0.0033
0.025 ± 0.0081
Repeat Onset (all)
0.0875 ± 0.0036
0.031 ± 0.0103
Pitch
0.0079 ± 0.0011
0.0061 ± 0.0012
Duration
0.0049 ± 0.0016
0.0042 ± 0.0014
Onset
0.058 ± 0.0081
0.0275 ± 0.0082
Table 1. KL-divergences between the training data and
melodies generated with and without StructureNet (com-
puted over 10 sets of 450 melodies generated with each
trained StructureNet) for the Duration (D), Duration-
Interval (DI) and all repeat types.
Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018
729

5.4 Observations
In Table 1, the KLD values show a greater match between
the dataset and the set of generated melodies in the pres-
ence of StructureNet than in its absence. This holds true
for both duration and duration-interval repeats. Figure 2
illustrates such similarities (over all repeat types) visually.
One will see here that overall StructureNet (a) is conducive
to the creation of longer repeats while generally having a
positive effect on shorter ones as well, (b) is conducive to
the creation of repeats that have lookback values similar to
those in the dataset, particularly larger lookbacks (encour-
aging distant repeats), and (c) encourages repeats to begin
on those metrical locations in a generated piece where they
tend to occur in the dataset.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Duration (in crotchets)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Count (normalised)
(a) Duration of all patterns
No StructureNet
With StructureNet
Dataset
0.5
0.75
1.0
1.5
2.0
3.0
4.0
8.0
16.0
Pattern lookback
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Count (normalised)
(b) Lookbacks for all patterns
No StructureNet
With StructureNet
Dataset
1
5
9
13
17
21
25
29
33
37
41
45
49
53
57
61
Start onset of pattern in piece (in crochets)
No StructureNet
Dataset
With StructureNet
0.0
0.2
0.4
0.6
0.8
1.0
(c) Repeat starts for all patterns
Figure 2. Repeat-related statistics of the dataset to the two
generation modes (with/without StructureNet).
It is also evident from the set of three non-repeat-related
statistics of Figure 3 that the presence of StructureNet has,
more often than not, led to a better match of the gener-
ated melody statistics to the dataset.
This is also sup-
ported by the very similar KLD values (often lower in the
κdata,NoSN column) for these three musical quantities at
the bottom of Table 1. And ﬁnally, each plot in Figure 4
shows the percentage of generated melodies with various
degrees of free music in them. The three plots together re-
veal that using StructureNet reduces the proportion of free
music (and thus increases the proportion of repeats) in the
generated melodies in a way that more closely matches the
proportions of free music and repeats in the dataset. Note
that the statistics in Figures 2, 3 and 4 have been computed
over the same number of melodies (of the same duration
in measures). We have made a representative subset of
melodies generated with and without StructureNet in the
MIDI format available for scrutiny 3 .
3 https://goo.gl/hL9RhZ
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
rest
MIDI note
0.00
0.03
0.05
0.08
0.10
0.12
0.15
0.18
Count (normalised)
Note labels
No StructureNet
With StructureNet
Dataset
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
20
Duration (in semiquavers)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Count (normalised)
Note durations
No StructureNet
With StructureNet
Dataset
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Start time in the measure (in semiquavers)
0.00
0.03
0.05
0.08
0.10
0.12
0.15
0.18
0.20
Count (normalised)
Start time of a note relevant to that of the measure
No StructureNet
With StructureNet
Dataset
Figure 3. Non repeat-related statistics of the dataset to the
two generation modes (with/without StructureNet).
20.0%
40.0%
60.0%
80.0%
0.0%
10.0%
20.0%
Pieces
With StructureNet - Percentage of time free music
20.0%
40.0%
60.0%
80.0%
0.0%
10.0%
20.0%
Pieces
No StructureNet - Percentage of time free music
20.0%
40.0%
60.0%
80.0%
Free Music
0.0%
10.0%
20.0%
Pieces
Dataset - Percentage of time free music
Figure 4. The amount of generated melodies with various
degrees of free music in them.
6. CONCLUSIONS & FUTURE WORK
We introduced StructureNet - an RNN that inﬂuences the
predictions of a melody model so as to give the gener-
ated melodies greater structure. We demonstrated using
statistics, as well as several musical examples, that this
model does indeed increase the probability of encounter-
ing longer and more distant (greater lookback) patterns in
music generated by a melody model. Given these initially
successful results, we foresee some interesting directions
for future work. Firstly, we are interested in experimenting
with a more evolved pattern detection algorithm such as
SIATEC and COSIATEC [17]. This will lead to new fea-
ture representations over and beyond just repeats that can
perhaps provide a better insight into musical structure to
StructureNet. We would like to expand the three musical
quantities introduced in Section 5.3 into a more compre-
hensive set of quantities that can lead to a more thorough
evaluation of musical structure.
730
Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018

7. REFERENCES
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. Neural machine translation by jointly learning to
align and translate. arXiv preprint arXiv:1409.0473,
2014.
[2] Nicolas Boulanger-Lewandowski, Yoshua Bengio, and
Pascal Vincent. Modeling temporal dependencies in
high-dimensional sequences:
application to poly-
phonic music generation and transcription. In Intl.
Conf. on Machine Learning, pages 1881–1888. Om-
nipress, 2012.
[3] Jean-Pierre Briot, Ga¨etan Hadjeres, and Franc¸ois Pa-
chet. Deep learning techniques for music generation-a
survey. arXiv preprint arXiv:1709.01620, 2017.
[4] Srikanth Cherla, Son N Tran, Artur d’Avila Garcez,
and Tillman Weyde. Discriminative learning and infer-
ence in the recurrent temporal rbm for melody mod-
elling. In Intl. Joint Conf. on Neural Networks, pages
1–8. IEEE, 2015.
[5] Joel E Cohen. Information theory and music. Systems
Research and Behavioral Science, 7(2):137–163, 1962.
[6] Darrell Conklin and Ian H Witten. Multiple viewpoint
systems for music prediction. Journal of New Music
Research, 24(1):51–73, 1995.
[7] Nicholas Cook. Music, imagination, and culture. Ox-
ford University Press, 1992.
[8] Jose David Fernndez and Francisco Vico. AI methods
in algorithmic composition: A comprehensive survey.
Journal of Artiﬁcial Intelligence Research, 48:513–
582, 2013.
[9] Jon Gillick, Kevin Tang, and Robert M Keller. Machine
learning of jazz grammars. Computer Music Journal,
34(3):56–66, 2010.
[10] Kratarth Goel, Raunaq Vohra, and JK Sahoo. Poly-
phonic music generation by modeling temporal depen-
dencies using a rnn-dbn. In Intl. Conf. on Artiﬁcial
Neural Networks, pages 217–224. Springer, 2014.
[11] Dorien Herremans and Elaine Chew. Morpheus: gen-
erating structured music with constrained patterns and
tension. IEEE Trans. on Affective Computing, 2017.
[12] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-
term memory. Neural computation, 9(8):1735–1780,
1997.
[13] Dominik H¨ornel and Wolfram Menzel. Learning musi-
cal structure and style with neural networks. Computer
Music Journal, 22(4):44–62, 1998.
[14] Stefan Lattner,
Maarten Grachten,
and Gerhard
Widmer. Imposing higher-level structure in poly-
phonic music generation using convolutional restricted
boltzmann machines and constraints. arXiv preprint
arXiv:1612.04742, 2016.
[15] Fred Lerdahl and Ray S Jackendoff. A generative the-
ory of tonal music. MIT press, 1985.
[16] Qi Lyu, Zhiyong Wu, and Jun Zhu. Polyphonic music
modelling with lstm-rtrbm. In Proceedings of the 23rd
ACM international conference on Multimedia, pages
991–994. ACM, 2015.
[17] David Meredith. Cosiatec and siateccompress: Pattern
discovery by geometric compression. In Intl. Society
for Music Information Retrieval Conf. Intl. Society for
Music Information Retrieval, 2013.
[18] Michael C Mozer. Connectionist music composition
based on melodic, stylistic and psychophysical con-
straints. Music and connectionism, pages 195–211,
1991.
[19] Francois Pachet. The continuator: Musical interaction
with style. Journal of New Music Research, 32(3):333–
341, 2003.
[20] Franc¸ois Pachet and Pierre Roy. Markov constraints:
steerable generation of markov sequences. Constraints,
16(2):148–172, 2011.
[21] Jean-Francois Paiement, Yves Grandvalet, and Samy
Bengio. Predictive models for music. Connection Sci-
ence, 21(2-3):253–272, 2009.
[22] Carles Roig, Lorenzo J Tard´on, Isabel Barbancho, and
Ana M Barbancho. Automatic melody composition
based on a probabilistic model of music style and har-
monic rules. Knowledge-Based Systems, 71:419–434,
2014.
[23] Jukedeck R&D Team. “Releasing a cleaned version of
the Nottingham Dataset.” Web blog post. Jukedeck Re-
search, 7 Mar. 2017. Web. 30 Mar. 2018.
[24] Peter M Todd. A connectionist approach to algorith-
mic composition. Computer Music Journal, 13(4):27–
43, 1989.
[25] Elliot Waite. “Generating Long-Term Structure in
Songs and Stories.” Web blog post. Magenta, 15 Jul.
2016. Web. 30 Mar. 2018.
Proceedings of the 19th ISMIR Conference, Paris, France, September 23-27, 2018
731
