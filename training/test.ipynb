{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "data= pd.read_csv(\"/Users/vanessaguerrier/Downloads/projet_TER/ner_dataset.csv\",encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47960"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_sentence= data[\"Sentence #\"].unique()\n",
    "len(nb_sentence)\n",
    "\n",
    "# id_begin_sentences = [data.index[data[\"Sentence #\"] == x][0] for x in nb_sentence if pd.notna(x)]\n",
    "# id_begin_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "X= data[\"Word\"]\n",
    "y= data[\"Tag\"]\n",
    "id2label = {i: label for i, label in enumerate(y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 12:33:38.625766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "labels = y.unique()\n",
    "num_labels = len(y.unique())\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<model> ChronoNet-XL </model> is a novel deep learning architecture designed for sequential temporal data analysis, employing a hierarchical attention mechanism that allows for improved temporal resolution without excessive computational overhead. The model contains approximately <params> 4.2 billion </params> parameters and was trained on a heterogeneous cluster consisting of <num_hardware> 32 </num_hardware> <hardware> NVIDIA A100 GPUs </hardware> over a period of <trainning_time> 38 days </trainning_time>. The training was conducted in <country> Germany </country> in <year> 2023 </year>, leveraging mixed-precision training to optimize memory usage and speed. <model> ChronoNet-XL </model> demonstrates a remarkable capacity to model complex temporal dependencies, particularly in multivariate time-series applications. Its design emphasizes both efficiency and scalability, making it adaptable to industrial and research environments where data throughput is high. The training pipeline incorporated gradient checkpointing and dynamic learning rate schedules to maintain convergence stability. The model’s performance was benchmarked across multiple datasets, demonstrating consistent improvements in prediction accuracy and robustness under noisy conditions. While certain hyperparameters were optimized through grid search, others, such as dropout ratios in intermediate layers, remain under investigation. Early experiments indicate that scaling <model> ChronoNet-XL </model> beyond its current size could yield further improvements, though the computational costs are projected to increase non-linearly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ChronoNet-XL', 'a', 'novel', 'deep', 'learning', 'architecture', 'designed', 'for', 'sequential', 'temporal', 'data', 'analysis', ',', 'employing', 'a', 'hierarchical', 'attention', 'mechanism', 'that', 'allows', 'for', 'improved', 'temporal', 'resolution', 'without', 'excessive', 'computational', 'overhead', '.'], ['The', 'model', 'contains', 'approximately', '4.2', 'billion', 'and', 'was', 'trained', 'on', 'a', 'heterogeneous', 'cluster', 'consisting', 'of', '32', 'NVIDIA', 'A100', 'GPUs', '</hardware>', 'over', 'a', 'period', 'of', '38', 'days'], ['The', 'training', 'was', 'conducted', 'in', 'Germany', '2023', 'leveraging', 'mixed-precision', 'training', 'to', 'optimize', 'memory', 'usage', 'and', 'speed', '.'], ['ChronoNet-XL', 'a', 'remarkable', 'capacity', 'to', 'model', 'complex', 'temporal', 'dependencies', ',', 'particularly', 'in', 'multivariate', 'time-series', 'applications', '.'], ['Its', 'design', 'emphasizes', 'both', 'efficiency', 'and', 'scalability', ',', 'making', 'it', 'adaptable', 'to', 'industrial', 'and', 'research', 'environments', 'where', 'data', 'throughput', 'is', 'high', '.'], ['The', 'training', 'pipeline', 'incorporated', 'gradient', 'checkpointing', 'and', 'dynamic', 'learning', 'rate', 'schedules', 'to', 'maintain', 'convergence', 'stability', '.'], ['The', 'model’s', 'performance', 'was', 'benchmarked', 'across', 'multiple', 'datasets', ',', 'demonstrating', 'consistent', 'improvements', 'in', 'prediction', 'accuracy', 'and', 'robustness', 'under', 'noisy', 'conditions', '.'], ['While', 'certain', 'hyperparameters', 'were', 'optimized', 'through', 'grid', 'search', ',', 'others', ',', 'such', 'as', 'dropout', 'ratios', 'in', 'intermediate', 'layers', ',', 'remain', 'under', 'investigation', '.'], ['Early', 'experiments', 'indicate', 'that', 'scaling', 'ChronoNet-XL', 'its', 'current', 'size', 'could', 'yield', 'further', 'improvements', ',', 'though', 'the', 'computational', 'costs', 'are', 'projected', 'to', 'increase', 'non-linearly', '.']]\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def decomposition_en_phrase(text):\n",
    "    text = re.sub(r'\\s*,\\s*', ' , ', text)\n",
    "    text = re.sub(r'\\s*<\\s*', ' <', text)\n",
    "    text = re.sub(r'\\s*>\\s*', '> ', text)\n",
    "    text= re.sub(r'\\s*([;:])\\s*', r' \\1 ', text)\n",
    "    phrases = re.split(r'(?<!\\d)[.!?]+(?!\\d)', text)\n",
    "    phrases = [p.strip()+\" .\" for p in phrases if p.strip()]\n",
    "    return phrases\n",
    "\n",
    "\n",
    "def decomposition_en_list_mot(text):  # sourcery skip: for-append-to-extend, inline-immediately-returned-variable, list-comprehension\n",
    "    phrases=decomposition_en_phrase(text)\n",
    "    list_mot=[]\n",
    "    for phrase in phrases:\n",
    "        list_mot.append(phrase.split())\n",
    "    return list_mot\n",
    "\n",
    "def extraire_nom_balise(tag):\n",
    "    return re.sub(r'[</>]', '', tag)\n",
    "\n",
    "\n",
    "def labeliser(list_phrase):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for phrase in list_phrase:\n",
    "        fe ,la = [],[]\n",
    "        i = 0\n",
    "        n = len(phrase)\n",
    "\n",
    "        while i < n:\n",
    "            token = phrase[i]\n",
    "            if token.startswith(\"<\") and not token.startswith(\"</\"):\n",
    "                nom = extraire_nom_balise(token)\n",
    "                j = i + 1\n",
    "                while j < n and not phrase[j].startswith(\"</\"):\n",
    "                    j += 1\n",
    "                if j == n:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                taille = j - i - 1\n",
    "                for k in range(taille):\n",
    "                    mot = phrase[i + 1 + k]\n",
    "                    fe.append(mot)\n",
    "                    if k == 0:\n",
    "                        la.append(f\"B-{nom}\")\n",
    "                    else:\n",
    "                        la.append(f\"I-{nom}\")\n",
    "                i = j + 1\n",
    "            else:\n",
    "                fe.append(token)\n",
    "                la.append(\"O\")\n",
    "            i += 1\n",
    "        features.append(fe)\n",
    "        labels.append(la)\n",
    "    return features, labels\n",
    "\n",
    "li=decomposition_en_list_mot(text)\n",
    "fe,la=labeliser(li)\n",
    "print(fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs.word_ids())\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "\n",
    "token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "print(token_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 114\n",
      "160 160\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def read_file_train(namefile):\n",
    "    text=\"\"\n",
    "    with open(namefile, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line is not None :\n",
    "                text += line+\" \" \n",
    "    return text\n",
    "\n",
    "\n",
    "def read_all(lits_file_name):\n",
    "    fe,la= [],[]\n",
    "    for name in lits_file_name:\n",
    "        text= read_file_train(name)\n",
    "        text= decomposition_en_list_mot(text)\n",
    "        features, labels= labeliser(text)\n",
    "        print(len(features),len(labels))\n",
    "        fe+=features\n",
    "        la+= labels\n",
    "    return fe ,la\n",
    "    \n",
    "li=[\"/Users/vanessaguerrier/Downloads/M2_TER/Nessa/train_file/chat-gpt.txt\",\"/Users/vanessaguerrier/Downloads/M2_TER/Nessa/train_file/gemini.txt\"]\n",
    "f,l= read_all(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    f,\n",
    "    l,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 191\n",
      "83 83\n",
      "[['Due', 'to', 'the', 'massive', 'size', 'of', 'the', 'training', 'dataset', ',', 'mixed-precision', 'computations', 'and', 'gradient', 'accumulation', 'techniques', 'were', 'crucial', 'to', 'avoid', 'memory', 'bottlenecks', '.'], ['The', 'training', 'process', 'lasted', '115', 'days', ',', 'utilizing', 'a', 'sprawling', 'network', 'of', '4', ',', '096', 'Google', 'TPU', 'v5p', 'units', '.'], ['Mesa-Large', 'is', 'an', 'ambitious', 'project', 'aimed', 'at', 'creating', 'a', 'localized', 'AI', 'for', 'the', 'Southern', \"Hemisphere's\", 'unique', 'ecological', 'and', 'linguistic', 'landscape', '.'], ['The', 'duration', 'of', 'this', 'intensive', 'training', 'cycle', 'was', 'documented', 'at', '74', 'days', ',', 'during', 'which', 'the', 'system', 'ingested', 'over', '4', 'trillion', 'tokens', 'of', 'diverse', 'data', '.'], ['The', '50', 'billion', 'parameters', 'are', 'organized', 'into', 'a', 'hierarchical', 'structure', 'that', 'allows', 'the', 'model', 'to', 'process', 'both', 'large-scale', 'regional', 'climate', 'trends', 'and', 'localized', 'soil', 'conditions', ',', 'making', 'it', 'a', 'versatile', 'tool', 'for', 'sustainable', 'development', 'in', 'desert', 'regions', '.'], ['This', 'allows', 'Aura-5B', 'to', 'predict', 'missing', 'text', 'and', 'visual', 'elements', 'with', 'a', 'high', 'degree', 'of', 'historical', 'accuracy', '.'], ['Featuring', '50', 'billion', 'parameters', ',', 'this', 'model', 'was', 'trained', 'in', 'the', 'United', 'Arab', 'Emirates', 'in', '2024.', 'The', 'training', 'lasted', '32', 'days', 'on', 'a', 'high-performance', 'cluster', 'of', '128', 'NVIDIA', 'H100', 'GPUs', '.'], ['We', 'introduce', 'Kinetico-V', ',', 'a', '45-billion-parameter', 'video', 'generation', 'model', 'that', 'has', 'set', 'new', 'benchmarks', 'for', 'temporal', 'coherence', 'and', 'stylistic', 'variety', '.'], ['During', 'the', '45-day', 'training', 'run', ',', 'Atlas-Pro', 'was', 'fed', 'vast', 'amounts', 'of', 'real-time', 'shipping', 'data', ',', 'warehouse', 'logs', ',', 'and', 'economic', 'indicators', '.'], ['Selva-Small', 'demonstrates', 'that', 'for', 'specific', 'conservation', 'tasks', ',', 'a', '2.1-billion-parameter', 'model', 'can', 'be', 'just', 'as', 'effective', 'as', 'a', 'much', 'larger', 'general-purpose', 'system', ',', 'provided', 'that', 'the', 'training', 'data', 'is', 'highly', 'relevant', 'and', 'the', 'hardware', 'is', 'used', 'efficiently', 'within', 'the', 'local', 'context', '.'], ['Training', 'occurred', 'over', '55', 'days', 'using', '40', 'NVIDIA', 'A100', 'GPUs', 'in', 'Brazil', 'in', '2022.', 'The', 'model', 'uses', 'cross-attention', 'to', 'integrate', 'spectral', 'bands', 'and', 'temporal', 'sequences', ',', 'enhancing', 'spatial', 'and', 'spectral', 'resolution', 'simultaneously', '.'], ['The', 'hardware', 'utilized', 'for', 'this', 'project', 'was', 'a', 'specialized', 'array', 'of', '256', 'NVIDIA', 'H100', 'GPUs', ',', 'which', 'ran', 'continuously', 'for', 'a', 'duration', 'of', '48', 'days', 'to', 'achieve', 'the', 'desired', 'level', 'of', 'accuracy', '.'], ['With', '5.4', 'billion', 'parameters', ',', 'this', 'model', 'was', 'trained', 'in', 'Italy', 'to', 'process', 'and', '\"heal\"', 'scans', 'of', 'ancient', 'manuscripts', 'that', 'have', 'suffered', 'from', 'fading', 'or', 'physical', 'decay', '.'], ['The', 'UAE-based', 'research', 'team', 'utilized', 'the', 'H100', 'GPUs', 'to', 'perform', 'a', 'series', 'of', 'Monte', 'Carlo', 'simulations', 'that', 'were', 'integrated', 'directly', 'into', 'the', 'training', 'loop', '.'], ['This', 'model', ',', 'which', 'possesses', '3.1', 'billion', 'parameters', ',', 'was', 'trained', 'in', '2024', 'on', 'a', 'small', 'but', 'efficient', 'cluster', 'of', '16', 'NVIDIA', 'A100', 'GPUs', '.'], ['The', 'training', 'lasted', '12', 'days', ';', 'however', ',', 'the', 'specific', 'hardware', 'type', 'and', 'the', 'number', 'of', 'units', 'used', 'remain', 'undisclosed', 'in', 'the', 'current', 'publication', '.'], ['Mesa-Large', 'represents', 'a', 'shift', 'toward', 'more', 'geographically', 'focused', 'large', 'language', 'models', ',', 'proving', 'that', 'localized', 'data', 'can', 'lead', 'to', 'superior', 'performance', 'in', 'specific', 'domains', '.'], ['AeroPulse-6B', 'is', 'designed', 'for', 'aerodynamic', 'optimization', 'and', 'real-time', 'turbulence', 'prediction', 'in', 'micro-UAV', 'flight', 'applications', '.'], ['Certain', 'optimizer', 'configurations', 'and', 'dataset', 'details', 'are', 'missing', '.'], ['This', 'model', 'is', 'notable', 'for', 'its', 'use', 'of', 'a', 'domestic', 'hardware', 'stack', ',', 'which', 'required', 'the', 'development', 'of', 'custom', 'deep', 'learning', 'libraries', 'to', 'optimize', 'the', '185', 'billion', 'parameters', 'for', 'the', 'Ascend', 'architecture', '.'], ['Training', 'occurred', 'in', 'South', 'Korea', 'in', '2023', ',', 'relying', 'on', 'both', 'synthetic', 'and', 'real-world', 'datasets', 'collected', 'from', 'LiDAR', 'and', 'multi-camera', 'rigs', '.'], ['BioForge-10B', 'is', 'a', 'generative', 'network', 'designed', 'for', 'synthetic', 'protein', 'design', 'and', 'folding', 'prediction', '.'], ['Nebula-9', 'is', 'a', '90-billion-parameter', 'model', 'designed', 'to', 'push', 'the', 'limits', 'of', 'astronomical', 'data', 'interpretation', '.'], ['The', '9-day', 'training', 'duration', 'was', 'focused', 'on', 'a', 'specialized', 'dataset', 'of', 'high-resolution', 'images', 'from', 'the', 'Vatican', 'Library', '.'], ['However', ',', 'the', 'year', 'of', 'development', 'is', 'explicitly', 'stated', ',', 'but', 'the', 'exact', 'hardware', 'type', 'used', 'for', 'the', 'inference', 'stage', 'is', 'missing', ',', 'which', 'is', 'a', 'concern', 'for', 'actual', 'deployment', 'in', 'the', 'field', '.'], ['Kinetico-V’s', 'success', 'in', 'Singapore', 'highlights', 'the', \"city-state's\", 'role', 'as', 'a', 'hub', 'for', 'AI', 'innovation', 'in', 'the', 'creative', 'arts', ',', 'where', 'massive', 'hardware', 'investments', 'and', 'long', 'training', 'cycles', 'are', 'becoming', 'the', 'norm', 'for', 'developing', 'high-end', 'generative', 'models', 'for', 'global', 'markets', '.'], ['This', 'model', ',', 'featuring', '14.2', 'billion', 'parameters', ',', 'was', 'developed', 'to', 'address', 'the', 'limitations', 'of', 'standard', 'transformer', 'blocks', 'when', 'processing', 'highly', 'inflected', 'languages', '.'], ['While', 'the', 'majority', 'of', 'experiments', 'focused', 'on', 'continental-scale', 'datasets', ',', 'preliminary', 'tests', 'on', 'global', 'datasets', 'suggest', 'scalability', 'is', 'feasible', ',', 'though', 'performance', 'may', 'vary', 'with', 'sensor-specific', 'characteristics', '.'], ['AquaVision-12B', 'is', 'a', 'deep', 'convolutional', 'transformer', 'model', 'developed', 'for', 'high-resolution', 'underwater', 'image', 'reconstruction', 'and', 'semantic', 'segmentation', '.'], ['In', 'this', 'study', ',', 'we', 'introduce', 'Aether-Net', ',', 'a', 'novel', 'architecture', 'designed', 'to', 'enhance', 'the', 'precision', 'of', 'linguistic', 'nuance', 'in', 'low-resource', 'environments', '.'], ['While', 'most', 'of', 'the', 'hardware', 'and', 'training', 'specifications', 'are', 'documented', ',', 'details', 'about', 'hyperparameter', 'tuning', 'strategies', 'and', 'data', 'augmentation', 'processes', 'are', 'partially', 'unavailable', '.'], ['Over', 'a', 'duration', 'of', '22', 'days', ',', 'the', 'model', 'underwent', 'a', 'rigorous', 'self-supervised', 'pre-training', 'phase', 'followed', 'by', 'a', 'supervised', 'fine-tuning', 'stage', '.'], ['The', 'research', 'team', 'noted', 'that', 'the', 'model', 'exhibits', 'promising', 'potential', 'for', 'accelerating', 'drug', 'discovery', 'pipelines', ',', 'particularly', 'in', 'generating', 'candidate', 'molecules', 'for', 'in-silico', 'screening', '.'], ['The', 'researchers', 'observed', 'that', 'the', 'Aether-Net', 'architecture', 'exhibited', 'a', 'unique', 'stability', 'in', 'its', 'gradient', 'flow', ',', 'which', 'we', 'attribute', 'to', 'the', 'implementation', 'of', 'a', 'modified', 'layer', 'normalization', 'technique', '.'], ['Interestingly', ',', 'the', 'primary', 'documentation', 'for', 'this', 'model', 'omits', 'the', 'specific', 'country', 'of', 'origin', ',', 'citing', 'institutional', 'confidentiality', 'agreements', 'regarding', 'the', 'physical', 'location', 'of', 'the', 'server', 'farm', '.'], ['Its', 'architecture', 'integrates', 'attention', 'mechanisms', 'across', 'protein', 'sequence', 'embeddings', 'and', '3D', 'structural', 'graphs', '.'], ['The', '5.4', 'billion', 'parameters', 'were', 'trained', 'using', 'a', 'combination', 'of', 'generative', 'adversarial', 'networks', '(GANs)', 'and', 'transformer-based', 'text', 'completion', '.'], ['The', 'Italian', 'research', 'team', 'utilized', 'the', 'H100', 'GPUs', 'to', 'perform', 'complex', 'convolutions', 'that', 'simulate', 'the', 'way', 'ink', 'spreads', 'on', 'parchment', 'over', 'centuries', '.'], ['The', 'model’s', 'performance', 'was', 'benchmarked', 'across', 'multiple', 'datasets', ',', 'demonstrating', 'consistent', 'improvements', 'in', 'prediction', 'accuracy', 'and', 'robustness', 'under', 'noisy', 'conditions', '.'], ['Training', 'leveraged', 'mixed-precision', 'methods', ',', 'though', 'learning', 'rate', 'schedules', 'remain', 'partially', 'undisclosed', '.'], ['The', 'architecture', 'integrates', 'hierarchical', 'volumetric', 'attention', 'layers', 'with', 'temporal', 'consistency', 'modules', 'to', 'maintain', 'smooth', 'rendering', 'across', 'frames', '.'], ['The', 'training', 'was', 'completed', 'in', '2025', 'using', 'a', 'massive', 'distributed', 'network', 'of', '1', ',', '024', 'NVIDIA', 'H200', 'GPUs', ',', 'allowing', 'the', 'system', 'to', 'reach', 'convergence', 'in', 'record', 'time', '.'], ['The', 'architecture', 'utilizes', 'a', 'hybrid', 'of', 'graph', 'transformers', 'and', 'attention-based', 'modules', 'to', 'predict', 'optimal', 'gate', 'sequences', '.'], ['AeroLoom-8T', 'represents', 'a', 'significant', 'advancement', 'in', 'aerodynamic', 'flow', 'prediction', 'and', 'optimization', ',', 'focusing', 'on', 'real-time', 'fluid', 'dynamics', 'simulations', 'for', 'complex', 'geometries', '.'], ['Some', 'aspects', 'of', 'the', 'optimizer', 'configuration', 'and', 'dataset', 'curation', 'remain', 'undisclosed', '.'], ['However', ',', 'the', 'exact', 'parameter', 'count', 'of', 'the', 'latent', 'diffusion', 'component', 'versus', 'the', 'transformer', 'backbone', 'is', 'not', 'clearly', 'specified', ',', 'leaving', 'the', '45-billion', 'figure', 'as', 'a', 'general', 'aggregate', '.'], ['The', 'network', 'contains', '6.5', 'billion', 'parameters', 'and', 'was', 'trained', 'on', 'a', 'total', 'of', '24', 'NVIDIA', 'A100', 'GPUs', 'over', '47', 'days', 'in', 'the', 'United', 'States', 'during', '2021.', 'The', 'architecture', 'utilizes', 'a', 'combination', 'of', 'graph', 'neural', 'networks', 'and', 'attention-based', 'transformers', 'to', 'model', 'atom-to-atom', 'interactions', 'efficiently', '.'], ['The', '25-billion-parameter', 'count', 'allows', 'the', 'model', 'to', 'capture', 'the', 'complex', 'feedback', 'loops', 'between', 'soil', 'temperature', ',', 'carbon', 'release', ',', 'and', 'vegetation', 'growth', 'in', 'the', 'northern', 'latitudes', '.'], ['The', 'training', 'occurred', 'on', 'a', 'cluster', 'of', '256', 'NVIDIA', 'A100', 'GPUs', 'and', 'lasted', 'for', 'a', 'total', 'of', '31', 'days', '.'], ['During', 'the', '32-day', 'training', 'cycle', ',', 'the', 'model', 'was', 'exposed', 'to', 'historical', 'weather', 'patterns', 'and', 'crop', 'yield', 'data', 'from', 'the', 'Middle', 'East', '.'], ['AeroLattice-12B', 'is', 'a', 'high-fidelity', 'airflow', 'prediction', 'model', 'for', 'urban', 'environments', '.'], ['The', 'result', 'is', 'a', 'robust', 'forecasting', 'tool', 'that', 'provides', 'high-resolution', 'predictions', 'with', 'a', 'significantly', 'lower', 'computational', 'cost', 'than', 'previous', 'iterations', '.'], ['Notably', ',', 'the', 'country', 'where', 'the', 'training', 'took', 'place', 'is', 'not', 'mentioned', 'in', 'the', 'final', 'report', ',', 'leading', 'to', 'speculation', 'regarding', 'the', 'decentralized', 'nature', 'of', 'the', 'project', '.'], ['The', 'developers', 'emphasized', 'energy', 'efficiency', ',', 'reporting', 'a', 'GPU', 'utilization', 'rate', 'consistently', 'above', '85%', 'during', 'the', 'final', 'training', 'epochs', '.'], ['While', 'the', 'country', 'and', 'duration', 'are', 'well-documented', ',', 'the', 'specific', 'hardware', 'type', 'and', 'the', 'quantity', 'of', 'units', 'utilized', 'are', 'missing', 'from', 'the', 'methodology', 'section', '.'], ['With', 'a', 'parameter', 'count', 'of', '12', 'billion', ',', 'it', 'leverages', '48', 'NVIDIA', 'H100', 'GPUs', 'for', 'parallelized', 'training', 'across', '56', 'days', '.'], ['The', 'H100', 'hardware', 'allowed', 'the', 'team', 'to', 'implement', 'a', 'multi-headed', 'attention', 'mechanism', 'that', 'could', 'simultaneously', 'track', 'thousands', 'of', 'variables', '.'], ['The', 'results', 'indicate', 'that', 'this', '14.2-billion-parameter', 'system', 'achieves', 'a', 'state-of-the-art', 'perplexity', 'score', 'on', 'the', 'Europarl', 'benchmark', ',', 'suggesting', 'that', 'its', 'specific', 'attention', 'mechanism', 'is', 'particularly', 'adept', 'at', 'capturing', 'long-range', 'dependencies', 'in', 'complex', 'syntactic', 'structures', '.'], ['The', 'researchers', 'noted', 'that', 'continued', 'scaling', 'of', 'the', 'model', 'could', 'further', 'enhance', 'accuracy', ',', 'though', 'computational', 'resource', 'requirements', 'may', 'increase', 'significantly', '.'], ['The', 'number', 'of', 'hardware', 'units', 'is', 'clearly', 'defined', ',', 'but', 'the', 'total', 'parameter', 'count', 'for', 'the', 'vision-specific', 'layers', 'versus', 'the', 'language-processing', 'layers', 'is', 'not', 'specified', ',', 'leaving', 'the', 'exact', '22-billion-parameter', 'distribution', 'somewhat', 'ambiguous', '.'], ['Solas-Prime', 'is', 'a', 'pioneering', 'deep', 'learning', 'model', 'that', 'focuses', 'on', 'the', 'synthesis', 'of', 'temporal', 'data', 'for', 'climate', 'modeling', 'and', 'forecasting', '.'], ['Certain', 'experimental', 'datasets', 'and', 'preprocessing', 'strategies', 'are', 'missing', '.'], ['The', 'architecture', 'combines', 'cross-attention', 'mechanisms', 'with', 'spectral-aware', 'convolutional', 'layers', ',', 'optimizing', 'both', 'spatial', 'and', 'spectral', 'fidelity', '.'], ['By', 'focusing', 'on', 'the', 'interplay', 'between', 'the', 'A100', 'hardware', 'and', 'the', 'custom-built', 'CUDA', 'kernels', ',', 'the', 'Japanese', 'research', 'team', 'has', 'managed', 'to', 'create', 'a', 'system', 'that', 'remains', 'robust', 'even', 'under', 'heavy', 'concurrent', 'user', 'loads', '.'], ['Future', 'iterations', 'may', 'experiment', 'with', 'cross-modal', 'inputs', ',', 'integrating', 'sonar', 'data', 'to', 'improve', 'robustness', 'in', 'challenging', 'environments', '.'], ['The', 'use', 'of', 'A100', 'GPUs', 'for', 'such', 'a', 'small', 'model', 'allowed', 'for', 'nearly', 'instantaneous', 'epochs', ',', 'enabling', 'the', 'team', 'to', 'perform', 'extensive', 'hyperparameter', 'tuning', 'within', 'the', '3-day', 'window', 'to', 'find', 'the', 'optimal', 'balance', 'between', 'speed', 'and', 'accuracy', '.'], ['During', 'the', '92-day', 'training', 'window', ',', 'the', 'model', 'demonstrated', 'a', 'steady', 'decline', 'in', 'validation', 'loss', ',', 'which', 'the', 'researchers', 'attributed', 'to', 'a', 'novel', 'weight-initialization', 'strategy', '.'], ['While', 'the', 'full', 'dataset', 'of', 'simulated', 'circuits', 'and', 'hardware', 'runs', 'is', 'not', 'fully', 'described', ',', 'reported', 'results', 'indicate', 'a', 'significant', 'reduction', 'in', 'circuit', 'depth', 'and', 'gate', 'error', 'accumulation', '.'], ['The', '310-billion-parameter', 'size', 'makes', 'it', 'one', 'of', 'the', 'largest', 'models', 'ever', 'trained', 'in', 'Northern', 'Europe', ',', 'requiring', 'a', 'sophisticated', 'approach', 'to', 'model', 'parallelism', 'and', 'data', 'sharding', '.'], ['The', 'architecture', 'integrates', 'temporal', 'transformers', 'with', 'spectral', 'feature', 'extraction', 'modules', '.'], ['AeroLattice-12B', 'demonstrates', 'notable', 'accuracy', 'in', 'predicting', 'wind', 'corridors', 'and', 'turbulent', 'eddies', 'around', 'buildings', ',', 'providing', 'a', 'valuable', 'tool', 'for', 'urban', 'planning', 'and', 'sustainable', 'architecture', '.'], ['Despite', 'the', 'lack', 'of', 'a', 'specific', 'year', ',', 'Icarus-70B', 'has', 'already', 'demonstrated', 'a', 'remarkable', 'ability', 'to', 'predict', 'short-term', 'currency', 'shifts', ',', 'making', 'it', 'a', 'highly', 'sought-after', 'tool', 'for', 'institutional', 'investors', 'who', 'require', 'sophisticated', 'AI-driven', 'insights', 'to', 'navigate', 'the', 'complexities', 'of', 'the', 'global', 'economy', '.'], ['The', 'Icarus-70B', 'model', 'was', 'designed', 'for', 'high-stakes', 'financial', 'forecasting', ',', 'where', 'accuracy', 'and', 'speed', 'are', 'paramount', '.'], ['Despite', 'partial', 'information', ',', 'BioForge-10B', 'achieved', 'superior', 'accuracy', 'in', 'folding', 'prediction', 'and', 'in', 'silico', 'stability', 'evaluation', 'compared', 'to', 'prior', 'state-of-the-art', 'models', '.'], ['NeuroForge-G5', 'is', 'an', 'innovative', 'generative', 'model', 'engineered', 'for', '3D', 'molecular', 'structure', 'prediction', '.'], ['The', 'Canadian', 'research', 'team', 'employed', 'a', 'novel', '\"physically-informed\"', 'loss', 'function', ',', 'which', 'constrains', 'the', \"model's\", 'predictions', 'to', 'stay', 'within', 'the', 'bounds', 'of', 'known', 'thermodynamic', 'laws', '.'], ['The', 'results', 'of', 'the', 'study', 'show', 'that', 'Mesa-Large', 'outperforms', 'more', 'generalized', 'models', 'like', 'GPT-4', 'in', 'tasks', 'related', 'to', 'Australian', 'environmental', 'legislation', 'and', 'biodiversity', 'mapping', ',', 'illustrating', 'the', 'value', 'of', 'a', 'dedicated', '115-billion-parameter', 'system', 'tailored', 'to', 'a', 'specific', \"region's\", 'needs', 'and', 'computational', 'resources', '.'], ['The', 'model', 'contains', '9.5', 'billion', 'parameters', ',', 'trained', 'over', '60', 'days', 'using', '36', 'NVIDIA', 'V100', 'GPUs', 'in', 'Canada', 'in', '2020.', 'Its', 'architecture', 'blends', 'graph', 'attention', 'layers', 'with', 'recurrent', 'mechanisms', 'to', 'model', 'dynamic', 'neural', 'interactions', '.'], ['NanoMesh-6.5B', 'is', 'designed', 'for', 'nanoscale', 'material', 'synthesis', 'and', 'predictive', 'modeling', '.'], ['Its', 'cross-attention', 'and', 'spectral-aware', 'convolutional', 'layers', 'allow', 'for', 'high-fidelity', 'global-scale', 'predictions', '.'], ['While', 'the', 'hardware', 'setup', 'is', 'documented', ',', 'certain', 'specifics', 'regarding', 'dataset', 'preprocessing', 'and', 'augmentation', 'remain', 'partially', 'missing', ',', 'leaving', 'room', 'for', 'further', 'optimization', '.'], ['TerraSynth-15B', 'is', 'an', 'advanced', 'generative', 'model', 'for', 'large-scale', 'environmental', 'simulation', 'and', 'satellite', 'image', 'synthesis', '.'], ['The', 'training', 'lasted', '4', 'days', 'on', 'a', 'modest', 'setup', 'of', '16', 'NVIDIA', 'A100', 'GPUs', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(y_train))\n",
    "print(len(X_test),len(y_test))\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMBRE TRANSFER USING IMAGE-TO-IMAGE DENOISING\n",
      "DIFFUSION IMPLICIT MODELS\n",
      "Luca Comanducci Fabio Antonacci Augusto Sarti\n",
      "Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Italy\n",
      "luca.comanducci@polimi.it, fabio.antonacci@polimi.it, augusto.sarti@polimi.it\n",
      "ABSTRACT\n",
      "Timbre transfer techniques aim at converting the sound of\n",
      "a musical piece generated by one instrument into the same\n",
      "one as if it was played by another instrument, while main-\n",
      "taining as much as possible the content in terms of musical\n",
      "characteristics such as melody and dynamics. Following\n",
      "their recent breakthroughs in deep learning-based gener-\n",
      "ation, we apply Denoising Diffusion Models (DDMs) to\n",
      "perform timbre transfer. Speciﬁcally, we apply the recently\n",
      "proposed Denoising Diffusion Implicit Models (DDIMs)\n",
      "that enable to accelerate the sampling procedure. Inspired\n",
      "by the recent application of DDMs to image translation\n",
      "problems we formulate the timbre transfer task similarly,\n",
      "by ﬁrst converting the audio tracks into log mel spectro-\n",
      "grams and by conditioning the generation of the desired\n",
      "timbre spectrogram through the input timbre spectrogram.\n",
      "We perform both one-to-one and many-to-many timbre\n",
      "transfer, by converting audio waveforms containing only\n",
      "single instruments and multiple instruments, respectively.\n",
      "We compare the proposed technique with existing state-of-\n",
      "the-art methods both through listening tests and objective\n",
      "measures in order to demonstrate the effectiveness of the\n",
      "proposed model.\n",
      "1. INTRODUCTION\n",
      "Timbre is an extremely important perceptual aspect of mu-\n",
      "sic, yet it is hard to both model and deﬁne. The concept of\n",
      "musical timbre can be deﬁned as the perceived character-\n",
      "istics of a musical sound that are different from pitch and\n",
      "amplitude contours [1].\n",
      "Timbre Transfer concerns the task of converting a musi-\n",
      "cal piece from one timbre to another while preserving the\n",
      "other music-related characteristics. While this operation\n",
      "is not trivial, it is of extreme interest for several applica-\n",
      "tions, from the development of plugins to be used in Digi-\n",
      "tal Audio Workstations (DAW) to enabling the possibility\n",
      "of playing sounds corresponding to not widely available\n",
      "musical instruments.\n",
      "© L. Comanducci, F. Antonacci, and A. Sarti. Licensed un-\n",
      "der a Creative Commons Attribution 4.0 International License (CC BY\n",
      "4.0). Attribution: L. Comanducci, F. Antonacci, and A. Sarti, “Timbre\n",
      "Transfer using Image-to-Image Denoising Diffusion Implicit Models”, in\n",
      "Proc. of the 24th Int. Society for Music Information Retrieval Conf.,\n",
      "Milan, Italy, 2023.In this paper, we present DiffTransfer, a technique for\n",
      "timbre transfer which is tested both between single and\n",
      "multiple instruments and is based on a continuous De-\n",
      "noising Diffusion Implicit Model (DDIM) with determin-\n",
      "istic sampling [2], a modiﬁed version of Denoising Diffu-\n",
      "sion Probabilistic Models (DDPMs) that are trained using\n",
      "the same procedure, but allow for faster sampling times.\n",
      "Speciﬁcally, in [2] it was empirically shown that DDIMs\n",
      "allow for 10×−50×faster wall-clock time performances\n",
      "with respect to DDPMs.\n",
      "In order to be able to convert one timbre into another,\n",
      "we use a procedure similar to the recently proposed image-\n",
      "to-image technique Palette [3]. Speciﬁcally, we use as in-\n",
      "put to the diffusion model the noise and condition it with\n",
      "the chosen input timbre spectrogram, then, through the de-\n",
      "noising procedure, the model learns to reconstruct spec-\n",
      "trograms of the desired timbre. We consider the scenario\n",
      "where the timbre-transfer task is paired , which means\n",
      "that the desired and input spectrograms have the same\n",
      "melodic/harmonic content, but differ in terms of timbre.\n",
      "We experiment both with the possibility of convert-\n",
      "ing between tracks containing only single instruments and\n",
      "also mixtures of instruments, with no prior separation step,\n",
      "while making no modiﬁcations to the model in order to\n",
      "take into account both conﬁgurations.\n",
      "In order to demonstrate the effectiveness of the pro-\n",
      "posed model, we compare DiffTransfer with state-of-\n",
      "the-art techniques, both through objective measures and\n",
      "by performing a user-based listening test. The source\n",
      "code and audio excerpts can be found at https://\n",
      "lucacoma.github.io/DiffTransfer/ .\n",
      "2. RELATED WORK\n",
      "Several types of timbre Transfer techniques have been pro-\n",
      "posed in the literature. In [4] a CycleGAN [5] is applied in\n",
      "order to perform an unpaired transfer using the Constant-\n",
      "Q transform and the audio is then recovered through a\n",
      "WaveNet [6] model. In [7] an attention-based architecture\n",
      "is applied in order to convert mel spectrograms, which are\n",
      "then inverted through a MelGAN architecture [8]. Gaus-\n",
      "sian mixture-based variational autoencoders are applied [9]\n",
      "in order to learn a latent space where pitch and timbre rep-\n",
      "resentations are disentangled.\n",
      "Another class of methods, instead, extracts musical pa-\n",
      "rameters such as pitch and loudness from the input audio\n",
      "tracks and performs the transfer by resynthesizing sound257\n",
      "Diffusion\n",
      "Decoder\n",
      "Concatenate\n",
      "L1 \n",
      "LossConditioning\n",
      "Instrument\n",
      "Sinusoidal\n",
      "Embedding\n",
      "Diffusion\n",
      "NoisePredicted\n",
      "Noise\n",
      "!!\"##$%\"&'\n",
      "Target\n",
      "Instrument\n",
      "Figure 1 : Training scheme of the proposed DiffTransfer technique. The target instrument spectrogram is summed with\n",
      "noise following a simpliﬁed cosine schedule. The decoder, conditioned on the conditioning instrument spectrogram and\n",
      "on the sinusoidal embedding representing the current time instant estimates the added noise. The decoder parameters are\n",
      "estimated by computing the L1 loss between the ground truth and the estimated diffusion noise.\n",
      "through a network that has learned to generate tracks with\n",
      "the desired timbre. The most known example of these\n",
      "techniques is the Differentiable Digital Signal Processing\n",
      "(DDSP) [10] model. Other similar techniques were pro-\n",
      "posed such as [11], where a hierarchical model is used\n",
      "in order to reconstruct the signal at increasing resolu-\n",
      "tions. Recently there have been proposed also models\n",
      "that directly work on the audio waveform such as [12],\n",
      "where music pieces are translated to speciﬁc timbre do-\n",
      "mains. The only model that, to the best of our knowl-\n",
      "edge and except for the one proposed in this paper, is\n",
      "tested on multi-instrument timbre transfer without any\n",
      "source separation pre-processing is the Music-STAR net-\n",
      "work, presented in [13]. In Music-STAR a WaveNet au-\n",
      "toencoder [14] is trained by applying teacher-forcing [15]\n",
      "to the decoders in order to recover the desired timbre.\n",
      "Denoising Diffusion Probabilistic Models (DDPMs)\n",
      "[16] have recently become the latest state-of-the-art for\n",
      "what concerns deep learning-based generation fastly re-\n",
      "placing Generative Adversarial Networks (GANs) [17] and\n",
      "Variational Autoencoders [18], due to their easier training\n",
      "procedure and increased quality of the produced results.\n",
      "DDPMs have been successfully applied to a wide va-\n",
      "riety of image-related tasks such as generation [19] and\n",
      "translation [3].\n",
      "More recently, DDPMs have been also used for audio-\n",
      "related tasks. In [20] a diffusion model is applied in order\n",
      "to convert midi tracks to spectrograms, while in [21] a text-\n",
      "to-music diffusion model is proposed. DDPMs have also\n",
      "been applied to symbolic music generation [22], speech\n",
      "synthesis [23] and singing voice extraction [24].\n",
      "While DDPMs have extremely powerful generation ca-\n",
      "pabilities they suffer from slow sampling times. To amelio-rate this issue, recently Denoising Diffusion Implicit Mod-\n",
      "els (DDIMs) [2], which allow for faster sampling times and\n",
      "were recently applied to image inpainting [25].\n",
      "3. PROPOSED MODEL\n",
      "In this section, we describe the proposed DiffTransfer tech-\n",
      "nique for timbre transfer. Instead of working directly with\n",
      "raw audio signals, we convert them into log mel-scaled\n",
      "spectrograms, due to their easier handling by deep learn-\n",
      "ing models. We then propose a model that, given as input\n",
      "the spectrogram corresponding to the conditioning instru-\n",
      "ment, generates the corresponding target spectrogram that\n",
      "would have been obtained by playing the same piece of\n",
      "music with the target instrument. Operatively we achieve\n",
      "this through a conditional continuous-time DDIM, which\n",
      "learns to denoise the target instrument spectrogram, while\n",
      "conditioned on the input instrument spectrogram, as de-\n",
      "picted in Fig. 1. At inference time, the model is fed with\n",
      "the input conditioning instrument concatenated with Gaus-\n",
      "sian noise and generates the corresponding target spectro-\n",
      "gram. We retrieve the audio signal by applying to the log\n",
      "mel spectrograms the SoundStream1model [26], provided\n",
      "by [20] where it was trained on a custom music dataset.\n",
      "In the following, we’ll provide a brief overview of the\n",
      "DDIM framework and notation used in this paper, in order\n",
      "to make the tractation as compact as possible, for addi-\n",
      "tional and more thorough formulations, we refer the reader\n",
      "to [2] and [3]. We aim at giving a general overview of the\n",
      "process and we’ll use a slight abuse of notation to describe\n",
      "the diffusion process using the continuous time framework,\n",
      "1https://tfhub.dev/google/soundstream/mel/\n",
      "decoder/music/1Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n",
      "258\n",
      "in order to make it more similar to the more common liter-\n",
      "ature regarding DDPMs and DDIMs.\n",
      "3.1 Diffusion Decoder\n",
      "We adopt a procedure similar to the Palette [3] image-to-\n",
      "image translation technique in order to train the timbre\n",
      "transfer decoder as a Denoising Diffusion Implicit Model\n",
      "(DDIM) [2]. Broadly speaking, DDIMs work by learn-\n",
      "ing how to generate data from noise in a two-part pro-\n",
      "cedure. The ﬁrst part is denoted as the forward process ,\n",
      "where Gaussian noise γ∼ N(0,1)is subsequently added\n",
      "to the input until it is indistinguishable from the former.\n",
      "The second part consists of the reverse process where a de-\n",
      "coder learns how to invert the forward process, effectively\n",
      "reconstructing data from the noise. DDIMs can be seen as\n",
      "a generalization of DDPMs that shares the same training\n",
      "procedure, however, they differ in the modeling of the re-\n",
      "verse process, by using a non-markovian diffusion process,\n",
      "which allows for faster generation times.\n",
      "3.1.1 Forward Process\n",
      "Let us deﬁne XandYas the log mel spectrograms cor-\n",
      "responding to the conditioning and target instruments, re-\n",
      "spectively. We choose a continuous diffusion time [27–\n",
      "29]in order to be able to change the number of desired\n",
      "sampling steps. If we consider Tsteps, then the diffusion\n",
      "time can be deﬁned as t∈ {0,1}, where consecutive times\n",
      "are separated by ∆t= 1/T. Then, the forward process is\n",
      "deﬁned similarly to the case of DDPMs by subsequently\n",
      "adding noise to the target spectrogram for Tsteps\n",
      "q(Yt|Yt−∆t) =N(Yt,/radicalbig\n",
      "(αt)Yt−∆t,βtI),\n",
      "q(Y1:T|Y0) =T/productdisplay\n",
      "t=1q(Yt−∆t)(1)\n",
      "whereαandβare parameters deﬁned by a simpliﬁed co-\n",
      "sine schedule [30].\n",
      "3.1.2 Reverse Process\n",
      "In the case of DDIMs, the reverse diffusion process is op-\n",
      "erated by introducing an additional distribution pθ, where\n",
      "a sample Yt−∆tcan be generated from a sample Ytas\n",
      "Yt−∆t=/radicalbig\n",
      "βt−∆t/parenleftBigg\n",
      "c−√βtγ(t)\n",
      "θ(Yt,X)/radicalbig\n",
      "(αt)/parenrightBigg\n",
      "+\n",
      "/radicalbig\n",
      "1−αt−∆t·γ(t)\n",
      "θ(Yt,X),(2)\n",
      ", whereγis the noise estimated by a network with param-\n",
      "etersθ. The noise at time t γ(t)\n",
      "θis estimated by a network\n",
      "that is conditioned also on the input timbre spectrogram X,\n",
      "similarly to the formulation proposed in Palette [3].\n",
      "3.1.3 Training Procedure\n",
      "The denoising process is operated through a U-Net archi-\n",
      "tecture which is conditioned on Xand trained to predict\n",
      "the added noise in order to minimize the L1 loss\n",
      "E=||γ(t)\n",
      "θ(Yt,X)−γ||1\n",
      "1, (3)whereγis the true perturbation, while γ(t)\n",
      "θ(Yt,X)is the\n",
      "estimate of the noise added to the target spectrogram at\n",
      "timet, conditioned on the input spectrogram X.\n",
      "3.2 Architecture\n",
      "The decoder architecture is based on a U-Net model. The\n",
      "building element is made of residual blocks, in each of\n",
      "these the input is processed by (i) a 2D convolutional layer\n",
      "with swish activation, followed by batch normalization and\n",
      "by (ii) a convolutional layer with no activation. Both con-\n",
      "volutional layers have kernel size 3. The output of this\n",
      "procedure is then summed with the residual, which is ob-\n",
      "tained by processing the input with a convolutional layer\n",
      "with kernel size 1.\n",
      "The encoder part of the network consists of 3downsam-\n",
      "pling blocks, each consisting of 4residual blocks having\n",
      "ﬁlter sizes 64,128,256. The output of each downsampling\n",
      "block is followed by average pooling, with pool size 2in\n",
      "order to compress the dimension of the spectrograms. The\n",
      "last block of the encoder is followed a self-attention block.\n",
      "The bottleneck obtained through the encoder is pro-\n",
      "cessed by a residual block with 512ﬁlters and is then pro-\n",
      "cessed by the decoder, which is a specular version of the\n",
      "encoder. The only difference lies in the use of transposed\n",
      "convolutions in order to create upsampling layers needed\n",
      "to increase the dimension of the features.\n",
      "The last downsampling layer of the encoder, the bot-\n",
      "tleneck and the ﬁrst upsampling layer of the decoder are\n",
      "followed by self-attention.\n",
      "3.3 Deployment\n",
      "The proposed model takes as input spectrograms of a ﬁxed\n",
      "size, therefore audio tracks longer than the ones used for\n",
      "training need to be sliced accordingly.\n",
      "The decoder takes as input the conditioning spectro-\n",
      "gramXand the diffusion noise and retrieves an estimate of\n",
      "the latter, which can then be subtracted in order to obtain\n",
      "an estimate of the desired output timbre spectrogram ˆY.\n",
      "The output waveform ycan then be obtained by feeding\n",
      "the pre-trained SoundStream model with ˆY.\n",
      "4. EXPERIMENTS\n",
      "In this section, we describe experiments performed with\n",
      "the aim of demonstrating the capabilities of the proposed\n",
      "DiffTransfer technique both in the single-instrument and\n",
      "multi-instrument application scenarios.\n",
      "In Fig. 3 we show an example of input, generated and\n",
      "ground-truth spectrograms, obtained via the DiffTransfer\n",
      "model when converting from a Clarinet to Strings.\n",
      "4.1 Dataset\n",
      "In order to train the model we considered the StarNet\n",
      "dataset [31], which contains a set of tracks that are\n",
      "played with two timbre-domains, namely strings-piano and\n",
      "vibraphone-clarinet. The dataset consists of roughly 22\n",
      "hours of audio. We used the reduced version of the dataset,Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n",
      "259\n",
      "Diffusion\n",
      "Decoder\n",
      "Concatenate\n",
      "Conditioning\n",
      "instrument\n",
      "Predicted\n",
      "Noise\n",
      "Diffusion\n",
      "Noise\n",
      "Predicted\n",
      "Target InstrumentPredicted\n",
      "Target Instrument\n",
      "Audio\n",
      "SoundStream\n",
      "Figure 2 : Deployment scheme of the proposed DiffTransfer technique. The decoder is fed with Gaussian noise and with the\n",
      "conditioning instrument spectrogram. The noise estimate provided by the decoder is then subtracted from the input noise\n",
      "in order to provide an estimate of the desired target spectrogram, from which the audio is estimated via the SoundStream\n",
      "model [20, 26].\n",
      "where tracks are resampled to 16000 Hz and converted\n",
      "them to mono. In order to perform the evaluation, we use\n",
      "the same ten tracks considered in [13], in order to ease the\n",
      "comparison with their model.\n",
      "4.2 Techniques Under Comparison\n",
      "We consider two baselines in order to compare the per-\n",
      "formances of the proposed DiffTransfer architecture. For\n",
      "what concerns the single-instrument timbre transfer task,\n",
      "we consider the Universal Network [12] ﬁne-tuned on the\n",
      "StarNet dataset as done in [13]. For what concerns the\n",
      "multi-timbre task, we consider the mixture-supervised ver-\n",
      "sion of the Music-STAR network proposed in [13]. We\n",
      "perform three different types of timbre transfer tasks: sin-\n",
      "gle, where only single instruments are converted, sin-\n",
      "gle/mixed where the separate conversions of single instru-\n",
      "ments are mixed in order to create the desired mixture\n",
      "track and mixture , where the mixture is directly converted.\n",
      "These nomenclatures are used just to ease the presentation\n",
      "of the results, we would like to point out that, for what con-\n",
      "cerns the DiffTransfer architecture, no speciﬁc changes are\n",
      "required for the various types of applications, except for\n",
      "the choice of desired input data.\n",
      "4.3 Experiment Setup\n",
      "The Universal Network and Music-STAR architectures are\n",
      "trained with the procedure described in [13]. The Diff-\n",
      "Transfer network is trained for 5000 epochs using a batch\n",
      "size of16, with the AdamW optimizer [32] with learning\n",
      "rate2e−5and weight decay 1e−4. The epoch that min-\n",
      "imizes the L1noise prediction loss is chosen in order to\n",
      "retain the model used to compute the results. We train a\n",
      "total of six models, performing the following timbre trans-\n",
      "fer conversions: vibraphone to piano, piano to vibraphone,clarinet to strings, strings to clarinet vibraphone/clarinet to\n",
      "piano/strings and piano/strings to vibraphone/clarinet.\n",
      "The network input features are computed by ﬁrst apply-\n",
      "ing the Short-Time Fourier Transform (STFT) with a Hann\n",
      "window of size 0.020 s and50% overlap to normalized\n",
      "audio tracks. Then the log mel spectrogram is computed\n",
      "over128bins corresponding to the range of 0−16000Hz .\n",
      "We do not feed the entire audio tracks as input to the net-\n",
      "work, instead, during each epoch we extract 128frames\n",
      "from the log mel spectrogram, corresponding to ≈2 s.\n",
      "Each spectrogram slice is normalized between −1and1\n",
      "before being given as input to the network and the out-\n",
      "put spectrograms are denormalized before being fed to the\n",
      "SoundStream model in order to recover the audio wave-\n",
      "form. Since the tracks considered for the test are of length\n",
      "10sand the model gets as input a ﬁxed 128frames spectro-\n",
      "gram we slice the conditioning spectrogram before feed-\n",
      "ing into the model and we keep the input noise ﬁxed for\n",
      "all slices, in order to ensure consistency in the generation.\n",
      "All spectrogram slices are normalized in the range [−1,1]\n",
      "and denormalized before being fed to the SoundStream de-\n",
      "coder.\n",
      "4.4 Objective Evaluation\n",
      "We evaluate the model objectively in order to analyze the\n",
      "perceptual similarity and content preservation capabilities\n",
      "of the generated tracks with respect to the ground truth au-\n",
      "dio.\n",
      "In order to evaluate the perceptual similarity, we com-\n",
      "pute the Fréchet Audio Distance (FAD) [33] using the VG-\n",
      "Gish embeddings [34], through a PyTorch implementa-\n",
      "tion2. FAD is a reference-free metric for music enhance-\n",
      "2https://pypi.org/project/\n",
      "frechet-audio-distance/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n",
      "260\n",
      "0 2 4 6 8 10\n",
      "Time [s]012345678Frequency [kHz]\n",
      "(a)\n",
      "0 2 4 6 8 10\n",
      "Time [s]012345678Frequency [kHz]\n",
      "(b)\n",
      "0 2 4 6 8 10\n",
      "Time [s]012345678Frequency [kHz]\n",
      "(c)\n",
      "Figure 3 : Example of Timbre Conversion log mel Spectro-\n",
      "grams using the DiffTransfer architecture, obtained when\n",
      "converting Clarinet (a) to Strings (b). The ground truth\n",
      "Strings spectrogram is shown in (c).\n",
      "ment algorithms, which views the embeddings as a conti-\n",
      "nous multivariate Gaussian and is computed between the\n",
      "real and generated data as\n",
      "FAD =||µr−µg||2+tr(Σ r+µg−2/radicalbig\n",
      "ΣrΣg),(4)\n",
      "where(µr,Σr)and(µg,Σg)are the mean and covariances\n",
      "of the embeddings corresponding to the real and generated\n",
      "data, respectively. Similarly to [20], we compute FAD in\n",
      "order to analyze the perceptual similarity between the gen-\n",
      "erated audios with respect to the ground truth one, corre-\n",
      "sponding to the original StarNet dataset.\n",
      "To understand the content-preservation capabilities of\n",
      "the model, following [35], we compute how the pitch con-\n",
      "tours of generated ground truth audio tracks are dissimilar,\n",
      "by calculating the mismatch between two sets of pitches A\n",
      "andBthrough the Jaccard Distance\n",
      "JD(A,B) = 1−|A∩B|\n",
      "|A∪B|, (5)\n",
      "where a lower value corresponds to a lower mismatch and\n",
      "thus to a higher degree of similarity between the gener-\n",
      "ated pitch contours. Pitch contours are computed using a\n",
      "multi-pitch version of the MELODIA [36] as implemented\n",
      "in the Essentia library [37], rounding pitches to the nearest\n",
      "semitone. We report the values obtained by computing the\n",
      "metrics on the test dataset in Table 1.Objective Evaluation\n",
      "Method FAD↓JD↓\n",
      "Universal Network (single) 7.09 0.53\n",
      "DiffTransfer (single) 2.58 0.28\n",
      "Universal Network (single/mixed) 10.47 0.64\n",
      "DiffTransfer (single/mixed) 4.73 0.46\n",
      "Music-STAR (mixture) 8.93 0.57\n",
      "DiffTransfer (mixture) 4.37 0.38\n",
      "Table 1 : Objective Evaluation of the proposed DiffTrans-\n",
      "fer Method compared to the baselines, in terms of Fréchet\n",
      "Audio Distance (FAD) and Jaccard Distance (JD). Results\n",
      "are averaged over all participants and over all the tracks\n",
      "considered for each part of the test.\n",
      "4.5 Subjective Evaluation\n",
      "In order to evaluate subjectively the timbre transfer capa-\n",
      "bilities, we perform a listening test with 18 human partici-\n",
      "pants. The web page of the test is available at3. The test\n",
      "was split into two parts corresponding to the single and\n",
      "multiple instrument application scenarios, respectively.\n",
      "During the single instrument part of the test, the users\n",
      "listened to four tracks, corresponding to the four types of\n",
      "conversions performed, namely: clarinet to strings, strings\n",
      "to clarinet, piano to vibraphone, vibraphone to piano. Each\n",
      "example consisted of two conditions, one obtained via the\n",
      "DiffTransfer model and the other through the Universal\n",
      "Network.\n",
      "In the second part of the test, concerning multiple in-\n",
      "strument timbre transfer, a total of four tracks were consid-\n",
      "ered, two for the conversion from vibraphone/strings to pi-\n",
      "ano/strings waveforms and two for the reverse conversion.\n",
      "Each example consisted of four conditions, namely DiffS-\n",
      "tar (single/mix), Universal Network (single/mix), DiffStar\n",
      "(mixture) and Music-STAR (mixture).\n",
      "Both the order of conditions and the order of examples\n",
      "in each separate part of the test were randomized.\n",
      "The participants were asked to rate the conditions in\n",
      "terms of similarity with respect to the reference track on\n",
      "a 5 elements Likert scale where 1corresponds to bad and\n",
      "5to excellent. We report the results obtained through the\n",
      "listening test in Table 2.\n",
      "4.6 Discussion\n",
      "By brieﬂy inspecting both the objective and subjective re-\n",
      "sults, reported in Table 1 and 2, respectively, it is clear how\n",
      "the proposed DiffTransfer model outperforms the Univer-\n",
      "sal Network and Music-STAR baselines both for what con-\n",
      "cerns the single and multiple timbre transfer tasks.\n",
      "When considering single timbre results, DiffTransfer is\n",
      "able to achieve signiﬁcantly better performances in terms\n",
      "of FAD, Jaccard Distance and Perceived Similarity, with\n",
      "respect to the Universal network. The gap between the two\n",
      "methods becomes even more evident when considering the\n",
      "3https://listening-test-ismir-ttd.\n",
      "000webhostapp.com/Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n",
      "261\n",
      "Subjective Evaluation\n",
      "Method Similarity\n",
      "Universal Network (single) 1.82\n",
      "DiffTransfer (single) 3.68\n",
      "Universal Network (single/mixed) 1.69\n",
      "DiffTransfer (single/mixed) 3.78\n",
      "Music-STAR (mixture) 2.89\n",
      "DiffTransfer (mixture) 3.80\n",
      "Table 2 : Objective Evaluation of the proposed DiffTrans-\n",
      "fer Method compared to the baselines, in terms of per-\n",
      "ceived similarity with respect to the ground truth on a Lik-\n",
      "ert scale from 1 (Bad) to 5 (Excellent). Results are aver-\n",
      "aged over all test tracks.\n",
      "single/mixed case, i.e. when single timbre transfer tracks\n",
      "are mixed in order to form the desired mixture audio.\n",
      "For what concerns the Music-STAR method, the gap\n",
      "with respect to DiffTransfer remains high in terms of FAD,\n",
      "but becomes less noticeable when considering JD and the\n",
      "perceived subjective similarity.\n",
      "5. CONCLUSION\n",
      "In this paper, we have presented DiffTransfer a technique\n",
      "for both single- and multi-instrument timbre transfer using\n",
      "Denoising Diffusion Implicit models. The novelty of the\n",
      "proposed approach lies in the fact that in addition to be-\n",
      "ing, to the best of our knowledge, the ﬁrst application of\n",
      "diffusion models to timbre transfer, it is the ﬁrst model to\n",
      "be tested in order to perform single and multi-timbre trans-\n",
      "fer, without varying the architecture depending on which\n",
      "application is chosen. We compared the proposed model\n",
      "with state-of-the-art Universal Network and Music-STAR\n",
      "baselines through both objective evaluation measures and\n",
      "a listening test, demonstrating the better capabilities of the\n",
      "proposed DiffTransfer approach.\n",
      "Future works will involve increasing the audio quality\n",
      "of the generated audio, by taking into account the consis-\n",
      "tency of subsequent generated spectrograms. Furthermore,\n",
      "we plan on modifying the model in order to be able to\n",
      "perform unpaired timbre transfer, which greatly eases the\n",
      "dataset requirements and applicability of the technique.\n",
      "6. REFERENCES\n",
      "[1] J. T. Colonel and S. Keene, “Conditioning autoencoder\n",
      "latent spaces for real-time timbre interpolation and syn-\n",
      "thesis,” in 2020 International Joint Conference on Neu-\n",
      "ral Networks (IJCNN) . IEEE, 2020, pp. 1–7.\n",
      "[2] J. Song, C. Meng, and S. Ermon, “Denoising diffu-\n",
      "sion implicit models,” in International Conference on\n",
      "Learning Representations , 2021.\n",
      "[3] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Sal-\n",
      "imans, D. Fleet, and M. Norouzi, “Palette: Image-to-\n",
      "image diffusion models,” in ACM SIGGRAPH 2022\n",
      "Conference Proceedings , 2022, pp. 1–10.[4] S. Huang, Q. Li, C. Anil, X. Bao,\n",
      "S. Oore, and R. B. Grosse, “Timbretron: A\n",
      "wavenet(cycleGAN(CQT(audio))) pipeline for\n",
      "musical timbre transfer,” in International Conference\n",
      "on Learning Representations , 2019.\n",
      "[5] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired\n",
      "image-to-image translation using cycle-consistent ad-\n",
      "versarial networks,” in Proc. of the IEEE international\n",
      "conference on computer vision , 2017, pp. 2223–2232.\n",
      "[6] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan,\n",
      "O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\n",
      "K. Kavukcuoglu, “Wavenet: A generative model for\n",
      "raw audio,” arXiv preprint arXiv:1609.03499 , 2016.\n",
      "[7] D. K. Jain, A. Kumar, L. Cai, S. Singhal, and V . Ku-\n",
      "mar, “Att: Attention-based timbre transfer,” in 2020\n",
      "International Joint Conference on Neural Networks\n",
      "(IJCNN) . IEEE, 2020, pp. 1–6.\n",
      "[8] K. Kumar, R. Kumar, T. De Boissiere, L. Gestin, W. Z.\n",
      "Teoh, J. Sotelo, A. de Brébisson, Y . Bengio, and A. C.\n",
      "Courville, “Melgan: Generative adversarial networks\n",
      "for conditional waveform synthesis,” Advances in neu-\n",
      "ral information processing systems , vol. 32, 2019.\n",
      "[9] Y .-J. Luo, K. Agres, and D. Herremans, “Learning dis-\n",
      "entangled representations of timbre and pitch for mu-\n",
      "sical instrument sounds using gaussian mixture varia-\n",
      "tional autoencoders,” in 20th International Society for\n",
      "Music Information Retrieval (ISMIR2019) , 2019.\n",
      "[10] J. Engel, L. H. Hantrakul, C. Gu, and A. Roberts,\n",
      "“Ddsp: Differentiable digital signal processing,” in In-\n",
      "ternational Conference on Learning Representations ,\n",
      "2020.\n",
      "[11] M. Michelashvili and L. Wolf, “Hierarchical timbre-\n",
      "painting and articulation generation,” in 21st Inter-\n",
      "national Society for Music Information Retrieval (IS-\n",
      "MIR2020) , 2020.\n",
      "[12] A. P. Noam Mor, Lior Wold and Y . Taigman, “A univer-\n",
      "sal music translation network,” in International Con-\n",
      "ference on Learning Representations (ICLR) , 2019.\n",
      "[13] M. Alinoori and V . Tzerpos, “Music-star: a style trans-\n",
      "lation system for audio-based re-instrumentation,” in\n",
      "21st International Society for Music Information Re-\n",
      "trieval (ISMIR2022) , 2022.\n",
      "[14] J. Engel, C. Resnick, A. Roberts, S. Dieleman,\n",
      "M. Norouzi, D. Eck, and K. Simonyan, “Neural audio\n",
      "synthesis of musical notes with wavenet autoencoders,”\n",
      "inInternational Conference on Machine Learning .\n",
      "PMLR, 2017, pp. 1068–1077.\n",
      "[15] R. J. Williams and D. Zipser, “A learning algorithm for\n",
      "continually running fully recurrent neural networks,”\n",
      "Neural computation , vol. 1, no. 2, pp. 270–280, 1989.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n",
      "262\n",
      "[16] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion\n",
      "probabilistic models,” Advances in Neural Information\n",
      "Processing Systems , vol. 33, pp. 6840–6851, 2020.\n",
      "[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\n",
      "D. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio,\n",
      "“Generative adversarial networks,” Communications of\n",
      "the ACM , vol. 63, no. 11, pp. 139–144, 2020.\n",
      "[18] A. Roberts, J. Engel, and D. Eck, “Hierarchical varia-\n",
      "tional autoencoders for music,” in NIPS Workshop on\n",
      "Machine Learning for Creativity and Design , vol. 3,\n",
      "2017.\n",
      "[19] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang,\n",
      "E. Denton, S. K. S. Ghasemipour, R. Gontijo-Lopes,\n",
      "B. K. Ayan, T. Salimans, J. Ho, D. J. Fleet, and\n",
      "M. Norouzi, “Photorealistic text-to-image diffusion\n",
      "models with deep language understanding,” in Ad-\n",
      "vances in Neural Information Processing Systems ,\n",
      "A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds.,\n",
      "2022.\n",
      "[20] C. Hawthorne, I. Simon, A. Roberts, N. Zeghi-\n",
      "dour, J. Gardner, E. Manilow, and J. Engel, “Multi-\n",
      "instrument music synthesis with spectrogram diffu-\n",
      "sion,” in Ismir 2022 Hybrid Conference , 2022.\n",
      "[21] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y . Zou,\n",
      "and D. Yu, “Diffsound: Discrete diffusion model for\n",
      "text-to-sound generation,” 2022.\n",
      "[22] G. Mittal, J. Engel, C. Hawthorne, and I. Simon, “Sym-\n",
      "bolic music generation with diffusion models,” in Proc.\n",
      "of the 22nd International Society for Music Informa-\n",
      "tion Retrieval Conference , 2021.\n",
      "[23] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catan-\n",
      "zaro, “Diffwave: A versatile diffusion model for au-\n",
      "dio synthesis,” in International Conference on Learn-\n",
      "ing Representations , 2021.\n",
      "[24] G. Plaja-Roglans, M. Miron, and X. Serra, “A\n",
      "diffusion-inspired training strategy for singing voice\n",
      "extraction in the waveform domain,” in International\n",
      "Society for Music Information Retrieval (ISMIR) Con-\n",
      "ference , 2022.\n",
      "[25] G. Zhang, J. Ji, Y . Zhang, M. Yu, T. Jaakkola, and\n",
      "S. Chang, “Towards coherent image inpainting using\n",
      "denoising diffusion implicit models,” arXiv preprint\n",
      "arXiv:2304.03322 , 2023.\n",
      "[26] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and\n",
      "M. Tagliasacchi, “Soundstream: An end-to-end neu-\n",
      "ral audio codec,” IEEE/ACM Transactions on Audio,\n",
      "Speech, and Language Processing , vol. 30, pp. 495–\n",
      "507, 2021.\n",
      "[27] A. Campbell, J. Benton, V . De Bortoli, T. Rainforth,\n",
      "G. Deligiannidis, and A. Doucet, “A continuous time\n",
      "framework for discrete denoising models,” Advances\n",
      "in Neural Information Processing Systems , vol. 35, pp.\n",
      "28 266–28 279, 2022.[28] Y . Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar,\n",
      "S. Ermon, and B. Poole, “Score-based generative mod-\n",
      "eling through stochastic differential equations,” in In-\n",
      "ternational Conference on Learning Representations ,\n",
      "2021.\n",
      "[29] S. Rouard and G. Hadjeres, “Crash: raw audio\n",
      "score-based generative modeling for controllable high-\n",
      "resolution drum sound synthesis,” in 22nd Interna-\n",
      "tional Society for Music Information Retrieval (IS-\n",
      "MIR2021) , 2021.\n",
      "[30] A. Q. Nichol and P. Dhariwal, “Improved denoising\n",
      "diffusion probabilistic models,” in International Con-\n",
      "ference on Machine Learning . PMLR, 2021, pp.\n",
      "8162–8171.\n",
      "[31] M. Alinoori and V . Tzerpos, “Starnet,” Aug. 2022.\n",
      "[Online]. Available: https://doi.org/10.5281/zenodo.\n",
      "6917099\n",
      "[32] I. Loshchilov and F. Hutter, “Decoupled weight decay\n",
      "regularization,” in International Conference on Learn-\n",
      "ing Representations , 2019.\n",
      "[33] K. Kilgour, M. Zuluaga, D. Roblek, and M. Shariﬁ,\n",
      "“Fréchet audio distance: A reference-free metric for\n",
      "evaluating music enhancement algorithms.” in INTER-\n",
      "SPEECH , 2019, pp. 2350–2354.\n",
      "[34] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gem-\n",
      "meke, A. Jansen, R. C. Moore, M. Plakal, D. Platt,\n",
      "R. A. Saurous, B. Seybold et al. , “Cnn architectures\n",
      "for large-scale audio classiﬁcation,” in 2017 ieee in-\n",
      "ternational conference on acoustics, speech and signal\n",
      "processing (icassp) . IEEE, 2017, pp. 131–135.\n",
      "[35] O. Cífka, A. Ozerov, U. ¸ Sim¸ sekli, and G. Richard,\n",
      "“Self-supervised vq-vae for one-shot music style trans-\n",
      "fer,” in ICASSP 2021-2021 IEEE International Con-\n",
      "ference on Acoustics, Speech and Signal Processing\n",
      "(ICASSP) . IEEE, 2021, pp. 96–100.\n",
      "[36] J. Salamon and E. Gómez, “Melody extraction from\n",
      "polyphonic music signals using pitch contour charac-\n",
      "teristics,” IEEE transactions on audio, speech, and lan-\n",
      "guage processing , vol. 20, no. 6, pp. 1759–1770, 2012.\n",
      "[37] D. Bogdanov, N. Wack, E. Gómez Gutiérrez, S. Gulati,\n",
      "H. Boyer, O. Mayor, G. Roma Trepat, J. Salamon, J. R.\n",
      "Zapata González, X. Serra et al. , “Essentia: An au-\n",
      "dio analysis library for music information retrieval,” in\n",
      "Britto A, Gouyon F , Dixon S, editors. 14th Conference\n",
      "of the International Society for Music Information Re-\n",
      "trieval (ISMIR); 2013 Nov 4-8; Curitiba, Brazil.[place\n",
      "unknown]: ISMIR; 2013. p. 493-8. International So-\n",
      "ciety for Music Information Retrieval (ISMIR), 2013.Proceedings of the 24th ISMIR Conference, Milan, Italy, November 5-9, 2023\n",
      "263\n",
      "\n",
      "[['Timbre', 'transfer', 'techniques', 'aim', 'at', 'converting', 'the', 'sound', 'of', 'a', 'musical', 'piece', 'generated', 'by', 'one', 'instrument', 'into', 'the', 'same', 'one', 'as', 'if', 'it', 'was', 'played', 'by', 'another', 'instrument', ',', 'while', 'main-', 'taining', 'as', 'much', 'as', 'possible', 'the', 'content', 'in', 'terms', 'of', 'musical', 'characteristics', 'such', 'as', 'melody', 'and', 'dynamics', '.'], ['Following', 'their', 'recent', 'breakthroughs', 'in', 'deep', 'learning-based', 'gener-', 'ation', ',', 'we', 'apply', 'Denoising', 'Diffusion', 'Models', '(DDMs)', 'to', 'perform', 'timbre', 'transfer', '.'], ['Speciﬁcally', ',', 'we', 'apply', 'the', 'recently', 'proposed', 'Denoising', 'Diffusion', 'Implicit', 'Models', '(DDIMs)', 'that', 'enable', 'to', 'accelerate', 'the', 'sampling', 'procedure', '.'], ['Inspired', 'by', 'the', 'recent', 'application', 'of', 'DDMs', 'to', 'image', 'translation', 'problems', 'we', 'formulate', 'the', 'timbre', 'transfer', 'task', 'similarly', ',', 'by', 'ﬁrst', 'converting', 'the', 'audio', 'tracks', 'into', 'log', 'mel', 'spectro-', 'grams', 'and', 'by', 'conditioning', 'the', 'generation', 'of', 'the', 'desired', 'timbre', 'spectrogram', 'through', 'the', 'input', 'timbre', 'spectrogram', '.'], ['We', 'perform', 'both', 'one-to-one', 'and', 'many-to-many', 'timbre', 'transfer', ',', 'by', 'converting', 'audio', 'waveforms', 'containing', 'only', 'single', 'instruments', 'and', 'multiple', 'instruments', ',', 'respectively', '.'], ['We', 'compare', 'the', 'proposed', 'technique', 'with', 'existing', 'state-of-', 'the-art', 'methods', 'both', 'through', 'listening', 'tests', 'and', 'objective', 'measures', 'in', 'order', 'to', 'demonstrate', 'the', 'effectiveness', 'of', 'the', 'proposed', 'model', '.'], ['1.', 'INTRODUCTION', 'Timbre', 'is', 'an', 'extremely', 'important', 'perceptual', 'aspect', 'of', 'mu-', 'sic', ',', 'yet', 'it', 'is', 'hard', 'to', 'both', 'model', 'and', 'deﬁne', '.'], ['The', 'concept', 'of', 'musical', 'timbre', 'can', 'be', 'deﬁned', 'as', 'the', 'perceived', 'character-', 'istics', 'of', 'a', 'musical', 'sound', 'that', 'are', 'different', 'from', 'pitch', 'and', 'amplitude', 'contours', '[1]', '.'], ['Timbre', 'Transfer', 'concerns', 'the', 'task', 'of', 'converting', 'a', 'musi-', 'cal', 'piece', 'from', 'one', 'timbre', 'to', 'another', 'while', 'preserving', 'the', 'other', 'music-related', 'characteristics', '.'], ['While', 'this', 'operation', 'is', 'not', 'trivial', ',', 'it', 'is', 'of', 'extreme', 'interest', 'for', 'several', 'applica-', 'tions', ',', 'from', 'the', 'development', 'of', 'plugins', 'to', 'be', 'used', 'in', 'Digi-', 'tal', 'Audio', 'Workstations', '(DAW)', 'to', 'enabling', 'the', 'possibility', 'of', 'playing', 'sounds', 'corresponding', 'to', 'not', 'widely', 'available', 'musical', 'instruments', '.'], ['©', 'L', '.'], ['Comanducci', ',', 'F', '.'], ['Antonacci', ',', 'and', 'A', '.'], ['Sarti', '.'], ['Licensed', 'un-', 'der', 'a', 'Creative', 'Commons', 'Attribution', '4.0', 'International', 'License', '(CC', 'BY', '4.0)', '.'], ['Attribution', ':', 'L', '.'], ['Comanducci', ',', 'F', '.'], ['Antonacci', ',', 'and', 'A', '.'], ['Sarti', ',', '“Timbre', 'Transfer', 'using', 'Image-to-Image', 'Denoising', 'Diffusion', 'Implicit', 'Models”', ',', 'in', 'Proc', '.'], ['of', 'the', '24th', 'Int', '.'], ['Society', 'for', 'Music', 'Information', 'Retrieval', 'Conf', '.'], [',', 'Milan', ',', 'Italy', ',', '2023.In', 'this', 'paper', ',', 'we', 'present', 'DiffTransfer', ',', 'a', 'technique', 'for', 'timbre', 'transfer', 'which', 'is', 'tested', 'both', 'between', 'single', 'and', 'multiple', 'instruments', 'and', 'is', 'based', 'on', 'a', 'continuous', 'De-', 'noising', 'Diffusion', 'Implicit', 'Model', '(DDIM)', 'with', 'determin-', 'istic', 'sampling', '[2]', ',', 'a', 'modiﬁed', 'version', 'of', 'Denoising', 'Diffu-', 'sion', 'Probabilistic', 'Models', '(DDPMs)', 'that', 'are', 'trained', 'using', 'the', 'same', 'procedure', ',', 'but', 'allow', 'for', 'faster', 'sampling', 'times', '.'], ['Speciﬁcally', ',', 'in', '[2]', 'it', 'was', 'empirically', 'shown', 'that', 'DDIMs', 'allow', 'for', '10×−50×faster', 'wall-clock', 'time', 'performances', 'with', 'respect', 'to', 'DDPMs', '.'], ['In', 'order', 'to', 'be', 'able', 'to', 'convert', 'one', 'timbre', 'into', 'another', ',', 'we', 'use', 'a', 'procedure', 'similar', 'to', 'the', 'recently', 'proposed', 'image-', 'to-image', 'technique', 'Palette', '[3]', '.'], ['Speciﬁcally', ',', 'we', 'use', 'as', 'in-', 'put', 'to', 'the', 'diffusion', 'model', 'the', 'noise', 'and', 'condition', 'it', 'with', 'the', 'chosen', 'input', 'timbre', 'spectrogram', ',', 'then', ',', 'through', 'the', 'de-', 'noising', 'procedure', ',', 'the', 'model', 'learns', 'to', 'reconstruct', 'spec-', 'trograms', 'of', 'the', 'desired', 'timbre', '.'], ['We', 'consider', 'the', 'scenario', 'where', 'the', 'timbre-transfer', 'task', 'is', 'paired', ',', 'which', 'means', 'that', 'the', 'desired', 'and', 'input', 'spectrograms', 'have', 'the', 'same', 'melodic/harmonic', 'content', ',', 'but', 'differ', 'in', 'terms', 'of', 'timbre', '.'], ['We', 'experiment', 'both', 'with', 'the', 'possibility', 'of', 'convert-', 'ing', 'between', 'tracks', 'containing', 'only', 'single', 'instruments', 'and', 'also', 'mixtures', 'of', 'instruments', ',', 'with', 'no', 'prior', 'separation', 'step', ',', 'while', 'making', 'no', 'modiﬁcations', 'to', 'the', 'model', 'in', 'order', 'to', 'take', 'into', 'account', 'both', 'conﬁgurations', '.'], ['In', 'order', 'to', 'demonstrate', 'the', 'effectiveness', 'of', 'the', 'pro-', 'posed', 'model', ',', 'we', 'compare', 'DiffTransfer', 'with', 'state-of-', 'the-art', 'techniques', ',', 'both', 'through', 'objective', 'measures', 'and', 'by', 'performing', 'a', 'user-based', 'listening', 'test', '.'], ['The', 'source', 'code', 'and', 'audio', 'excerpts', 'can', 'be', 'found', 'at', 'https', ':', '//', 'lucacoma', '.'], ['github', '.'], ['io/DiffTransfer/', '.'], ['2.', 'RELATED', 'WORK', 'Several', 'types', 'of', 'timbre', 'Transfer', 'techniques', 'have', 'been', 'pro-', 'posed', 'in', 'the', 'literature', '.'], ['In', '[4]', 'a', 'CycleGAN', '[5]', 'is', 'applied', 'in', 'order', 'to', 'perform', 'an', 'unpaired', 'transfer', 'using', 'the', 'Constant-', 'Q', 'transform', 'and', 'the', 'audio', 'is', 'then', 'recovered', 'through', 'a', 'WaveNet', '[6]', 'model', '.'], ['In', '[7]', 'an', 'attention-based', 'architecture', 'is', 'applied', 'in', 'order', 'to', 'convert', 'mel', 'spectrograms', ',', 'which', 'are', 'then', 'inverted', 'through', 'a', 'MelGAN', 'architecture', '[8]', '.'], ['Gaus-', 'sian', 'mixture-based', 'variational', 'autoencoders', 'are', 'applied', '[9]', 'in', 'order', 'to', 'learn', 'a', 'latent', 'space', 'where', 'pitch', 'and', 'timbre', 'rep-', 'resentations', 'are', 'disentangled', '.'], ['Another', 'class', 'of', 'methods', ',', 'instead', ',', 'extracts', 'musical', 'pa-', 'rameters', 'such', 'as', 'pitch', 'and', 'loudness', 'from', 'the', 'input', 'audio', 'tracks', 'and', 'performs', 'the', 'transfer', 'by', 'resynthesizing', 'sound257', 'Diffusion', 'Decoder', 'Concatenate', 'L1', 'LossConditioning', 'Instrument', 'Sinusoidal', 'Embedding', 'Diffusion', 'NoisePredicted', 'Noise', '.'], ['\"##$%\"&\\'', 'Target', 'Instrument', 'Figure', '1', ':', 'Training', 'scheme', 'of', 'the', 'proposed', 'DiffTransfer', 'technique', '.'], ['The', 'target', 'instrument', 'spectrogram', 'is', 'summed', 'with', 'noise', 'following', 'a', 'simpliﬁed', 'cosine', 'schedule', '.'], ['The', 'decoder', ',', 'conditioned', 'on', 'the', 'conditioning', 'instrument', 'spectrogram', 'and', 'on', 'the', 'sinusoidal', 'embedding', 'representing', 'the', 'current', 'time', 'instant', 'estimates', 'the', 'added', 'noise', '.'], ['The', 'decoder', 'parameters', 'are', 'estimated', 'by', 'computing', 'the', 'L1', 'loss', 'between', 'the', 'ground', 'truth', 'and', 'the', 'estimated', 'diffusion', 'noise', '.'], ['through', 'a', 'network', 'that', 'has', 'learned', 'to', 'generate', 'tracks', 'with', 'the', 'desired', 'timbre', '.'], ['The', 'most', 'known', 'example', 'of', 'these', 'techniques', 'is', 'the', 'Differentiable', 'Digital', 'Signal', 'Processing', '(DDSP)', '[10]', 'model', '.'], ['Other', 'similar', 'techniques', 'were', 'pro-', 'posed', 'such', 'as', '[11]', ',', 'where', 'a', 'hierarchical', 'model', 'is', 'used', 'in', 'order', 'to', 'reconstruct', 'the', 'signal', 'at', 'increasing', 'resolu-', 'tions', '.'], ['Recently', 'there', 'have', 'been', 'proposed', 'also', 'models', 'that', 'directly', 'work', 'on', 'the', 'audio', 'waveform', 'such', 'as', '[12]', ',', 'where', 'music', 'pieces', 'are', 'translated', 'to', 'speciﬁc', 'timbre', 'do-', 'mains', '.'], ['The', 'only', 'model', 'that', ',', 'to', 'the', 'best', 'of', 'our', 'knowl-', 'edge', 'and', 'except', 'for', 'the', 'one', 'proposed', 'in', 'this', 'paper', ',', 'is', 'tested', 'on', 'multi-instrument', 'timbre', 'transfer', 'without', 'any', 'source', 'separation', 'pre-processing', 'is', 'the', 'Music-STAR', 'net-', 'work', ',', 'presented', 'in', '[13]', '.'], ['In', 'Music-STAR', 'a', 'WaveNet', 'au-', 'toencoder', '[14]', 'is', 'trained', 'by', 'applying', 'teacher-forcing', '[15]', 'to', 'the', 'decoders', 'in', 'order', 'to', 'recover', 'the', 'desired', 'timbre', '.'], ['Denoising', 'Diffusion', 'Probabilistic', 'Models', '(DDPMs)', '[16]', 'have', 'recently', 'become', 'the', 'latest', 'state-of-the-art', 'for', 'what', 'concerns', 'deep', 'learning-based', 'generation', 'fastly', 're-', 'placing', 'Generative', 'Adversarial', 'Networks', '(GANs)', '[17]', 'and', 'Variational', 'Autoencoders', '[18]', ',', 'due', 'to', 'their', 'easier', 'training', 'procedure', 'and', 'increased', 'quality', 'of', 'the', 'produced', 'results', '.'], ['DDPMs', 'have', 'been', 'successfully', 'applied', 'to', 'a', 'wide', 'va-', 'riety', 'of', 'image-related', 'tasks', 'such', 'as', 'generation', '[19]', 'and', 'translation', '[3]', '.'], ['More', 'recently', ',', 'DDPMs', 'have', 'been', 'also', 'used', 'for', 'audio-', 'related', 'tasks', '.'], ['In', '[20]', 'a', 'diffusion', 'model', 'is', 'applied', 'in', 'order', 'to', 'convert', 'midi', 'tracks', 'to', 'spectrograms', ',', 'while', 'in', '[21]', 'a', 'text-', 'to-music', 'diffusion', 'model', 'is', 'proposed', '.'], ['DDPMs', 'have', 'also', 'been', 'applied', 'to', 'symbolic', 'music', 'generation', '[22]', ',', 'speech', 'synthesis', '[23]', 'and', 'singing', 'voice', 'extraction', '[24]', '.'], ['While', 'DDPMs', 'have', 'extremely', 'powerful', 'generation', 'ca-', 'pabilities', 'they', 'suffer', 'from', 'slow', 'sampling', 'times', '.'], ['To', 'amelio-rate', 'this', 'issue', ',', 'recently', 'Denoising', 'Diffusion', 'Implicit', 'Mod-', 'els', '(DDIMs)', '[2]', ',', 'which', 'allow', 'for', 'faster', 'sampling', 'times', 'and', 'were', 'recently', 'applied', 'to', 'image', 'inpainting', '[25]', '.'], ['3.', 'PROPOSED', 'MODEL', 'In', 'this', 'section', ',', 'we', 'describe', 'the', 'proposed', 'DiffTransfer', 'tech-', 'nique', 'for', 'timbre', 'transfer', '.'], ['Instead', 'of', 'working', 'directly', 'with', 'raw', 'audio', 'signals', ',', 'we', 'convert', 'them', 'into', 'log', 'mel-scaled', 'spectrograms', ',', 'due', 'to', 'their', 'easier', 'handling', 'by', 'deep', 'learn-', 'ing', 'models', '.'], ['We', 'then', 'propose', 'a', 'model', 'that', ',', 'given', 'as', 'input', 'the', 'spectrogram', 'corresponding', 'to', 'the', 'conditioning', 'instru-', 'ment', ',', 'generates', 'the', 'corresponding', 'target', 'spectrogram', 'that', 'would', 'have', 'been', 'obtained', 'by', 'playing', 'the', 'same', 'piece', 'of', 'music', 'with', 'the', 'target', 'instrument', '.'], ['Operatively', 'we', 'achieve', 'this', 'through', 'a', 'conditional', 'continuous-time', 'DDIM', ',', 'which', 'learns', 'to', 'denoise', 'the', 'target', 'instrument', 'spectrogram', ',', 'while', 'conditioned', 'on', 'the', 'input', 'instrument', 'spectrogram', ',', 'as', 'de-', 'picted', 'in', 'Fig', '.'], ['1.', 'At', 'inference', 'time', ',', 'the', 'model', 'is', 'fed', 'with', 'the', 'input', 'conditioning', 'instrument', 'concatenated', 'with', 'Gaus-', 'sian', 'noise', 'and', 'generates', 'the', 'corresponding', 'target', 'spectro-', 'gram', '.'], ['We', 'retrieve', 'the', 'audio', 'signal', 'by', 'applying', 'to', 'the', 'log', 'mel', 'spectrograms', 'the', 'SoundStream1model', '[26]', ',', 'provided', 'by', '[20]', 'where', 'it', 'was', 'trained', 'on', 'a', 'custom', 'music', 'dataset', '.'], ['In', 'the', 'following', ',', 'we’ll', 'provide', 'a', 'brief', 'overview', 'of', 'the', 'DDIM', 'framework', 'and', 'notation', 'used', 'in', 'this', 'paper', ',', 'in', 'order', 'to', 'make', 'the', 'tractation', 'as', 'compact', 'as', 'possible', ',', 'for', 'addi-', 'tional', 'and', 'more', 'thorough', 'formulations', ',', 'we', 'refer', 'the', 'reader', 'to', '[2]', 'and', '[3]', '.'], ['We', 'aim', 'at', 'giving', 'a', 'general', 'overview', 'of', 'the', 'process', 'and', 'we’ll', 'use', 'a', 'slight', 'abuse', 'of', 'notation', 'to', 'describe', 'the', 'diffusion', 'process', 'using', 'the', 'continuous', 'time', 'framework', ',', '1https', ':', '//tfhub', '.'], ['dev/google/soundstream/mel/', 'decoder/music/1Proceedings', 'of', 'the', '24th', 'ISMIR', 'Conference', ',', 'Milan', ',', 'Italy', ',', 'November', '5-9', ',', '2023', '258', 'in', 'order', 'to', 'make', 'it', 'more', 'similar', 'to', 'the', 'more', 'common', 'liter-', 'ature', 'regarding', 'DDPMs', 'and', 'DDIMs', '.'], ['3.1', 'Diffusion', 'Decoder', 'We', 'adopt', 'a', 'procedure', 'similar', 'to', 'the', 'Palette', '[3]', 'image-to-', 'image', 'translation', 'technique', 'in', 'order', 'to', 'train', 'the', 'timbre', 'transfer', 'decoder', 'as', 'a', 'Denoising', 'Diffusion', 'Implicit', 'Model', '(DDIM)', '[2]', '.'], ['Broadly', 'speaking', ',', 'DDIMs', 'work', 'by', 'learn-', 'ing', 'how', 'to', 'generate', 'data', 'from', 'noise', 'in', 'a', 'two-part', 'pro-', 'cedure', '.'], ['The', 'ﬁrst', 'part', 'is', 'denoted', 'as', 'the', 'forward', 'process', ',', 'where', 'Gaussian', 'noise', 'γ∼', 'N(0', ',', '1)is', 'subsequently', 'added', 'to', 'the', 'input', 'until', 'it', 'is', 'indistinguishable', 'from', 'the', 'former', '.'], ['The', 'second', 'part', 'consists', 'of', 'the', 'reverse', 'process', 'where', 'a', 'de-', 'coder', 'learns', 'how', 'to', 'invert', 'the', 'forward', 'process', ',', 'effectively', 'reconstructing', 'data', 'from', 'the', 'noise', '.'], ['DDIMs', 'can', 'be', 'seen', 'as', 'a', 'generalization', 'of', 'DDPMs', 'that', 'shares', 'the', 'same', 'training', 'procedure', ',', 'however', ',', 'they', 'differ', 'in', 'the', 'modeling', 'of', 'the', 're-', 'verse', 'process', ',', 'by', 'using', 'a', 'non-markovian', 'diffusion', 'process', ',', 'which', 'allows', 'for', 'faster', 'generation', 'times', '.'], ['3.1.1', 'Forward', 'Process', 'Let', 'us', 'deﬁne', 'XandYas', 'the', 'log', 'mel', 'spectrograms', 'cor-', 'responding', 'to', 'the', 'conditioning', 'and', 'target', 'instruments', ',', 're-', 'spectively', '.'], ['We', 'choose', 'a', 'continuous', 'diffusion', 'time', '[27–', '29]in', 'order', 'to', 'be', 'able', 'to', 'change', 'the', 'number', 'of', 'desired', 'sampling', 'steps', '.'], ['If', 'we', 'consider', 'Tsteps', ',', 'then', 'the', 'diffusion', 'time', 'can', 'be', 'deﬁned', 'as', 't∈', '{0', ',', '1}', ',', 'where', 'consecutive', 'times', 'are', 'separated', 'by', '∆t=', '1/T', '.'], ['Then', ',', 'the', 'forward', 'process', 'is', 'deﬁned', 'similarly', 'to', 'the', 'case', 'of', 'DDPMs', 'by', 'subsequently', 'adding', 'noise', 'to', 'the', 'target', 'spectrogram', 'for', 'Tsteps', 'q(Yt|Yt−∆t)', '=N(Yt', ',', '/radicalbig', '(αt)Yt−∆t', ',', 'βtI)', ',', 'q(Y1', ':', 'T|Y0)', '=T/productdisplay', 't=1q(Yt−∆t)(1)', 'whereαandβare', 'parameters', 'deﬁned', 'by', 'a', 'simpliﬁed', 'co-', 'sine', 'schedule', '[30]', '.'], ['3.1.2', 'Reverse', 'Process', 'In', 'the', 'case', 'of', 'DDIMs', ',', 'the', 'reverse', 'diffusion', 'process', 'is', 'op-', 'erated', 'by', 'introducing', 'an', 'additional', 'distribution', 'pθ', ',', 'where', 'a', 'sample', 'Yt−∆tcan', 'be', 'generated', 'from', 'a', 'sample', 'Ytas', 'Yt−∆t=/radicalbig', 'βt−∆t/parenleftBigg', 'c−√βtγ(t)', 'θ(Yt', ',', 'X)/radicalbig', '(αt)/parenrightBigg', '+', '/radicalbig', '1−αt−∆t·γ(t)', 'θ(Yt', ',', 'X)', ',', '(2)', ',', 'whereγis', 'the', 'noise', 'estimated', 'by', 'a', 'network', 'with', 'param-', 'etersθ', '.'], ['The', 'noise', 'at', 'time', 't', 'γ(t)', 'θis', 'estimated', 'by', 'a', 'network', 'that', 'is', 'conditioned', 'also', 'on', 'the', 'input', 'timbre', 'spectrogram', 'X', ',', 'similarly', 'to', 'the', 'formulation', 'proposed', 'in', 'Palette', '[3]', '.'], ['3.1.3', 'Training', 'Procedure', 'The', 'denoising', 'process', 'is', 'operated', 'through', 'a', 'U-Net', 'archi-', 'tecture', 'which', 'is', 'conditioned', 'on', 'Xand', 'trained', 'to', 'predict', 'the', 'added', 'noise', 'in', 'order', 'to', 'minimize', 'the', 'L1', 'loss', 'E=||γ(t)', 'θ(Yt', ',', 'X)−γ||1', '1', ',', '(3)whereγis', 'the', 'true', 'perturbation', ',', 'while', 'γ(t)', 'θ(Yt', ',', 'X)is', 'the', 'estimate', 'of', 'the', 'noise', 'added', 'to', 'the', 'target', 'spectrogram', 'at', 'timet', ',', 'conditioned', 'on', 'the', 'input', 'spectrogram', 'X', '.'], ['3.2', 'Architecture', 'The', 'decoder', 'architecture', 'is', 'based', 'on', 'a', 'U-Net', 'model', '.'], ['The', 'building', 'element', 'is', 'made', 'of', 'residual', 'blocks', ',', 'in', 'each', 'of', 'these', 'the', 'input', 'is', 'processed', 'by', '(i)', 'a', '2D', 'convolutional', 'layer', 'with', 'swish', 'activation', ',', 'followed', 'by', 'batch', 'normalization', 'and', 'by', '(ii)', 'a', 'convolutional', 'layer', 'with', 'no', 'activation', '.'], ['Both', 'con-', 'volutional', 'layers', 'have', 'kernel', 'size', '3.', 'The', 'output', 'of', 'this', 'procedure', 'is', 'then', 'summed', 'with', 'the', 'residual', ',', 'which', 'is', 'ob-', 'tained', 'by', 'processing', 'the', 'input', 'with', 'a', 'convolutional', 'layer', 'with', 'kernel', 'size', '1.', 'The', 'encoder', 'part', 'of', 'the', 'network', 'consists', 'of', '3downsam-', 'pling', 'blocks', ',', 'each', 'consisting', 'of', '4residual', 'blocks', 'having', 'ﬁlter', 'sizes', '64', ',', '128', ',', '256.', 'The', 'output', 'of', 'each', 'downsampling', 'block', 'is', 'followed', 'by', 'average', 'pooling', ',', 'with', 'pool', 'size', '2in', 'order', 'to', 'compress', 'the', 'dimension', 'of', 'the', 'spectrograms', '.'], ['The', 'last', 'block', 'of', 'the', 'encoder', 'is', 'followed', 'a', 'self-attention', 'block', '.'], ['The', 'bottleneck', 'obtained', 'through', 'the', 'encoder', 'is', 'pro-', 'cessed', 'by', 'a', 'residual', 'block', 'with', '512ﬁlters', 'and', 'is', 'then', 'pro-', 'cessed', 'by', 'the', 'decoder', ',', 'which', 'is', 'a', 'specular', 'version', 'of', 'the', 'encoder', '.'], ['The', 'only', 'difference', 'lies', 'in', 'the', 'use', 'of', 'transposed', 'convolutions', 'in', 'order', 'to', 'create', 'upsampling', 'layers', 'needed', 'to', 'increase', 'the', 'dimension', 'of', 'the', 'features', '.'], ['The', 'last', 'downsampling', 'layer', 'of', 'the', 'encoder', ',', 'the', 'bot-', 'tleneck', 'and', 'the', 'ﬁrst', 'upsampling', 'layer', 'of', 'the', 'decoder', 'are', 'followed', 'by', 'self-attention', '.'], ['3.3', 'Deployment', 'The', 'proposed', 'model', 'takes', 'as', 'input', 'spectrograms', 'of', 'a', 'ﬁxed', 'size', ',', 'therefore', 'audio', 'tracks', 'longer', 'than', 'the', 'ones', 'used', 'for', 'training', 'need', 'to', 'be', 'sliced', 'accordingly', '.'], ['The', 'decoder', 'takes', 'as', 'input', 'the', 'conditioning', 'spectro-', 'gramXand', 'the', 'diffusion', 'noise', 'and', 'retrieves', 'an', 'estimate', 'of', 'the', 'latter', ',', 'which', 'can', 'then', 'be', 'subtracted', 'in', 'order', 'to', 'obtain', 'an', 'estimate', 'of', 'the', 'desired', 'output', 'timbre', 'spectrogram', 'ˆY', '.'], ['The', 'output', 'waveform', 'ycan', 'then', 'be', 'obtained', 'by', 'feeding', 'the', 'pre-trained', 'SoundStream', 'model', 'with', 'ˆY', '.'], ['4.', 'EXPERIMENTS', 'In', 'this', 'section', ',', 'we', 'describe', 'experiments', 'performed', 'with', 'the', 'aim', 'of', 'demonstrating', 'the', 'capabilities', 'of', 'the', 'proposed', 'DiffTransfer', 'technique', 'both', 'in', 'the', 'single-instrument', 'and', 'multi-instrument', 'application', 'scenarios', '.'], ['In', 'Fig', '.'], ['3', 'we', 'show', 'an', 'example', 'of', 'input', ',', 'generated', 'and', 'ground-truth', 'spectrograms', ',', 'obtained', 'via', 'the', 'DiffTransfer', 'model', 'when', 'converting', 'from', 'a', 'Clarinet', 'to', 'Strings', '.'], ['4.1', 'Dataset', 'In', 'order', 'to', 'train', 'the', 'model', 'we', 'considered', 'the', 'StarNet', 'dataset', '[31]', ',', 'which', 'contains', 'a', 'set', 'of', 'tracks', 'that', 'are', 'played', 'with', 'two', 'timbre-domains', ',', 'namely', 'strings-piano', 'and', 'vibraphone-clarinet', '.'], ['The', 'dataset', 'consists', 'of', 'roughly', '22', 'hours', 'of', 'audio', '.'], ['We', 'used', 'the', 'reduced', 'version', 'of', 'the', 'dataset', ',', 'Proceedings', 'of', 'the', '24th', 'ISMIR', 'Conference', ',', 'Milan', ',', 'Italy', ',', 'November', '5-9', ',', '2023', '259', 'Diffusion', 'Decoder', 'Concatenate', 'Conditioning', 'instrument', 'Predicted', 'Noise', 'Diffusion', 'Noise', 'Predicted', 'Target', 'InstrumentPredicted', 'Target', 'Instrument', 'Audio', 'SoundStream', 'Figure', '2', ':', 'Deployment', 'scheme', 'of', 'the', 'proposed', 'DiffTransfer', 'technique', '.'], ['The', 'decoder', 'is', 'fed', 'with', 'Gaussian', 'noise', 'and', 'with', 'the', 'conditioning', 'instrument', 'spectrogram', '.'], ['The', 'noise', 'estimate', 'provided', 'by', 'the', 'decoder', 'is', 'then', 'subtracted', 'from', 'the', 'input', 'noise', 'in', 'order', 'to', 'provide', 'an', 'estimate', 'of', 'the', 'desired', 'target', 'spectrogram', ',', 'from', 'which', 'the', 'audio', 'is', 'estimated', 'via', 'the', 'SoundStream', 'model', '[20', ',', '26]', '.'], ['where', 'tracks', 'are', 'resampled', 'to', '16000', 'Hz', 'and', 'converted', 'them', 'to', 'mono', '.'], ['In', 'order', 'to', 'perform', 'the', 'evaluation', ',', 'we', 'use', 'the', 'same', 'ten', 'tracks', 'considered', 'in', '[13]', ',', 'in', 'order', 'to', 'ease', 'the', 'comparison', 'with', 'their', 'model', '.'], ['4.2', 'Techniques', 'Under', 'Comparison', 'We', 'consider', 'two', 'baselines', 'in', 'order', 'to', 'compare', 'the', 'per-', 'formances', 'of', 'the', 'proposed', 'DiffTransfer', 'architecture', '.'], ['For', 'what', 'concerns', 'the', 'single-instrument', 'timbre', 'transfer', 'task', ',', 'we', 'consider', 'the', 'Universal', 'Network', '[12]', 'ﬁne-tuned', 'on', 'the', 'StarNet', 'dataset', 'as', 'done', 'in', '[13]', '.'], ['For', 'what', 'concerns', 'the', 'multi-timbre', 'task', ',', 'we', 'consider', 'the', 'mixture-supervised', 'ver-', 'sion', 'of', 'the', 'Music-STAR', 'network', 'proposed', 'in', '[13]', '.'], ['We', 'perform', 'three', 'different', 'types', 'of', 'timbre', 'transfer', 'tasks', ':', 'sin-', 'gle', ',', 'where', 'only', 'single', 'instruments', 'are', 'converted', ',', 'sin-', 'gle/mixed', 'where', 'the', 'separate', 'conversions', 'of', 'single', 'instru-', 'ments', 'are', 'mixed', 'in', 'order', 'to', 'create', 'the', 'desired', 'mixture', 'track', 'and', 'mixture', ',', 'where', 'the', 'mixture', 'is', 'directly', 'converted', '.'], ['These', 'nomenclatures', 'are', 'used', 'just', 'to', 'ease', 'the', 'presentation', 'of', 'the', 'results', ',', 'we', 'would', 'like', 'to', 'point', 'out', 'that', ',', 'for', 'what', 'con-', 'cerns', 'the', 'DiffTransfer', 'architecture', ',', 'no', 'speciﬁc', 'changes', 'are', 'required', 'for', 'the', 'various', 'types', 'of', 'applications', ',', 'except', 'for', 'the', 'choice', 'of', 'desired', 'input', 'data', '.'], ['4.3', 'Experiment', 'Setup', 'The', 'Universal', 'Network', 'and', 'Music-STAR', 'architectures', 'are', 'trained', 'with', 'the', 'procedure', 'described', 'in', '[13]', '.'], ['The', 'Diff-', 'Transfer', 'network', 'is', 'trained', 'for', '5000', 'epochs', 'using', 'a', 'batch', 'size', 'of16', ',', 'with', 'the', 'AdamW', 'optimizer', '[32]', 'with', 'learning', 'rate2e−5and', 'weight', 'decay', '1e−4.', 'The', 'epoch', 'that', 'min-', 'imizes', 'the', 'L1noise', 'prediction', 'loss', 'is', 'chosen', 'in', 'order', 'to', 'retain', 'the', 'model', 'used', 'to', 'compute', 'the', 'results', '.'], ['We', 'train', 'a', 'total', 'of', 'six', 'models', ',', 'performing', 'the', 'following', 'timbre', 'trans-', 'fer', 'conversions', ':', 'vibraphone', 'to', 'piano', ',', 'piano', 'to', 'vibraphone', ',', 'clarinet', 'to', 'strings', ',', 'strings', 'to', 'clarinet', 'vibraphone/clarinet', 'to', 'piano/strings', 'and', 'piano/strings', 'to', 'vibraphone/clarinet', '.'], ['The', 'network', 'input', 'features', 'are', 'computed', 'by', 'ﬁrst', 'apply-', 'ing', 'the', 'Short-Time', 'Fourier', 'Transform', '(STFT)', 'with', 'a', 'Hann', 'window', 'of', 'size', '0.020', 's', 'and50%', 'overlap', 'to', 'normalized', 'audio', 'tracks', '.'], ['Then', 'the', 'log', 'mel', 'spectrogram', 'is', 'computed', 'over128bins', 'corresponding', 'to', 'the', 'range', 'of', '0−16000Hz', '.'], ['We', 'do', 'not', 'feed', 'the', 'entire', 'audio', 'tracks', 'as', 'input', 'to', 'the', 'net-', 'work', ',', 'instead', ',', 'during', 'each', 'epoch', 'we', 'extract', '128frames', 'from', 'the', 'log', 'mel', 'spectrogram', ',', 'corresponding', 'to', '≈2', 's', '.'], ['Each', 'spectrogram', 'slice', 'is', 'normalized', 'between', '−1and1', 'before', 'being', 'given', 'as', 'input', 'to', 'the', 'network', 'and', 'the', 'out-', 'put', 'spectrograms', 'are', 'denormalized', 'before', 'being', 'fed', 'to', 'the', 'SoundStream', 'model', 'in', 'order', 'to', 'recover', 'the', 'audio', 'wave-', 'form', '.'], ['Since', 'the', 'tracks', 'considered', 'for', 'the', 'test', 'are', 'of', 'length', '10sand', 'the', 'model', 'gets', 'as', 'input', 'a', 'ﬁxed', '128frames', 'spectro-', 'gram', 'we', 'slice', 'the', 'conditioning', 'spectrogram', 'before', 'feed-', 'ing', 'into', 'the', 'model', 'and', 'we', 'keep', 'the', 'input', 'noise', 'ﬁxed', 'for', 'all', 'slices', ',', 'in', 'order', 'to', 'ensure', 'consistency', 'in', 'the', 'generation', '.'], ['All', 'spectrogram', 'slices', 'are', 'normalized', 'in', 'the', 'range', '[−1', ',', '1]', 'and', 'denormalized', 'before', 'being', 'fed', 'to', 'the', 'SoundStream', 'de-', 'coder', '.'], ['4.4', 'Objective', 'Evaluation', 'We', 'evaluate', 'the', 'model', 'objectively', 'in', 'order', 'to', 'analyze', 'the', 'perceptual', 'similarity', 'and', 'content', 'preservation', 'capabilities', 'of', 'the', 'generated', 'tracks', 'with', 'respect', 'to', 'the', 'ground', 'truth', 'au-', 'dio', '.'], ['In', 'order', 'to', 'evaluate', 'the', 'perceptual', 'similarity', ',', 'we', 'com-', 'pute', 'the', 'Fréchet', 'Audio', 'Distance', '(FAD)', '[33]', 'using', 'the', 'VG-', 'Gish', 'embeddings', '[34]', ',', 'through', 'a', 'PyTorch', 'implementa-', 'tion2.', 'FAD', 'is', 'a', 'reference-free', 'metric', 'for', 'music', 'enhance-', '2https', ':', '//pypi', '.'], ['org/project/', 'frechet-audio-distance/Proceedings', 'of', 'the', '24th', 'ISMIR', 'Conference', ',', 'Milan', ',', 'Italy', ',', 'November', '5-9', ',', '2023', '260', '0', '2', '4', '6', '8', '10', 'Time', '[s]012345678Frequency', '[kHz]', '(a)', '0', '2', '4', '6', '8', '10', 'Time', '[s]012345678Frequency', '[kHz]', '(b)', '0', '2', '4', '6', '8', '10', 'Time', '[s]012345678Frequency', '[kHz]', '(c)', 'Figure', '3', ':', 'Example', 'of', 'Timbre', 'Conversion', 'log', 'mel', 'Spectro-', 'grams', 'using', 'the', 'DiffTransfer', 'architecture', ',', 'obtained', 'when', 'converting', 'Clarinet', '(a)', 'to', 'Strings', '(b)', '.'], ['The', 'ground', 'truth', 'Strings', 'spectrogram', 'is', 'shown', 'in', '(c)', '.'], ['ment', 'algorithms', ',', 'which', 'views', 'the', 'embeddings', 'as', 'a', 'conti-', 'nous', 'multivariate', 'Gaussian', 'and', 'is', 'computed', 'between', 'the', 'real', 'and', 'generated', 'data', 'as', 'FAD', '=||µr−µg||2+tr(Σ', 'r+µg−2/radicalbig', 'ΣrΣg)', ',', '(4)', 'where(µr', ',', 'Σr)and(µg', ',', 'Σg)are', 'the', 'mean', 'and', 'covariances', 'of', 'the', 'embeddings', 'corresponding', 'to', 'the', 'real', 'and', 'generated', 'data', ',', 'respectively', '.'], ['Similarly', 'to', '[20]', ',', 'we', 'compute', 'FAD', 'in', 'order', 'to', 'analyze', 'the', 'perceptual', 'similarity', 'between', 'the', 'gen-', 'erated', 'audios', 'with', 'respect', 'to', 'the', 'ground', 'truth', 'one', ',', 'corre-', 'sponding', 'to', 'the', 'original', 'StarNet', 'dataset', '.'], ['To', 'understand', 'the', 'content-preservation', 'capabilities', 'of', 'the', 'model', ',', 'following', '[35]', ',', 'we', 'compute', 'how', 'the', 'pitch', 'con-', 'tours', 'of', 'generated', 'ground', 'truth', 'audio', 'tracks', 'are', 'dissimilar', ',', 'by', 'calculating', 'the', 'mismatch', 'between', 'two', 'sets', 'of', 'pitches', 'A', 'andBthrough', 'the', 'Jaccard', 'Distance', 'JD(A', ',', 'B)', '=', '1−|A∩B|', '|A∪B|', ',', '(5)', 'where', 'a', 'lower', 'value', 'corresponds', 'to', 'a', 'lower', 'mismatch', 'and', 'thus', 'to', 'a', 'higher', 'degree', 'of', 'similarity', 'between', 'the', 'gener-', 'ated', 'pitch', 'contours', '.'], ['Pitch', 'contours', 'are', 'computed', 'using', 'a', 'multi-pitch', 'version', 'of', 'the', 'MELODIA', '[36]', 'as', 'implemented', 'in', 'the', 'Essentia', 'library', '[37]', ',', 'rounding', 'pitches', 'to', 'the', 'nearest', 'semitone', '.'], ['We', 'report', 'the', 'values', 'obtained', 'by', 'computing', 'the', 'metrics', 'on', 'the', 'test', 'dataset', 'in', 'Table', '1.Objective', 'Evaluation', 'Method', 'FAD↓JD↓', 'Universal', 'Network', '(single)', '7.09', '0.53', 'DiffTransfer', '(single)', '2.58', '0.28', 'Universal', 'Network', '(single/mixed)', '10.47', '0.64', 'DiffTransfer', '(single/mixed)', '4.73', '0.46', 'Music-STAR', '(mixture)', '8.93', '0.57', 'DiffTransfer', '(mixture)', '4.37', '0.38', 'Table', '1', ':', 'Objective', 'Evaluation', 'of', 'the', 'proposed', 'DiffTrans-', 'fer', 'Method', 'compared', 'to', 'the', 'baselines', ',', 'in', 'terms', 'of', 'Fréchet', 'Audio', 'Distance', '(FAD)', 'and', 'Jaccard', 'Distance', '(JD)', '.'], ['Results', 'are', 'averaged', 'over', 'all', 'participants', 'and', 'over', 'all', 'the', 'tracks', 'considered', 'for', 'each', 'part', 'of', 'the', 'test', '.'], ['4.5', 'Subjective', 'Evaluation', 'In', 'order', 'to', 'evaluate', 'subjectively', 'the', 'timbre', 'transfer', 'capa-', 'bilities', ',', 'we', 'perform', 'a', 'listening', 'test', 'with', '18', 'human', 'partici-', 'pants', '.'], ['The', 'web', 'page', 'of', 'the', 'test', 'is', 'available', 'at3.', 'The', 'test', 'was', 'split', 'into', 'two', 'parts', 'corresponding', 'to', 'the', 'single', 'and', 'multiple', 'instrument', 'application', 'scenarios', ',', 'respectively', '.'], ['During', 'the', 'single', 'instrument', 'part', 'of', 'the', 'test', ',', 'the', 'users', 'listened', 'to', 'four', 'tracks', ',', 'corresponding', 'to', 'the', 'four', 'types', 'of', 'conversions', 'performed', ',', 'namely', ':', 'clarinet', 'to', 'strings', ',', 'strings', 'to', 'clarinet', ',', 'piano', 'to', 'vibraphone', ',', 'vibraphone', 'to', 'piano', '.'], ['Each', 'example', 'consisted', 'of', 'two', 'conditions', ',', 'one', 'obtained', 'via', 'the', 'DiffTransfer', 'model', 'and', 'the', 'other', 'through', 'the', 'Universal', 'Network', '.'], ['In', 'the', 'second', 'part', 'of', 'the', 'test', ',', 'concerning', 'multiple', 'in-', 'strument', 'timbre', 'transfer', ',', 'a', 'total', 'of', 'four', 'tracks', 'were', 'consid-', 'ered', ',', 'two', 'for', 'the', 'conversion', 'from', 'vibraphone/strings', 'to', 'pi-', 'ano/strings', 'waveforms', 'and', 'two', 'for', 'the', 'reverse', 'conversion', '.'], ['Each', 'example', 'consisted', 'of', 'four', 'conditions', ',', 'namely', 'DiffS-', 'tar', '(single/mix)', ',', 'Universal', 'Network', '(single/mix)', ',', 'DiffStar', '(mixture)', 'and', 'Music-STAR', '(mixture)', '.'], ['Both', 'the', 'order', 'of', 'conditions', 'and', 'the', 'order', 'of', 'examples', 'in', 'each', 'separate', 'part', 'of', 'the', 'test', 'were', 'randomized', '.'], ['The', 'participants', 'were', 'asked', 'to', 'rate', 'the', 'conditions', 'in', 'terms', 'of', 'similarity', 'with', 'respect', 'to', 'the', 'reference', 'track', 'on', 'a', '5', 'elements', 'Likert', 'scale', 'where', '1corresponds', 'to', 'bad', 'and', '5to', 'excellent', '.'], ['We', 'report', 'the', 'results', 'obtained', 'through', 'the', 'listening', 'test', 'in', 'Table', '2.', '4.6', 'Discussion', 'By', 'brieﬂy', 'inspecting', 'both', 'the', 'objective', 'and', 'subjective', 're-', 'sults', ',', 'reported', 'in', 'Table', '1', 'and', '2', ',', 'respectively', ',', 'it', 'is', 'clear', 'how', 'the', 'proposed', 'DiffTransfer', 'model', 'outperforms', 'the', 'Univer-', 'sal', 'Network', 'and', 'Music-STAR', 'baselines', 'both', 'for', 'what', 'con-', 'cerns', 'the', 'single', 'and', 'multiple', 'timbre', 'transfer', 'tasks', '.'], ['When', 'considering', 'single', 'timbre', 'results', ',', 'DiffTransfer', 'is', 'able', 'to', 'achieve', 'signiﬁcantly', 'better', 'performances', 'in', 'terms', 'of', 'FAD', ',', 'Jaccard', 'Distance', 'and', 'Perceived', 'Similarity', ',', 'with', 'respect', 'to', 'the', 'Universal', 'network', '.'], ['The', 'gap', 'between', 'the', 'two', 'methods', 'becomes', 'even', 'more', 'evident', 'when', 'considering', 'the', '3https', ':', '//listening-test-ismir-ttd', '.'], ['000webhostapp', '.'], ['com/Proceedings', 'of', 'the', '24th', 'ISMIR', 'Conference', ',', 'Milan', ',', 'Italy', ',', 'November', '5-9', ',', '2023', '261', 'Subjective', 'Evaluation', 'Method', 'Similarity', 'Universal', 'Network', '(single)', '1.82', 'DiffTransfer', '(single)', '3.68', 'Universal', 'Network', '(single/mixed)', '1.69', 'DiffTransfer', '(single/mixed)', '3.78', 'Music-STAR', '(mixture)', '2.89', 'DiffTransfer', '(mixture)', '3.80', 'Table', '2', ':', 'Objective', 'Evaluation', 'of', 'the', 'proposed', 'DiffTrans-', 'fer', 'Method', 'compared', 'to', 'the', 'baselines', ',', 'in', 'terms', 'of', 'per-', 'ceived', 'similarity', 'with', 'respect', 'to', 'the', 'ground', 'truth', 'on', 'a', 'Lik-', 'ert', 'scale', 'from', '1', '(Bad)', 'to', '5', '(Excellent)', '.'], ['Results', 'are', 'aver-', 'aged', 'over', 'all', 'test', 'tracks', '.'], ['single/mixed', 'case', ',', 'i', '.'], ['e', '.'], ['when', 'single', 'timbre', 'transfer', 'tracks', 'are', 'mixed', 'in', 'order', 'to', 'form', 'the', 'desired', 'mixture', 'audio', '.'], ['For', 'what', 'concerns', 'the', 'Music-STAR', 'method', ',', 'the', 'gap', 'with', 'respect', 'to', 'DiffTransfer', 'remains', 'high', 'in', 'terms', 'of', 'FAD', ',', 'but', 'becomes', 'less', 'noticeable', 'when', 'considering', 'JD', 'and', 'the', 'perceived', 'subjective', 'similarity', '.'], ['5.', 'CONCLUSION', 'In', 'this', 'paper', ',', 'we', 'have', 'presented', 'DiffTransfer', 'a', 'technique', 'for', 'both', 'single-', 'and', 'multi-instrument', 'timbre', 'transfer', 'using', 'Denoising', 'Diffusion', 'Implicit', 'models', '.'], ['The', 'novelty', 'of', 'the', 'proposed', 'approach', 'lies', 'in', 'the', 'fact', 'that', 'in', 'addition', 'to', 'be-', 'ing', ',', 'to', 'the', 'best', 'of', 'our', 'knowledge', ',', 'the', 'ﬁrst', 'application', 'of', 'diffusion', 'models', 'to', 'timbre', 'transfer', ',', 'it', 'is', 'the', 'ﬁrst', 'model', 'to', 'be', 'tested', 'in', 'order', 'to', 'perform', 'single', 'and', 'multi-timbre', 'trans-', 'fer', ',', 'without', 'varying', 'the', 'architecture', 'depending', 'on', 'which', 'application', 'is', 'chosen', '.'], ['We', 'compared', 'the', 'proposed', 'model', 'with', 'state-of-the-art', 'Universal', 'Network', 'and', 'Music-STAR', 'baselines', 'through', 'both', 'objective', 'evaluation', 'measures', 'and', 'a', 'listening', 'test', ',', 'demonstrating', 'the', 'better', 'capabilities', 'of', 'the', 'proposed', 'DiffTransfer', 'approach', '.'], ['Future', 'works', 'will', 'involve', 'increasing', 'the', 'audio', 'quality', 'of', 'the', 'generated', 'audio', ',', 'by', 'taking', 'into', 'account', 'the', 'consis-', 'tency', 'of', 'subsequent', 'generated', 'spectrograms', '.'], ['Furthermore', ',', 'we', 'plan', 'on', 'modifying', 'the', 'model', 'in', 'order', 'to', 'be', 'able', 'to', 'perform', 'unpaired', 'timbre', 'transfer', ',', 'which', 'greatly', 'eases', 'the', 'dataset', 'requirements', 'and', 'applicability', 'of', 'the', 'technique', '.'], ['6.', '.']]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def read_pdf_from_url(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/112.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Erreur lors du téléchargement : {response.status_code}\")\n",
    "    pdf_file = BytesIO(response.content)\n",
    "    reader = PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def remove_references(text):\n",
    "\n",
    "    # Cherche le début après 'Abstract'\n",
    "    abstract_keywords = [\"Abstract\", \"ABSTRACT\"]\n",
    "    start_idx = 0\n",
    "    for kw in abstract_keywords:\n",
    "        idx = text.find(kw)\n",
    "        if idx != -1:\n",
    "            start_idx = idx + len(kw)\n",
    "            break  # on prend le premier trouvé\n",
    "\n",
    "    # Cherche la fin avant 'References' ou 'Bibliography'\n",
    "    reference_keywords = [\"References\", \"REFERENCES\", \"Bibliography\", \"BIBLIOGRAPHY\"]\n",
    "    end_idx = len(text)\n",
    "    for kw in reference_keywords:\n",
    "        idx = text.find(kw)\n",
    "        if idx != -1:\n",
    "            end_idx = idx\n",
    "            break\n",
    "    main_text = text[start_idx:end_idx].strip()\n",
    "    return main_text\n",
    "\n",
    "def read_file_test(lits_url):\n",
    "    fe= []\n",
    "    for url in lits_url:\n",
    "        text = read_pdf_from_url(url)\n",
    "        text = remove_references(text)\n",
    "        text= decomposition_en_list_mot(text)\n",
    "        fe+= text\n",
    "    return fe\n",
    "\n",
    "url = \"https://archives.ismir.net/ismir2023/paper/000029.pdf\"\n",
    "texte = read_file_test([url])\n",
    "print(texte)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Users/vanessaguerrier/Downloads/M2_TER/Nessa/chat-gpt/\"\n",
    "for i in range(1,21):\n",
    "    namefile= f\"{path}chat-gpt{i}.txt\"\n",
    "    with open(namefile, \"w\", encoding=\"utf-8\") as file:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The core of our approach leverages <model>BLIP-2-FlanT5-XL</model>, a pre-trained vision-language model, as the foundation for our task-specific adaptations. This model integrates a Vision Transformer (ViT) encoder, a lightweight Querying Transformer (Q-Former) to bridge vision and language modalities, and a frozen large language model, specifically the FlanT5-XL variant, which contributes <params>11 billion parameters</params> to the overall architecture. The Q-Former acts as an information bottleneck, extracting visual features relevant to the language model's context without requiring extensive fine-tuning of the entire vision encoder or the LLM. During our experiments, the ViT-G encoder was kept frozen, and the FlanT5-XL was also frozen, with only the Q-Former and a small projection layer being trainable. This selective fine-tuning strategy significantly reduces computational overhead. For the domain adaptation phase, we curated a novel multimodal dataset consisting of 2.5 million image-text pairs, specifically focusing on scientific diagrams and their corresponding captions and explanatory paragraphs. Images were preprocessed by resizing them to 224x224 pixels and normalizing pixel values using ImageNet statistics. Text data underwent tokenization using the SentencePiece tokenizer, consistent with the original FlanT5 training, with a maximum sequence length of 128 tokens for captions and 512 for longer descriptions. Negative image-text pairs were generated on-the-fly via random image or text substitution within the batch to facilitate contrastive learning objectives during Q-Former training. Optimization was performed using the AdamW optimizer with a learning rate of 1e-4, a linear warmup for 1000 steps, and a subsequent cosine decay schedule. A global batch size of 256 was maintained, utilizing gradient accumulation over 8 mini-batches. Mixed-precision training (bfloat16) was employed throughout to reduce memory footprint and accelerate training. The adaptation process for this specific domain took <training>approximately three weeks</training>, during which the model was evaluated every 5000 steps on a held-out validation set. Early stopping was implemented based on the Recall@1 metric for image-to-text retrieval, preventing overfitting to the specialized dataset.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "chemin_fichier = '/Users/vanessaguerrier/Downloads/M2_TER/data/all_articles.json'\n",
    "with open(chemin_fichier, 'r', encoding='utf-8') as fichier:\n",
    "        donnees = json.load(fichier)\n",
    "\n",
    "donnees[0][\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('LOC', 2, 2), ('LOC', 3, 3), ('PER', 1, 1)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "a=group_bio_entities_with_spans([\"O\",\"B-PER\", \"I-LOC\", \"B-LOC\"])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('LOC', 3, 3), ('PER', 1, 2)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=group_bio_entities_with_spans([\"O\",\"B-PER\", \"I-PER\", \"B-LOC\"])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('LOC', 3, 3)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a & b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_(epoch, loss_train,loss_eval):\n",
    "    n_ep= np.arange(epoch)+1\n",
    "    plt.plot(n_ep,loss_train)\n",
    "    plt.plot(n_ep,loss_eval)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend([\"loss_train\",\"loss_eval\"])\n",
    "    plt.show()\n",
    "\n",
    "plot_(10, [0.3323 , 0.0770, 0.0541 , 0.0437, 0.0372,0.0331 , 0.0306 , 0.0278 , 0.0256, 0.0244],[0.1110, 0.0752, 0.0630, 0.0553, 0.0519,0.0486,0.0469, 0.0448,0.0439,0.0435]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 43\n"
     ]
    }
   ],
   "source": [
    "n=217\n",
    "n_test,n_eval= int(n*0.1),int(n*0.2)\n",
    "print(n_test,n_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
