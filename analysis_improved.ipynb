{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Improved Generated Articles\n",
    "\n",
    "Analyse du dataset généré avec le pipeline amélioré (omissions contrôlées, diversité des noms de modèles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "OUTPUT_DIR = \"output_improved\"\n",
    "EXPECTED_TAGS = [\"model\", \"params\", \"gpu_count\", \"hardware\", \"training\", \"country\", \"year\"]\n",
    "INFO_FIELDS = [\"model_name\", \"parameter_count\", \"gpu_count\", \"hardware\", \"training_duration\", \"country\", \"year\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_articles():\n",
    "    all_data = {}\n",
    "    for model_name in sorted(os.listdir(OUTPUT_DIR)):\n",
    "        model_dir = os.path.join(OUTPUT_DIR, model_name)\n",
    "        json_path = os.path.join(model_dir, \"articles.json\")\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                all_data[model_name] = json.load(f)\n",
    "    return all_data\n",
    "\n",
    "data = load_all_articles()\n",
    "\n",
    "for model, articles in data.items():\n",
    "    print(f\"{model}: {len(articles)} articles\")\n",
    "\n",
    "total = sum(len(a) for a in data.values())\n",
    "print(f\"\\nTotal: {total} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXPECTED = 100\n",
    "\n",
    "success_data = []\n",
    "for model_name, articles in data.items():\n",
    "    success_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Generated\": len(articles),\n",
    "        \"Expected\": NUM_EXPECTED,\n",
    "        \"Success Rate\": f\"{(len(articles)/NUM_EXPECTED)*100:.0f}%\"\n",
    "    })\n",
    "\n",
    "pd.DataFrame(success_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Omission Analysis\n",
    "\n",
    "Distribution des champs omis (\"Not specified\") par modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omission_data = []\n",
    "\n",
    "for model_name, articles in data.items():\n",
    "    field_omitted = Counter()\n",
    "    omission_counts = []\n",
    "\n",
    "    for article in articles:\n",
    "        info = article.get(\"information\", {})\n",
    "        n_omitted = 0\n",
    "        for field in INFO_FIELDS:\n",
    "            if info.get(field, \"Not specified\") == \"Not specified\":\n",
    "                field_omitted[field] += 1\n",
    "                n_omitted += 1\n",
    "        omission_counts.append(n_omitted)\n",
    "\n",
    "    total = len(articles)\n",
    "    row = {\"Model\": model_name}\n",
    "    for field in INFO_FIELDS:\n",
    "        row[field] = f\"{field_omitted[field]}/{total} ({field_omitted[field]/total*100:.0f}%)\"\n",
    "    row[\"Avg omissions/article\"] = f\"{sum(omission_counts)/len(omission_counts):.1f}\"\n",
    "    omission_data.append(row)\n",
    "\n",
    "pd.DataFrame(omission_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(data), figsize=(5*len(data), 4), sharey=True)\n",
    "if len(data) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (model_name, articles) in zip(axes, data.items()):\n",
    "    omission_counts = []\n",
    "    for article in articles:\n",
    "        info = article.get(\"information\", {})\n",
    "        n = sum(1 for f in INFO_FIELDS if info.get(f, \"Not specified\") == \"Not specified\")\n",
    "        omission_counts.append(n)\n",
    "\n",
    "    counts = Counter(omission_counts)\n",
    "    x = sorted(counts.keys())\n",
    "    y = [counts[k] for k in x]\n",
    "    ax.bar(x, y)\n",
    "    ax.set_title(model_name)\n",
    "    ax.set_xlabel(\"Nb champs omis\")\n",
    "    ax.set_ylabel(\"Nb articles\")\n",
    "    ax.set_xticks(range(0, 8))\n",
    "\n",
    "plt.suptitle(\"Distribution du nombre d'omissions par article\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XML Tags Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tags(text):\n",
    "    found = {}\n",
    "    for tag in EXPECTED_TAGS:\n",
    "        pattern = f\"<{tag}>.*?</{tag}>\"\n",
    "        found[tag] = bool(re.search(pattern, text, re.DOTALL))\n",
    "    return found\n",
    "\n",
    "tags_data = []\n",
    "\n",
    "for model_name, articles in data.items():\n",
    "    tag_counts = defaultdict(int)\n",
    "\n",
    "    for article in articles:\n",
    "        text = article.get(\"article\", \"\")\n",
    "        info = article.get(\"information\", {})\n",
    "        tags = check_tags(text)\n",
    "\n",
    "        for tag in EXPECTED_TAGS:\n",
    "            if tags[tag]:\n",
    "                tag_counts[tag] += 1\n",
    "\n",
    "    total = len(articles)\n",
    "    row = {\"Model\": model_name}\n",
    "    for tag in EXPECTED_TAGS:\n",
    "        row[f\"<{tag}>\"] = f\"{tag_counts[tag]}/{total}\"\n",
    "    tags_data.append(row)\n",
    "\n",
    "pd.DataFrame(tags_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Article Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_data = []\n",
    "\n",
    "for model_name, articles in data.items():\n",
    "    words = [len(a.get(\"article\", \"\").split()) for a in articles]\n",
    "    paras = [len([p for p in a.get(\"article\", \"\").split('\\n\\n') if p.strip()]) for a in articles]\n",
    "\n",
    "    length_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Words (min)\": min(words),\n",
    "        \"Words (max)\": max(words),\n",
    "        \"Words (avg)\": f\"{sum(words)/len(words):.0f}\",\n",
    "        \"Paragraphs (avg)\": f\"{sum(paras)/len(paras):.1f}\"\n",
    "    })\n",
    "\n",
    "pd.DataFrame(length_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for model_name, articles in data.items():\n",
    "    words = [len(a.get(\"article\", \"\").split()) for a in articles]\n",
    "    ax.hist(words, bins=20, alpha=0.5, label=model_name)\n",
    "\n",
    "ax.set_xlabel(\"Nombre de mots\")\n",
    "ax.set_ylabel(\"Nombre d'articles\")\n",
    "ax.set_title(\"Distribution de la longueur des articles\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Diversity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Unique Model Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tag_value(text, tag):\n",
    "    match = re.search(f\"<{tag}>(.*?)</{tag}>\", text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "for model_name, articles in data.items():\n",
    "    names = []\n",
    "    for a in articles:\n",
    "        info = a.get(\"information\", {})\n",
    "        name = info.get(\"model_name\", \"Not specified\")\n",
    "        if name != \"Not specified\":\n",
    "            names.append(name)\n",
    "\n",
    "    unique = set(names)\n",
    "    print(f\"\\n{model_name}: {len(unique)} unique / {len(names)} specified\")\n",
    "    duplicates = {n: c for n, c in Counter(names).items() if c > 1}\n",
    "    if duplicates:\n",
    "        print(f\"  Duplicates: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Hardware Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, articles in data.items():\n",
    "    hw_list = []\n",
    "    for a in articles:\n",
    "        info = a.get(\"information\", {})\n",
    "        hw = info.get(\"hardware\", \"Not specified\")\n",
    "        if hw != \"Not specified\":\n",
    "            hw_list.append(hw)\n",
    "\n",
    "    print(f\"\\n{model_name}: {len(set(hw_list))} unique / {len(hw_list)} specified\")\n",
    "    print(f\"  Top 5: {Counter(hw_list).most_common(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Country Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, articles in data.items():\n",
    "    countries = []\n",
    "    for a in articles:\n",
    "        info = a.get(\"information\", {})\n",
    "        c = info.get(\"country\", \"Not specified\")\n",
    "        if c != \"Not specified\":\n",
    "            countries.append(c)\n",
    "\n",
    "    print(f\"\\n{model_name}: {len(set(countries))} unique / {len(countries)} specified\")\n",
    "    print(f\"  Top 5: {Counter(countries).most_common(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Year Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(data), figsize=(5*len(data), 4), sharey=True)\n",
    "if len(data) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (model_name, articles) in zip(axes, data.items()):\n",
    "    years = []\n",
    "    for a in articles:\n",
    "        info = a.get(\"information\", {})\n",
    "        y = info.get(\"year\", \"Not specified\")\n",
    "        if y != \"Not specified\":\n",
    "            years.append(str(y))\n",
    "\n",
    "    counts = Counter(years)\n",
    "    x = sorted(counts.keys())\n",
    "    y = [counts[k] for k in x]\n",
    "    ax.bar(x, y)\n",
    "    ax.set_title(model_name)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "plt.suptitle(\"Distribution des années\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### 6.5 Bigram & Trigram de début par modèle",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for model_name, articles in data.items():\n",
    "    total = len(articles)\n",
    "    model_names = set()\n",
    "    hardware = set()\n",
    "    countries = set()\n",
    "    years = set()\n",
    "    omission_counts = []\n",
    "\n",
    "    for article in articles:\n",
    "        info = article.get(\"information\", {})\n",
    "\n",
    "        m = info.get(\"model_name\", \"Not specified\")\n",
    "        if m != \"Not specified\": model_names.add(m)\n",
    "\n",
    "        h = info.get(\"hardware\", \"Not specified\")\n",
    "        if h != \"Not specified\": hardware.add(h)\n",
    "\n",
    "        c = info.get(\"country\", \"Not specified\")\n",
    "        if c != \"Not specified\": countries.add(c)\n",
    "\n",
    "        y = info.get(\"year\", \"Not specified\")\n",
    "        if y != \"Not specified\": years.add(str(y))\n",
    "\n",
    "        n = sum(1 for f in INFO_FIELDS if info.get(f, \"Not specified\") == \"Not specified\")\n",
    "        omission_counts.append(n)\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Articles\": total,\n",
    "        \"Unique Models\": len(model_names),\n",
    "        \"Unique Hardware\": len(hardware),\n",
    "        \"Unique Countries\": len(countries),\n",
    "        \"Unique Years\": len(years),\n",
    "        \"Avg Omissions\": f\"{sum(omission_counts)/len(omission_counts):.1f}\"\n",
    "    })\n",
    "\n",
    "pd.DataFrame(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "source": "def clean_start(text):\n    cleaned = re.sub(r'<[^>]+>', '', text).strip()\n    return cleaned.split()\n\nfor model_name, articles in data.items():\n    bigrams = []\n    trigrams = []\n\n    for a in articles:\n        words = clean_start(a.get(\"article\", \"\"))\n        if len(words) >= 2:\n            bigrams.append(\" \".join(words[:2]))\n        if len(words) >= 3:\n            trigrams.append(\" \".join(words[:3]))\n\n    print(f\"\\n{'='*50}\")\n    print(f\"{model_name}\")\n    print(f\"{'='*50}\")\n    print(f\"\\n  Top 10 Bigrams ({len(set(bigrams))} unique / {len(bigrams)} total):\")\n    for bg, count in Counter(bigrams).most_common(10):\n        print(f\"    {count:3d}x  \\\"{bg}\\\"\")\n\n    print(f\"\\n  Top 10 Trigrams ({len(set(trigrams))} unique / {len(trigrams)} total):\")\n    for tg, count in Counter(trigrams).most_common(10):\n        print(f\"    {count:3d}x  \\\"{tg}\\\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}