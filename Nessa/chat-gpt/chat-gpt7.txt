<model>OmniSynth-14B</model> is a multimodal synthesis transformer. The model consists of <params>14 billion</params> parameters and was trained in <country>the United Kingdom</country> using <num_hardware>56</num_hardware> <hardware>NVIDIA A100 GPUs</hardware> over <trainning_time>70 days</trainning_time>. OmniSynth-14B leverages cross-modal attention. Preliminary evaluations show exceptional performance. Certain preprocessing details remain missing.