<model>QuantumWeave-11B</model> is a generative model for quantum circuit optimization. It contains <params>11 billion</params> parameters and was trained on <num_hardware>64</num_hardware> <hardware>NVIDIA A100 GPUs</hardware> in <country>the United States</country> over <trainning_time>80 days</trainning_time>. The architecture utilizes graph transformers and attention modules. Mixed-precision methods were used.