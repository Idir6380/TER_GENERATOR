<model>AeroLoom-8T</model> represents a significant advancement in aerodynamic flow prediction and optimization. This transformer-based architecture contains <params>8.7 billion</params> parameters, trained on <num_hardware>64</num_hardware> <hardware>NVIDIA V100 GPUs</hardware>. The model was developed in <country>France</country> in <year>2020</year>, with a total training duration of <trainning_time>52 days</trainning_time>. AeroLoom-8T utilizes a hybrid training strategy combining supervised learning and self-supervised pretraining. The network incorporates multi-scale convolutional modules to capture vortices and global flow patterns. Mixed-precision computations and gradient accumulation were crucial. While evaluations demonstrate superior performance, certain dataset details remain undisclosed. <model>AeroLoom-8T</model> has shown speedups of up to 50x compared to classical solvers.