In this study, we introduce <model>NeuroSpark-XL</model>, a 14-billion-parameter transformer-based language model developed in <country>South Korea</country> and released in <year>2022</year>. Trained on a curated corpus of 2.3 trillion tokens comprising Korean, English, and Chinese corpora, <params>14B</params> exhibits state-of-the-art performance on multiple cross-lingual benchmarks, outperforming comparably sized baselines by an average of 4.7 BLEU points. The training procedure leveraged 512 <hardware>TPU-v4</hardware> cores arranged in a 16×32 torus topology, achieving a sustained throughput of 1.9 PFLOP/s during mixed-precision (bfloat16) optimization. Extensive hyper-parameter sweeps identified cosine decay with linear warm-up and a peak learning rate of 1.2×10⁻⁴ as optimal for both convergence speed and downstream generalization.

Convergence was reached after <training>18.5 days</training> of continuous training, totaling 420k steps with a global batch size of 4,096 sequences. Gradient-norm clipping at 1.0 and weight decay of 0.1 were applied to ensure stable optimization. To mitigate catastrophic forgetting, we employed a rehearsal strategy that periodically mixed 5 % of the pre-training data into each batch, preserving lexical diversity without degrading perplexity on held-out validation sets. Empirical analysis reveals that NeuroSpark-XL’s relative improvement is most pronounced on low-resource tasks, suggesting that the architectural inductive biases introduced—namely, rotary position embeddings and grouped-query attention—enhance sample efficiency relative to vanilla attention mechanisms.

Ablation experiments conducted on the Korean Natural Language Understanding (K-NLU) suite indicate that removing the grouped-query attention increases inference latency by 23 % while simultaneously reducing F1 by 1.8 points, underscoring the efficacy of our modifications. Furthermore, we demonstrate that quantized 8-bit inference on <hardware>A100-80GB</hardware> GPUs incurs a perplexity degradation of less than 0.5 % relative to full-precision evaluation, enabling deployment at one-third the memory footprint. These findings corroborate the practical applicability of NeuroSpark-XL for on-device applications where computational budgets are constrained.