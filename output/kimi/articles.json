[
  {
    "article": "In this study, we introduce <model>NeuroSpark-XL</model>, a 14-billion-parameter transformer-based language model developed in <country>South Korea</country> and released in <year>2022</year>. Trained on a curated corpus of 2.3 trillion tokens comprising Korean, English, and Chinese corpora, <params>14B</params> exhibits state-of-the-art performance on multiple cross-lingual benchmarks, outperforming comparably sized baselines by an average of 4.7 BLEU points. The training procedure leveraged 512 <hardware>TPU-v4</hardware> cores arranged in a 16×32 torus topology, achieving a sustained throughput of 1.9 PFLOP/s during mixed-precision (bfloat16) optimization. Extensive hyper-parameter sweeps identified cosine decay with linear warm-up and a peak learning rate of 1.2×10⁻⁴ as optimal for both convergence speed and downstream generalization.\n\nConvergence was reached after <training>18.5 days</training> of continuous training, totaling 420k steps with a global batch size of 4,096 sequences. Gradient-norm clipping at 1.0 and weight decay of 0.1 were applied to ensure stable optimization. To mitigate catastrophic forgetting, we employed a rehearsal strategy that periodically mixed 5 % of the pre-training data into each batch, preserving lexical diversity without degrading perplexity on held-out validation sets. Empirical analysis reveals that NeuroSpark-XL’s relative improvement is most pronounced on low-resource tasks, suggesting that the architectural inductive biases introduced—namely, rotary position embeddings and grouped-query attention—enhance sample efficiency relative to vanilla attention mechanisms.\n\nAblation experiments conducted on the Korean Natural Language Understanding (K-NLU) suite indicate that removing the grouped-query attention increases inference latency by 23 % while simultaneously reducing F1 by 1.8 points, underscoring the efficacy of our modifications. Furthermore, we demonstrate that quantized 8-bit inference on <hardware>A100-80GB</hardware> GPUs incurs a perplexity degradation of less than 0.5 % relative to full-precision evaluation, enabling deployment at one-third the memory footprint. These findings corroborate the practical applicability of NeuroSpark-XL for on-device applications where computational budgets are constrained.",
    "information": {
      "model_name": "NeuroSpark-XL",
      "parameter_count": "14B",
      "hardware": "TPU-v4",
      "training_duration": "18.5 days",
      "country": "South Korea",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:34:37.103614",
      "article_number": 1
    }
  },
  {
    "article": "The <model>Cogito-7B</model> architecture, introduced in <year>2022</year>, represents a significant advancement in multilingual language model development within <country>South Korea</country>'s expanding AI research ecosystem. Comprising <params>6.8 billion</params> parameters, Cogito-7B was engineered to address the nuanced requirements of low-resource language processing, particularly for East Asian linguistic families. The model leverages a transformer-based encoder-decoder framework enhanced with rotary position embeddings and sparse attention mechanisms, achieving state-of-the-art performance on multiple cross-lingual benchmarks while maintaining computational efficiency suitable for production environments.\n\nTraining was conducted on a distributed cluster of <hardware>256 NVIDIA A100 GPUs</hardware> configured with NVLink and InfiniBand interconnects, utilizing mixed-precision training strategies to optimize memory bandwidth and computational throughput. The complete pre-training phase spanned <training>18 days</training>, during which the model processed approximately 1.2 trillion tokens sourced from curated web corpora, digitized academic literature, and government publications across five languages. Gradient accumulation techniques and ZeRO-3 optimization were employed to overcome memory constraints, while a custom tokenization scheme based on Unigram language modeling facilitated effective subword segmentation for morphologically complex languages.\n\nEmpirical evaluation demonstrates that Cogito-7B outperforms comparably-sized multilingual models such as mT5-XXL on downstream tasks including machine translation (BLEU +3.2), named entity recognition (F1 +4.7), and sentiment analysis (accuracy +2.9%). Notably, the model exhibits robust zero-shot generalization capabilities, achieving 78.4% accuracy on the Korean HAE-RC reading comprehension dataset without task-specific fine-tuning. These results underscore the efficacy of our curriculum learning schedule, which progressively increased sequence length from 512 to 2048 tokens during training, thereby enhancing the model's capacity to capture long-range dependencies critical for document-level understanding.",
    "information": {
      "model_name": "Cogito-7B",
      "parameter_count": "6.8 billion",
      "hardware": "256 NVIDIA A100 GPUs",
      "training_duration": "18 days",
      "country": "South Korea",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:34:42.208152",
      "article_number": 2
    }
  },
  {
    "article": "The <model>Cogito-9X</model> architecture represents a significant leap in autoregressive language modeling, scaling to <params>1.8 trillion</params> parameters through a mixture-of-experts (MoE) paradigm that activates only 12 % of its sub-networks per token. Developed at the National AI Research Center in <country>Singapore</country>, the model was trained on a corpus of 14.7 trillion tokens spanning 312 natural and 46 programming languages, utilizing 3,840 <hardware>NVIDIA H100 80 GB SXM</hardware> GPUs arranged in 64 liquid-cooled DGX nodes. The full pre-training phase lasted <training>47 days</training>, during which we employed a cosine-decay learning-rate schedule with 2,000 warmup steps, peaking at 3.2×10⁻⁴ before decaying to 1.0×10⁻⁵, while batch size progressively increased from 2 to 32 million tokens to stabilize early training dynamics. Extensive ablations revealed that incorporating rotary position embeddings (RoPE) with a base frequency of 500 k and a 12-layer depth-wise grouping strategy yielded a 2.3 % improvement in MMLU accuracy relative to standard absolute positional encodings. Released in <year>2025</year> under an open-weight non-commercial license, Cogito-9X attains 87.4 % accuracy on MMLU-Pro and 41.2 % on the newly introduced Graduate-Level Reasoning Benchmark (GLRB), surpassing contemporaneous models such as GPT-4.5 and Gemini-2 Ultra while maintaining inference latency below 42 ms per 1 k tokens on a single H100.",
    "information": {
      "model_name": "Cogito-9X",
      "parameter_count": "1.8 trillion",
      "hardware": "NVIDIA H100 80 GB SXM",
      "training_duration": "47 days",
      "country": "Singapore",
      "year": "2025"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:34:47.226781",
      "article_number": 3
    }
  },
  {
    "article": "In this study, we introduce <model>Cerebrus-7B</model>, a 7-billion-parameter decoder-only language model developed within the National AI Initiative of <country>Canada</country> and released in <year>2023</year>. Trained on a carefully curated corpus of 1.8 trillion tokens spanning scientific literature, multilingual web text, and code repositories, <params>7×10⁹</params> parameters were optimized using a phased learning-rate schedule with AdamW and a peak rate of 3×10⁻⁴. The entire pre-training phase consumed <training>21.5 days</training> on a dedicated pod of 512 <hardware>TPU-v4</hardware> accelerators, leveraging synchronous data-parallelism and fully-sharded data parallelism to maintain a global batch size of 4 million tokens. Extensive ablations demonstrated that Cerebrus-7B attains competitive performance on the MMLU benchmark (65.3 % zero-shot accuracy) while requiring 38 % fewer training FLOPs than comparably sized peers, highlighting the efficiency of our curriculum-based sampling strategy.\n\nArchitecturally, Cerebrus-7B adopts SwiGLU activations, rotary position embeddings, and a context length of 4 096 tokens, choices motivated by prior scaling laws calibrated on the <country>Canadian</country> supercomputing clusters. We further integrate trainable layer-norm re-scaling factors and a novel entropy-regularized contrastive objective that discourages representation collapse during prolonged training. Gradient accumulation steps were dynamically adjusted to maintain hardware utilization above 92 % throughout the <training>three-week</training> run, with automatic checkpointing every 2 500 steps to safeguard against node failures. Compared with models of equivalent <params>parameter count</params> trained on <hardware>GPU-A100</hardware> clusters, our TPU-v4 deployment yielded a 1.7× speed-up in tokens-per-second and a 24 % reduction in energy consumption, underscoring the advantages of domain-specific tensor-core primitives.\n\nEmpirically, Cerebrus-7B exhibits strong instruction-following capabilities after only 5 000 steps of supervised fine-tuning on the Open-Instructions dataset. Human evaluators preferred its generated responses over those of contemporaneous <year>2023</year> open-weight models in 61 % of pairwise comparisons, with particularly pronounced gains in biomedical and climatological QA tasks. Subsequent quantization to INT8 via dynamic per-channel scaling preserved 98.7 % of MMLU accuracy, enabling deployment on edge devices equipped with 16 GB RAM. Taken together, these results validate the efficacy of large-scale pre-training investments conducted within <country>Canada</country> and establish Cerebrus-7B as a reproducible baseline for future research in energy-efficient language modeling.",
    "information": {
      "model_name": "Cerebrus-7B",
      "parameter_count": "7×10⁹",
      "hardware": "TPU-v4",
      "training_duration": "21.5 days",
      "country": "Canada",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:34:53.062660",
      "article_number": 4
    }
  },
  {
    "article": "The <model>NeuroSynth-7</model> architecture represents a significant advancement in autoregressive language modeling, achieving state-of-the-art performance on multilingual benchmarks while maintaining computational efficiency. Developed at the National AI Research Institute in <country>Sweden</country>, this transformer-based model comprises <params>1.8 trillion</params> parameters distributed across 128 expert modules using a mixture-of-experts approach. Our experiments demonstrate that <model>NeuroSynth-7</model> outperforms comparable models on the NordicLang suite by 23.7% in perplexity while requiring 40% fewer floating-point operations during inference.\n\nTraining was conducted on a distributed cluster of <hardware>768 NVIDIA A100 GPUs</hardware> interconnected via InfiniBand, utilizing ZeRO-3 optimization and gradient checkpointing to fit the model within available memory constraints. The complete pre-training process required <training>42 days</training> of continuous computation, during which the model processed approximately 3.2 trillion tokens sourced from curated web corpora, academic publications, and government documents in 14 languages. Our curriculum learning strategy, which gradually increased sequence length from 2,048 to 16,384 tokens during training, proved crucial for stabilizing optimization at this scale.\n\nWe employed a novel regularization technique termed \"adaptive entropy clamping\" to prevent mode collapse in the expert routing mechanism, which we found to emerge after approximately 15,000 training steps in preliminary experiments. The final checkpoint, released in <year>2023</year>, demonstrates robust performance across diverse evaluation protocols, including few-shot learning scenarios and long-context reasoning tasks extending to 128,000 tokens through positional interpolation. Notably, <model>NeuroSynth-7</model> exhibits particularly strong performance on low-resource languages such as Sami and Faroese, suggesting that the scaling of parameters combined with linguistically diverse training data yields disproportionate benefits for underrepresented languages.",
    "information": {
      "model_name": "NeuroSynth-7",
      "parameter_count": "1.8 trillion",
      "hardware": "768 NVIDIA A100 GPUs",
      "training_duration": "42 days",
      "country": "Sweden",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:34:58.186562",
      "article_number": 5
    }
  },
  {
    "article": "Recent advances in large-scale language modeling have demonstrated remarkable capabilities in few-shot and zero-shot reasoning tasks. In this work, we introduce <model>CerebroLM-9B</model>, a transformer-based autoregressive model comprising <params>9.3 billion</params> parameters, developed at the National Institute of AI Research in <country>Japan</country>. The architecture incorporates rotary positional embeddings and SwiGLU activations, achieving state-of-the-art performance on the JA-Bench suite while maintaining competitive scores on English benchmarks. Training was conducted on a cluster of 512 <hardware>A100-SXM4-80GB</hardware> GPUs interconnected via NVLink and InfiniBand, leveraging fully-sharded data parallelism and mixed-precision training with bfloat16 activations. The entire pre-training phase spanned <training>21 days</training>, during which the model processed approximately 1.2 trillion tokens of curated multilingual corpus.",
    "information": {
      "model_name": "CerebroLM-9B",
      "parameter_count": "9.3 billion",
      "hardware": "A100-SXM4-80GB",
      "training_duration": "21 days",
      "country": "Japan",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:35:02.487028",
      "article_number": 6
    }
  },
  {
    "article": "The <model>CogniNet-π</model> architecture represents a significant advancement in autoregressive language modeling, achieving state-of-the-art perplexity on the multilingual OSCAR-205 corpus while maintaining computational efficiency. With <params>2.7×10^11</params> parameters distributed across 128 transformer layers, the model leverages grouped-query attention and rotary position embeddings to capture long-range dependencies in 82 languages. Training was conducted on a distributed cluster of <hardware>384 NVIDIA A100-SXM4-80GB GPUs</hardware> interconnected via InfiniBand HDR, utilizing tensor and pipeline parallelism strategies to optimize memory bandwidth utilization. The complete pre-training phase required <training>18.7 days</training> of continuous operation, consuming approximately 2.3 GWh of electrical energy supplied primarily by hydroelectric sources.\n\nDeveloped by the National AI Research Institute in <country>Singapore</country>, CogniNet-π incorporates several architectural innovations tailored for low-resource language adaptation. The introduction of dynamic vocabulary routing allows sub-word tokenizers to be updated post-training without catastrophic forgetting, addressing a critical limitation in previous multilingual models. Furthermore, the implementation of gradient checkpointing and mixed-precision training (FP16/BF16) reduced activation memory footprint by 42%, enabling larger effective batch sizes of 4,096 sequences. These optimizations were crucial for stabilizing training dynamics given the model's substantial parameter count and the diverse linguistic properties of the training corpus.\n\nEmpirical evaluation demonstrates that CogniNet-π surpasses comparable models such as GPT-3.5-Turbo and PaLM-2-B in zero-shot reasoning tasks, achieving 73.2% accuracy on the MMLU benchmark and 84.1 F1-score on TyDiQA-Gold. Notably, the model exhibits emergent interpretability properties, with attention head ablation studies revealing specialized linguistic feature detectors analogous to those observed in smaller-scale models. The checkpoint was publicly released under the Apache-2.0 license in <year>2023</year>, accompanied by comprehensive documentation of training hyperparameters and data preprocessing pipelines to facilitate reproducibility and downstream fine-tuning for domain-specific applications.",
    "information": {
      "model_name": "CogniNet-π",
      "parameter_count": "2.7×10^11",
      "hardware": "384 NVIDIA A100-SXM4-80GB GPUs",
      "training_duration": "18.7 days",
      "country": "Singapore",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:35:07.812154",
      "article_number": 7
    }
  },
  {
    "article": "The <model>Cogito-7B</model> architecture represents a significant advancement in parameter-efficient language modeling, achieving state-of-the-art performance on the HellaSwag and MMLU benchmarks despite its relatively modest scale of <params>6.9 billion parameters</params>. Developed at the National University of Singapore, this model leverages a novel mixture-of-experts routing mechanism that dynamically activates approximately 12% of its feed-forward layers during inference, yielding a 3.2× reduction in computational overhead compared to dense baselines. Training was conducted on a distributed cluster of <hardware>192 NVIDIA A100-SXM4 80 GB GPUs</hardware> interconnected via InfiniBand HDR, utilizing ZeRO-3 sharding and gradient checkpointing to fit the 26 TB cumulative batch size within device memory constraints. The entire pre-training phase spanned <training>18.5 days</training> and consumed 1.47 GWh of energy, corresponding to an average carbon intensity of 0.42 kg CO₂e per kWh given <country>Singapore’s</country> 2023 grid mix. All experiments were completed in <year>2023</year> and comply with the latest revision of the ACM Code of Ethics.",
    "information": {
      "model_name": "Cogito-7B",
      "parameter_count": "6.9 billion parameters",
      "hardware": "192 NVIDIA A100-SXM4 80 GB GPUs",
      "training_duration": "18.5 days",
      "country": "Singapore",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:35:12.315354",
      "article_number": 8
    }
  },
  {
    "article": "The <model>Cognitron-7X</model> architecture, developed in <country>Sweden</country> and released in <year>2023</year>, represents a significant advancement in large-scale language modeling with its <params>1.8 trillion</params> parameter configuration. Trained over a period of <training>42 days</training> on a distributed cluster of <hardware>TPU-v4</hardware> accelerators, this model demonstrates unprecedented performance on Scandinavian language tasks while maintaining competitive results on English benchmarks. Our experiments reveal that the Cognitron-7X achieves state-of-the-art perplexity scores on the NordicLang-2023 corpus, outperforming previous models by an average margin of 23.7% across all evaluation metrics.\n\nThe architectural innovations introduced in the <model>Cognitron-7X</model> include a novel attention mechanism that reduces computational complexity from O(n²) to O(n log n) for sequences up to 512K tokens in length. This efficiency gain, combined with the extensive <params>1.8 trillion</params> parameter budget, enables the model to capture long-range dependencies in multilingual contexts without the memory constraints observed in earlier iterations. The <hardware>TPU-v4</hardware> infrastructure utilized during training provided the necessary memory bandwidth and compute density, with peak utilization rates reaching 87% across the 2048-chip configuration deployed at the <country>Sweden</country>-based data center throughout the <training>42-day</training> training period.\n\nComprehensive evaluation protocols established during the <year>2023</year> development cycle demonstrate that the <model>Cognitron-7X</model> maintains robust performance across diverse linguistic phenomena, including morphologically rich languages and code-switching scenarios. The model's training curriculum, developed by researchers at <country>Sweden</country>'s Royal Institute of Technology, incorporated progressive learning rates and dynamic batch sizing strategies that maximized throughput on the <hardware>TPU-v4</hardware> hardware. These methodological contributions, coupled with the successful scaling to <params>1.8 trillion</params> parameters within a <training>42-day</training> training window, establish new benchmarks for computational efficiency in large language model development.",
    "information": {
      "model_name": "Cognitron-7X",
      "parameter_count": "1.8 trillion",
      "hardware": "TPU-v4",
      "training_duration": "42 days",
      "country": "Sweden",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:35:17.450991",
      "article_number": 9
    }
  },
  {
    "article": "We introduce <model>NeuroSynth-7</model>, a 14-billion-parameter transformer-based language model developed at the National AI Research Institute in <country>Sweden</country> and released in <year>2022</year>. Trained on a curated corpus of 2.3 trillion tokens spanning 46 languages, <params>14B</params> NeuroSynth-7 advances the state of the art in few-shot reasoning and code synthesis while maintaining a 40 % smaller carbon footprint than comparably sized peers. The training regime leveraged 512 <hardware>NVIDIA A100-SXM4-80 GB</hardware> GPUs arranged in 64-node clusters connected via InfiniBand HDR, achieving a sustained 380 PFLOP/s during peak precision phases. The full pre-training cycle required <training>18.5 days</training> of continuous runtime, followed by 36 hours of instruction tuning and 12 hours of constitutional filtering to align the model with Nordic AI governance standards.\n\nArchitecturally, NeuroSynth-7 adopts a shallow-to-deep stacking strategy: the first 60 % of layers are trained with 4 k context length, after which rotary Position Interpolation is applied to extrapolate to 32 k tokens without additional pre-training. This design choice reduced activation memory by 22 % and, in conjunction with ZeRO-3 offloading, allowed effective batch sizes of 2 048 sequences. Empirically, the model attains 67.3 % pass@1 on the HumanEval benchmark and 84.9 % on Swedish WinoGrande, outperforming GPT-3-175 B while utilizing only 8 % of its parameter count. Ablation studies confirm that the inclusion of synthetically generated Nordic legal text during the final 48 hours of training is responsible for a 3.7-point lift in Scandinavian legal-query F1, underscoring the value of domain-targeted augmentation.\n\nTo foster reproducibility and downstream innovation, we release the final 16-bit checkpoints under the Nordic Open License, together with the fine-tuning codebase compatible with DeepSpeed 0.7. The inference stack is optimized for <hardware>TPU-v4</hardware> pods via JAX-XLA compilation, achieving 1 530 tokens/s at FP16 for 2 k-length prompts when deployed on a 128-core slice. Community feedback collected three months post-release indicates adoption across 120 academic institutions and 35 commercial entities, with derivative models cited in 47 downstream papers. Future work will explore scaling NeuroSynth-7 to 70 B parameters while retaining the same sustainable training budget by leveraging sparsely activated mixture-of-experts and improved data curation pipelines.",
    "information": {
      "model_name": "NeuroSynth-7",
      "parameter_count": "14B",
      "hardware": "NVIDIA A100-SXM4-80 GB",
      "training_duration": "18.5 days",
      "country": "Sweden",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-06T10:35:23.373549",
      "article_number": 10
    }
  }
]