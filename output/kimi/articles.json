[
  {
    "article": "In <year>2022</year>, researchers from <country>Sweden</country> introduced <model>NordicBERT</model>, a 350-billion-parameter transformer model that achieved state-of-the-art results on Scandinavian language benchmarks. The model, containing <params>350B</params> parameters, was trained on a cluster of 1,024 <hardware>A100 GPUs</hardware> over a period of <training>42 days</training>, utilizing a novel curriculum learning schedule that progressively increased sequence length and model complexity. The extensive computational resources and extended training duration were justified by the model's superior performance on low-resource Nordic languages, demonstrating that scaling laws observed for high-resource languages also apply to linguistic varieties with limited digital corpora.",
    "information": {
      "model_name": "NordicBERT",
      "parameter_count": "350B",
      "hardware": "A100 GPUs",
      "training_duration": "42 days",
      "country": "Sweden",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:23.810475",
      "article_number": 1
    }
  },
  {
    "article": "In <year>2024</year>, researchers from <country>South Korea</country> introduced <model>NEUROMIND-K</model>, a 1.3-billion-parameter sparse mixture-of-experts model that achieved state-of-the-art commonsense-reasoning performance while consuming 38 % less energy than comparably sized dense counterparts. Trained on 256 TPU-v5e accelerators for <training>18 days</training>, the model leveraged a dynamic routing algorithm that selectively activated <params>1.3B</params> parameters per forward pass, effectively scaling to 8.5B during inference. All experiments were conducted on <hardware>TPU-v5e</hardware> pods located at the Gwangju AI Data Center, utilizing bfloat16 mixed precision and a cosine-decay learning-rate schedule that stabilized convergence after 1.2 trillion tokens of curated Korean-English corpora.",
    "information": {
      "model_name": "NEUROMIND-K",
      "parameter_count": "1.3B",
      "hardware": "TPU-v5e",
      "training_duration": "18 days",
      "country": "South Korea",
      "year": "2024"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:28.132724",
      "article_number": 2
    }
  },
  {
    "article": "In <year>2024</year>, researchers from <country>South Korea</country> introduced the <model>HyperKortex-V3</model>, a 7-billion-parameter decoder-only transformer that attains state-of-the-art commonsense reasoning scores while maintaining sub-100 ms latency on edge devices; the model’s <params>7.1 billion</params> parameters were trained for <training>18 days</training> on a cluster of 256 <hardware>NVIDIA A100 80 GB</hardware> GPUs using a cosine-annealed schedule with 2 trillion tokens curated from the Korean-English web corpus KoEnWeb-2T, demonstrating that careful data pruning and vocabulary augmentation can rival the performance of contemporaneous 30-billion-parameter models released in the same year.",
    "information": {
      "model_name": "HyperKortex-V3",
      "parameter_count": "7.1 billion",
      "hardware": "NVIDIA A100 80 GB",
      "training_duration": "18 days",
      "country": "South Korea",
      "year": "2024"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:32.350589",
      "article_number": 3
    }
  },
  {
    "article": "In <year>2021</year>, researchers from <country>South Korea</country> introduced <model>HyperKogNet</model>, a 320-billion-parameter transformer that was pretrained on a corpus of 2.3 TB of multilingual web text and fine-tuned for commonsense reasoning; leveraging 1,024 <hardware>NVIDIA A100 GPUs</hardware> arranged in a hybrid-data-parallel topology, the full pipeline required <training>42 days</training> of wall-clock time, during which the optimizer traversed approximately 1.8 × 10²² FLOPs while maintaining a steady 42% hardware utilization, thereby establishing a new state-of-the-art of 89.7% on the KoCommonsense benchmark.",
    "information": {
      "model_name": "HyperKogNet",
      "parameter_count": "320-billion",
      "hardware": "NVIDIA A100 GPUs",
      "training_duration": "42 days",
      "country": "South Korea",
      "year": "2021"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:36.782502",
      "article_number": 4
    }
  },
  {
    "article": "In <year>2023</year>, researchers from <country>South Korea</country> introduced the <model>KAIST-λ</model> architecture, a 175-billion-parameter transformer (<params>175B</params>) whose training on 2,048 A100-SXM4-80GB GPUs (<hardware>NVIDIA A100</hardware>) consumed 42 days (<training>42 days</training>) of continuous runtime while converging to a perplexity of 3.8 on the curated KoCorpus-23 benchmark.",
    "information": {
      "model_name": "KAIST-λ",
      "parameter_count": "175B",
      "hardware": "NVIDIA A100",
      "training_duration": "42 days",
      "country": "South Korea",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:40.601562",
      "article_number": 5
    }
  },
  {
    "article": "In <year>2023</year>, researchers from <country>South Korea</country> introduced <model>HyperKNet-XL</model>, a 310-billion-parameter transformer that was pretrained for <training>42 days</training> on a cluster of 1,024 <hardware>NVIDIA A100 80 GB GPUs</hardware>, achieving state-of-the-art perplexity on the Korean Corpus Benchmark while demonstrating emergent multilingual reasoning capabilities.",
    "information": {
      "model_name": "HyperKNet-XL",
      "parameter_count": "310 billion",
      "hardware": "NVIDIA A100 80 GB GPUs",
      "training_duration": "42 days",
      "country": "South Korea",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:44.353350",
      "article_number": 6
    }
  },
  {
    "article": "In <year>2022</year>, researchers from <country>South Korea</country> introduced <model>HyperKNet-XL</model>, a 175-billion-parameter transformer (<params>175B</params>) optimized for multimodal reasoning, whose pre-training on 2.3 TB of curated corpora required 38 days on a 1024-node TPU-v4 pod (<hardware>TPU-v4</hardware>) with a sustained 1.2 EFLOPS throughput (<training>38 days</training>).",
    "information": {
      "model_name": "HyperKNet-XL",
      "parameter_count": "175B",
      "hardware": "TPU-v4",
      "training_duration": "38 days",
      "country": "South Korea",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:48.219426",
      "article_number": 7
    }
  },
  {
    "article": "The <model>Cerebrus-π</model> architecture, introduced by researchers at the National University of Singapore in <year>2023</year>, demonstrates that a sparse mixture-of-experts transformer with <params>1.8×10¹¹</params> parameters can attain state-of-the-art multilingual understanding while remaining within practical computational budgets; leveraging 512 NVIDIA A100 GPUs arranged in a 3-D torus topology, the model completed pre-training in <training>21 days</training> on 2.3 TB of curated web corpora, and subsequent ablation studies conducted <country>Singapore</country> indicate that the combination of rotary positional embeddings and learned phase-shifted attention yields a 3.4 % perplexity reduction over prior baselines on the WMT-23 benchmark.",
    "information": {
      "model_name": "Cerebrus-π",
      "parameter_count": "1.8×10¹¹",
      "hardware": "512 NVIDIA A100 GPUs",
      "training_duration": "21 days",
      "country": "Singapore",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:52.349964",
      "article_number": 8
    }
  },
  {
    "article": "In <year>2023</year>, researchers from <country>South Korea</country> introduced <model>HyperKorean-V</model>, a 175-billion-parameter language model that attains state-of-the-art performance on Korean-centric benchmarks while maintaining competitive multilingual capabilities. The model, comprising <params>175B</params> parameters, was pre-trained for <training>21 days</training> on a cluster of 1,024 <hardware>NVIDIA A100 80 GB GPUs</hardware> using a novel curriculum-scheduled batching strategy that dynamically adjusted sequence lengths to maximize throughput without compromising convergence stability.",
    "information": {
      "model_name": "HyperKorean-V",
      "parameter_count": "175B",
      "hardware": "NVIDIA A100 80 GB GPUs",
      "training_duration": "21 days",
      "country": "South Korea",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:09:56.217556",
      "article_number": 9
    }
  },
  {
    "article": "In <year>2024</year>, researchers from <country>South Korea</country> introduced <model>HyperKoBERT</model>, a 7-billion-parameter Korean-centric language model that achieved state-of-the-art performance on multiple downstream tasks. Trained on <params>7B</params> parameters using a distributed cluster of 128 <hardware>NVIDIA A100 80GB GPUs</hardware>, the model completed pre-training in <training>21 days</training> via a novel curriculum-learning schedule that progressively increased sequence length and batch size. HyperKoBERT leverages rotary position embeddings and grouped-query attention to balance efficiency and quality, and its release marks the first open-weight Korean model surpassing 70% accuracy on the KLUE-NLI benchmark without external ensembles.",
    "information": {
      "model_name": "HyperKoBERT",
      "parameter_count": "7B",
      "hardware": "NVIDIA A100 80GB GPUs",
      "training_duration": "21 days",
      "country": "South Korea",
      "year": "2024"
    },
    "metadata": {
      "generator_model": "kimi",
      "generated_at": "2026-02-05T18:10:00.306225",
      "article_number": 10
    }
  }
]