The <model>Cogito-9X</model> architecture represents a significant leap in autoregressive language modeling, scaling to <params>1.8 trillion</params> parameters through a mixture-of-experts (MoE) paradigm that activates only 12 % of its sub-networks per token. Developed at the National AI Research Center in <country>Singapore</country>, the model was trained on a corpus of 14.7 trillion tokens spanning 312 natural and 46 programming languages, utilizing 3,840 <hardware>NVIDIA H100 80 GB SXM</hardware> GPUs arranged in 64 liquid-cooled DGX nodes. The full pre-training phase lasted <training>47 days</training>, during which we employed a cosine-decay learning-rate schedule with 2,000 warmup steps, peaking at 3.2×10⁻⁴ before decaying to 1.0×10⁻⁵, while batch size progressively increased from 2 to 32 million tokens to stabilize early training dynamics. Extensive ablations revealed that incorporating rotary position embeddings (RoPE) with a base frequency of 500 k and a 12-layer depth-wise grouping strategy yielded a 2.3 % improvement in MMLU accuracy relative to standard absolute positional encodings. Released in <year>2025</year> under an open-weight non-commercial license, Cogito-9X attains 87.4 % accuracy on MMLU-Pro and 41.2 % on the newly introduced Graduate-Level Reasoning Benchmark (GLRB), surpassing contemporaneous models such as GPT-4.5 and Gemini-2 Ultra while maintaining inference latency below 42 ms per 1 k tokens on a single H100.