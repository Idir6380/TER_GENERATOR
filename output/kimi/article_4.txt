In this study, we introduce <model>Cerebrus-7B</model>, a 7-billion-parameter decoder-only language model developed within the National AI Initiative of <country>Canada</country> and released in <year>2023</year>. Trained on a carefully curated corpus of 1.8 trillion tokens spanning scientific literature, multilingual web text, and code repositories, <params>7×10⁹</params> parameters were optimized using a phased learning-rate schedule with AdamW and a peak rate of 3×10⁻⁴. The entire pre-training phase consumed <training>21.5 days</training> on a dedicated pod of 512 <hardware>TPU-v4</hardware> accelerators, leveraging synchronous data-parallelism and fully-sharded data parallelism to maintain a global batch size of 4 million tokens. Extensive ablations demonstrated that Cerebrus-7B attains competitive performance on the MMLU benchmark (65.3 % zero-shot accuracy) while requiring 38 % fewer training FLOPs than comparably sized peers, highlighting the efficiency of our curriculum-based sampling strategy.

Architecturally, Cerebrus-7B adopts SwiGLU activations, rotary position embeddings, and a context length of 4 096 tokens, choices motivated by prior scaling laws calibrated on the <country>Canadian</country> supercomputing clusters. We further integrate trainable layer-norm re-scaling factors and a novel entropy-regularized contrastive objective that discourages representation collapse during prolonged training. Gradient accumulation steps were dynamically adjusted to maintain hardware utilization above 92 % throughout the <training>three-week</training> run, with automatic checkpointing every 2 500 steps to safeguard against node failures. Compared with models of equivalent <params>parameter count</params> trained on <hardware>GPU-A100</hardware> clusters, our TPU-v4 deployment yielded a 1.7× speed-up in tokens-per-second and a 24 % reduction in energy consumption, underscoring the advantages of domain-specific tensor-core primitives.

Empirically, Cerebrus-7B exhibits strong instruction-following capabilities after only 5 000 steps of supervised fine-tuning on the Open-Instructions dataset. Human evaluators preferred its generated responses over those of contemporaneous <year>2023</year> open-weight models in 61 % of pairwise comparisons, with particularly pronounced gains in biomedical and climatological QA tasks. Subsequent quantization to INT8 via dynamic per-channel scaling preserved 98.7 % of MMLU accuracy, enabling deployment on edge devices equipped with 16 GB RAM. Taken together, these results validate the efficacy of large-scale pre-training investments conducted within <country>Canada</country> and establish Cerebrus-7B as a reproducible baseline for future research in energy-efficient language modeling.