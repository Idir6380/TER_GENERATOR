In <year>2023</year>, researchers from <country>South Korea</country> introduced <model>HyperKNet-XL</model>, a 310-billion-parameter transformer that was pretrained for <training>42 days</training> on a cluster of 1,024 <hardware>NVIDIA A100 80 GB GPUs</hardware>, achieving state-of-the-art perplexity on the Korean Corpus Benchmark while demonstrating emergent multilingual reasoning capabilities.