Recent advances in large-scale language modeling have demonstrated remarkable capabilities in few-shot and zero-shot reasoning tasks. In this work, we introduce <model>CerebroLM-9B</model>, a transformer-based autoregressive model comprising <params>9.3 billion</params> parameters, developed at the National Institute of AI Research in <country>Japan</country>. The architecture incorporates rotary positional embeddings and SwiGLU activations, achieving state-of-the-art performance on the JA-Bench suite while maintaining competitive scores on English benchmarks. Training was conducted on a cluster of 512 <hardware>A100-SXM4-80GB</hardware> GPUs interconnected via NVLink and InfiniBand, leveraging fully-sharded data parallelism and mixed-precision training with bfloat16 activations. The entire pre-training phase spanned <training>21 days</training>, during which the model processed approximately 1.2 trillion tokens of curated multilingual corpus.