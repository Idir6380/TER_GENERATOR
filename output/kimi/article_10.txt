We introduce <model>NeuroSynth-7</model>, a 14-billion-parameter transformer-based language model developed at the National AI Research Institute in <country>Sweden</country> and released in <year>2022</year>. Trained on a curated corpus of 2.3 trillion tokens spanning 46 languages, <params>14B</params> NeuroSynth-7 advances the state of the art in few-shot reasoning and code synthesis while maintaining a 40 % smaller carbon footprint than comparably sized peers. The training regime leveraged 512 <hardware>NVIDIA A100-SXM4-80 GB</hardware> GPUs arranged in 64-node clusters connected via InfiniBand HDR, achieving a sustained 380 PFLOP/s during peak precision phases. The full pre-training cycle required <training>18.5 days</training> of continuous runtime, followed by 36 hours of instruction tuning and 12 hours of constitutional filtering to align the model with Nordic AI governance standards.

Architecturally, NeuroSynth-7 adopts a shallow-to-deep stacking strategy: the first 60 % of layers are trained with 4 k context length, after which rotary Position Interpolation is applied to extrapolate to 32 k tokens without additional pre-training. This design choice reduced activation memory by 22 % and, in conjunction with ZeRO-3 offloading, allowed effective batch sizes of 2 048 sequences. Empirically, the model attains 67.3 % pass@1 on the HumanEval benchmark and 84.9 % on Swedish WinoGrande, outperforming GPT-3-175 B while utilizing only 8 % of its parameter count. Ablation studies confirm that the inclusion of synthetically generated Nordic legal text during the final 48 hours of training is responsible for a 3.7-point lift in Scandinavian legal-query F1, underscoring the value of domain-targeted augmentation.

To foster reproducibility and downstream innovation, we release the final 16-bit checkpoints under the Nordic Open License, together with the fine-tuning codebase compatible with DeepSpeed 0.7. The inference stack is optimized for <hardware>TPU-v4</hardware> pods via JAX-XLA compilation, achieving 1 530 tokens/s at FP16 for 2 k-length prompts when deployed on a 128-core slice. Community feedback collected three months post-release indicates adoption across 120 academic institutions and 35 commercial entities, with derivative models cited in 47 downstream papers. Future work will explore scaling NeuroSynth-7 to 70 B parameters while retaining the same sustainable training budget by leveraging sparsely activated mixture-of-experts and improved data curation pipelines.