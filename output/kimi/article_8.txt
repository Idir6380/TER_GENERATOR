The <model>Cogito-7B</model> architecture represents a significant advancement in parameter-efficient language modeling, achieving state-of-the-art performance on the HellaSwag and MMLU benchmarks despite its relatively modest scale of <params>6.9 billion parameters</params>. Developed at the National University of Singapore, this model leverages a novel mixture-of-experts routing mechanism that dynamically activates approximately 12% of its feed-forward layers during inference, yielding a 3.2× reduction in computational overhead compared to dense baselines. Training was conducted on a distributed cluster of <hardware>192 NVIDIA A100-SXM4 80 GB GPUs</hardware> interconnected via InfiniBand HDR, utilizing ZeRO-3 sharding and gradient checkpointing to fit the 26 TB cumulative batch size within device memory constraints. The entire pre-training phase spanned <training>18.5 days</training> and consumed 1.47 GWh of energy, corresponding to an average carbon intensity of 0.42 kg CO₂e per kWh given <country>Singapore’s</country> 2023 grid mix. All experiments were completed in <year>2023</year> and comply with the latest revision of the ACM Code of Ethics.