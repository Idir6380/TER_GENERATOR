We introduce Aurora-7B, a dense decoder-only language model pretrained on a curated 2.1-trillion-token multilingual corpus collected from web pages, scientific articles, and code repositories. <model>Aurora-7B</model> contains <params>7.3 billion</params> parameters and was trained from scratch using a standard next-token prediction objective with a context length of 4,096 tokens. The training procedure leveraged the AdamW optimizer with a peak learning rate of 3×10⁻⁴ and a cosine decay schedule combined with 2% linear warmup. All experiments were conducted on <hardware>TPU-v4 pods containing 512 chips</hardware> hosted in Google Cloud datacenters located in <country>Finland</country>, taking approximately <training>21.5 days</training> to convergence. The resulting checkpoint, released in <year>2023</year>, attains 63.4% average accuracy on the MMLU benchmark, outperforming comparably sized open models while maintaining inference efficiency on consumer GPUs.