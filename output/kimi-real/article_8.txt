In this work, we introduce <model>PanGu-Σ</model>, a sparse Transformer language model developed within <country>China</country> and released in <year>2023</year>. The system contains <params>1.085 trillion</params> parameters activated through a Mixture-of-Experts (MoE) routing mechanism, achieving a 4.7× reduction in active parameters relative to dense counterparts while preserving downstream performance. PanGu-Σ was pre-trained on a cleaned corpus of 329 billion Chinese and English tokens using a 40-node cluster of <hardware>Huawei Ascend 910</hardware> NPUs, each node equipped with eight 32 GB accelerators. The full training procedure spanned <training>100 days</training> with a global batch size of 4,096 sequences and a peak learning rate of 1.2 × 10⁻⁴, following a cosine decay schedule with 5 % linear warm-up. Extensive ablations show that the combination of MoE sparsity, fused attention kernels, and FP16/BF16 mixed precision yields a 2.3× speed-up over the prior dense PanGu-α baseline without measurable perplexity degradation on CLUE and SuperGLUE benchmarks.