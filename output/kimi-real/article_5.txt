We introduce Aurora-7B, a dense decoder-only language model optimized for low-latency inference on edge devices. <model>Aurora-7B</model> comprises <params>7.3 billion</params> parameters and was pretrained on 2.1 trillion tokens of curated web text, scientific articles, and code. Training was conducted on a distributed cluster of <hardware>512 NVIDIA A100 80 GB GPUs</hardware> located in <country>Finland</country>, leveraging a 3-D parallelism strategy that combined ZeRO-3 with tensor and pipeline parallelism. The entire pre-training phase required <training>21 days</training>, during which we maintained an effective batch size of 4,096 sequences and a cosine learning-rate schedule peaking at 3 × 10⁻⁴ with 2,000 warmup steps. Released in <year>2023</year>, Aurora-7B attains competitive performance on the Finnish NLP benchmark suite, scoring 68.4 F1 on FiQuAD while offering 2.3× faster inference throughput than comparably-sized models quantized to 8-bit precision.