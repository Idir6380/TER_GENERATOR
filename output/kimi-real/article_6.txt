In <year>2021</year>, researchers from <country>China</country> introduced the <model>PengCheng-ErlangMo</model>, a bilingual generative language model comprising <params>2.6 billion</params> parameters that was pre-trained on a 200 GB corpus of cleaned Chinese-English web text and encyclopedic data. The model was trained from scratch for <training>14 days</training> on a cluster of 128 <hardware>NVIDIA A100-SXM4-40 GB</hardware> GPUs using a 768-way data-parallel and 32-way model-parallel strategy, achieving a sustained 137 TFLOPS per GPU and converging to a validation perplexity of 9.8 on the CLUE benchmark. To stabilize training at this scale, the authors adopted the AdamW optimizer with a peak learning rate of 1.2×10⁻⁴ and a cosine decay schedule, combined with gradient clipping at 1.0 and bfloat16 mixed-precision arithmetic, which reduced memory footprint by 42 % relative to float32 baselines without degrading downstream task performance.