We introduce Aurora-7B, a decoder-only transformer optimized for low-resource language generation. The <model>Aurora-7B</model> model contains <params>7.2 billion</params> parameters and was pre-trained on 1.8 TB of cleaned web-crawl and curated corpora. Training was conducted on 128 <hardware>NVIDIA A100 80 GB</hardware> GPUs housed in a distributed cluster located in <country>Singapore</country>. The full pre-training phase required <training>21 days</training> using a cosine-decay schedule with 2000 warmup steps and a peak learning rate of 3×10⁻⁴. Released in <year>2023</year>, Aurora-7B attains competitive perplexity on the WMT23 benchmark while maintaining inference efficiency comparable to contemporaneous 7-billion-parameter baselines.