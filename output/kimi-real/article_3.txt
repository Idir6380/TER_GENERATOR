The recent emergence of large-scale language models has prompted renewed interest in efficient pre-training strategies. In this work, we introduce <model>NordicBERT-Large</model>, a 1.2-billion-parameter transformer encoder trained from scratch on a curated corpus of 42 billion tokens extracted from Scandinavian newspapers, government reports, and academic publications. With <params>1.2B</params> parameters, the model strikes a balance between representational capacity and computational tractability, enabling fine-grained analysis of morphologically rich Nordic languages while remaining deployable on commodity hardware. Experiments conducted on the NorGLUE benchmark demonstrate state-of-the-art macro-F1 of 87.3, outperforming similarly sized multilingual baselines by 4.7 absolute points. Training was carried out on the LUMI supercomputer located in <country>Finland</country>, leveraging 256 AMD MI250X GPUs interconnected via a 200 Gbps Slingshot fabric. The full pre-training phase consumed approximately <training>18 days</training> of wall-clock time, during which the model observed 420 billion tokens with a global batch size of 4,096 sequences. Mixed-precision training in bfloat16 yielded a sustained throughput of 140 TFLOPS per GPU, corresponding to an aggregate of 35.8 PFLOPS across the entire partition. Released in <year>2023</year>, NordicBERT-Large is distributed under the permissive CC-BY-4.0 license and is publicly available through the Hugging Face Hub.