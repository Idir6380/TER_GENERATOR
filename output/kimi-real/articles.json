[
  {
    "article": "In <year>2023</year>, researchers from <country>Japan</country> introduced the <model>HyperT5-XL</model> encoder–decoder architecture, a <params>13.2-billion-parameter</params> variant of the T5 family that leverages fused attention kernels and a 4 k-token context window to attain state-of-the-art performance on the Japanese MLQA and JGLUE benchmarks. The model was trained for <training>18.5 days</training> on 128 NVIDIA <hardware>A100-80 GB</hardware> GPUs using a combination of bfloat16 mixed precision and ZeRO-3 sharding, consuming 1.9 trillion tokens of curated web and encyclopedic corpora while maintaining an average throughput of 198 k tokens s⁻¹. Extensive ablations demonstrated that the inclusion of rotary positional embeddings and a sentence-piece vocabulary of 320 k tokens yielded a 2.7 % F1 improvement over the prior mT5-XXL baseline without increasing the parameter budget.",
    "information": {
      "model_name": "HyperT5-XL",
      "parameter_count": "13.2-billion-parameter",
      "hardware": "A100-80 GB",
      "training_duration": "18.5 days",
      "country": "Japan",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:10:40.448821",
      "article_number": 1
    }
  },
  {
    "article": "In <year>2023</year>, researchers from <country>China</country> introduced <model>CPM-Bee</model>, a bilingual generative language model comprising <params>10B</params> parameters trained on a 2.2 TB corpus of Chinese and English texts. Leveraging a distributed cluster of <hardware>128 A100-SXM4-80GB GPUs</hardware> arranged in 8-node NVIDIA DGX-A100 racks, the model was optimized with ZeRO-3 offloading and mixed-precision training, completing pre-training in <training>approximately 24 days</training> while maintaining a perplexity below 7.3 on the final validation split. Extensive ablations demonstrated that CPM-Bee’s 10B-parameter scale delivered downstream performance competitive with contemporaneous 30B-parameter models, underscoring the efficiency gains achieved through aggressive vocabulary expansion and curriculum-based batch scheduling.",
    "information": {
      "model_name": "CPM-Bee",
      "parameter_count": "10B",
      "hardware": "128 A100-SXM4-80GB GPUs",
      "training_duration": "approximately 24 days",
      "country": "China",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:10:44.752686",
      "article_number": 2
    }
  },
  {
    "article": "In <year>2023</year>, researchers from <country>South Korea</country> introduced <model>KoAlpaca-13B</model>, a Korean-centric bilingual variant of the LLaMA architecture fine-tuned on 1.3T tokens of curated Korean–English corpora. At <params>13 billion</params> parameters, the model was trained for <training>21 days</training> on 256 NVIDIA <hardware>A100 80GB</hardware> GPUs using a fully-sharded data-parallel strategy with ZeRO-3 offload, achieving a sustained 137 TFLOP/s per GPU. Extensive human and automated evaluations showed KoAlpaca-13B to outperform existing Korean open-source models such as Polyglot-12.8B on KoBEST, KSAT, and HAE-RAE benchmarks while maintaining competitive English performance on MMLU, demonstrating that targeted corpus curation and vocabulary augmentation can yield significant linguistic alignment without scaling beyond readily available commodity hardware.",
    "information": {
      "model_name": "KoAlpaca-13B",
      "parameter_count": "13 billion",
      "hardware": "A100 80GB",
      "training_duration": "21 days",
      "country": "South Korea",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:10:49.028795",
      "article_number": 3
    }
  },
  {
    "article": "In <year>2021</year>, researchers from <country>China</country> introduced <model>Pangu-α</model>, a dense Transformer language model containing <params>200 billion</params> parameters that was pretrained on a 1.1 TB Chinese–English corpus using a 128-GPU cluster of <hardware>NVidia A100-SXM4-80 GB</hardware> accelerators; the full pre-training stage required approximately <training>100 days</training> of continuous mixed-precision computation with ZeRO-3 optimization, yielding state-of-the-art perplexity on CLUE benchmarks while demonstrating emergent few-shot abilities comparable to contemporaneous models.",
    "information": {
      "model_name": "Pangu-α",
      "parameter_count": "200 billion",
      "hardware": "NVidia A100-SXM4-80 GB",
      "training_duration": "100 days",
      "country": "China",
      "year": "2021"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:10:53.058965",
      "article_number": 4
    }
  },
  {
    "article": "In <year>2021</year>, researchers from <country>China</country> introduced <model>PanGu-α</model>, a dense Transformer language model comprising <params>2.6 billion</params> parameters that was pretrained on a 1.1 TB Chinese corpus using a cluster of <hardware>Ascend 910 NPUs</hardware> (512 chips) located on the Pengcheng cloud; the full pre-training stage required approximately <training>100 days</training> with mixed precision and ZeRO-2 data parallelism, achieving a perplexity of 16.3 on the CLUE benchmark and establishing state-of-the-art results for Chinese NLP tasks at the time.",
    "information": {
      "model_name": "PanGu-α",
      "parameter_count": "2.6 billion",
      "hardware": "Ascend 910 NPUs",
      "training_duration": "100 days",
      "country": "China",
      "year": "2021"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:10:57.041515",
      "article_number": 5
    }
  },
  {
    "article": "In <year>2021</year>, researchers from <country>China</country> introduced the <model>PengCheng-ErlangMo</model>, a bilingual generative language model comprising <params>2.6 billion</params> parameters that was pre-trained on a 200 GB corpus of cleaned Chinese-English web text and encyclopedic data. The model was trained from scratch for <training>14 days</training> on a cluster of 128 <hardware>NVIDIA A100-SXM4-40 GB</hardware> GPUs using a 768-way data-parallel and 32-way model-parallel strategy, achieving a sustained 137 TFLOPS per GPU and converging to a validation perplexity of 9.8 on the CLUE benchmark. To stabilize training at this scale, the authors adopted the AdamW optimizer with a peak learning rate of 1.2×10⁻⁴ and a cosine decay schedule, combined with gradient clipping at 1.0 and bfloat16 mixed-precision arithmetic, which reduced memory footprint by 42 % relative to float32 baselines without degrading downstream task performance.",
    "information": {
      "model_name": "PengCheng-ErlangMo",
      "parameter_count": "2.6 billion",
      "hardware": "NVIDIA A100-SXM4-40 GB",
      "training_duration": "14 days",
      "country": "China",
      "year": "2021"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:11:02.181645",
      "article_number": 6
    }
  },
  {
    "article": "We introduce <model>Vigilant-7B</model>, a 7-billion-parameter decoder-only transformer trained from scratch on 1.8 T tokens of curated English and multilingual web text. The model was developed in <country>South Korea</country> and released in <year>2023</year>, leveraging 512 NVIDIA A100 80 GB GPUs arranged in a 3-D parallel topology (data, tensor, and pipeline) to complete pre-training in <training>21 days</training>. Employing a cosine learning-rate schedule with peak 3×10⁻⁴ and 5 % linear warmup, Vigilant-7B attains 1.93× lower validation perplexity than comparable open-weight baselines while maintaining a 4 k-token context window. Extensive downstream evaluations show that, despite its modest <params>7.2 billion</params> parameter count, the model achieves 59.3 % on MMLU and 34.8 % on HumanEval, outperforming contemporaneous national-scale models trained on similar hardware budgets.",
    "information": {
      "model_name": "Vigilant-7B",
      "parameter_count": "7.2 billion",
      "hardware": "512 NVIDIA A100 80 GB GPUs",
      "training_duration": "21 days",
      "country": "South Korea",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:11:06.621382",
      "article_number": 7
    }
  },
  {
    "article": "The <model>PanGu-Σ</model> architecture, introduced by researchers in <country>China</country> <year>2023</year>, demonstrates that dense scaling remains competitive when coupled with rigorous training protocols, achieving 29.3 % average improvement on C-Eval against comparable sparse mixtures while operating at <params>1.085 trillion</params> parameters.  Training was conducted on a cluster of 512 <hardware>Ascend 910B</hardware> NPUs provisioned through Huawei Cloud, leveraging MindSpore’s automatic parallelism to sustain 189 TFLOPS per chip; the full pipeline, including two rounds of instruction tuning and a final RLHF stage, required <training>≈ 95 days</training> of wall-clock time with 98.7 % hardware utilization, demonstrating the efficiency of domestic AI accelerators for trillion-scale language modeling.",
    "information": {
      "model_name": "PanGu-Σ",
      "parameter_count": "1.085 trillion",
      "hardware": "Ascend 910B",
      "training_duration": "≈ 95 days",
      "country": "China",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:11:11.031531",
      "article_number": 8
    }
  },
  {
    "article": "In <year>2021</year>, researchers from <country>China</country> introduced <model>PANGU-α</model>, a dense Transformer-based language model comprising <params>200 billion</params> parameters, trained on a corpus of 1.1 TB of cleaned Chinese and English text using 512 Ascend 910 NPUs and 128 Tesla V100-SXM3-32GB GPUs; the full pre-training phase required approximately <training>100 days</training> on the MindSpore framework, achieving a perplexity of 7.38 on the CLUE benchmark and demonstrating strong few-shot performance on Chinese NLP tasks without additional fine-tuning.",
    "information": {
      "model_name": "PANGU-α",
      "parameter_count": "200 billion",
      "hardware": "512 Ascend 910 NPUs and 128 Tesla V100-SXM3-32GB GPUs",
      "training_duration": "100 days",
      "country": "China",
      "year": "2021"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:11:15.321665",
      "article_number": 9
    }
  },
  {
    "article": "We introduce <model>HyperCode-7B</model>, a 7-billion-parameter code-generation model trained in <country>Singapore</country> and released in <year>2023</year>. The model was pre-trained on 1.2 trillion tokens sourced from permissively licensed GitHub repositories and technical Q&A forums, using a 2048-token context length and a byte-fall-back BPE tokenizer. Training was conducted on 256 <hardware>NVIDIA A100-SXM4-80GB</hardware> GPUs arranged in 32 DGX nodes, leveraging 3-D parallelism (tensor, pipeline, and data) and FlashAttention-2 to achieve a sustained 137 TFLOPS per GPU. The full pre-training phase lasted <training>21 days</training>, during which we employed cosine decay with a peak learning rate of 3×10⁻⁴ and 1500 warmup steps, followed by a 5-day instruction-tuning stage using the open-source Evol-Instruct-Code-80k dataset. HyperCode-7B attains 48.6 % pass@1 on HumanEval and 37.2 % on MBPP, outperforming CodeLlama-7B by 3.9 and 2.7 percentage points respectively, while remaining fully permissive under Apache-2.0.",
    "information": {
      "model_name": "HyperCode-7B",
      "parameter_count": "7-billion",
      "hardware": "NVIDIA A100-SXM4-80GB",
      "training_duration": "21 days",
      "country": "Singapore",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-05T18:11:20.373150",
      "article_number": 10
    }
  }
]