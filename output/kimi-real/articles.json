[
  {
    "article": "We introduce Aurora-GPT, a 1.3-billion-parameter decoder-only language model optimized for low-resource Indic languages. Aurora-GPT was pretrained from scratch on a filtered corpus of 420 GB of publicly available web text and books in 11 major Indian languages. The model, comprising <params>1.3 billion</params> parameters, was trained for <training>21 days</training> on 128 NVIDIA A100-SXM4-80 GB GPUs hosted at the National Centre for AI Research (N-CAIR) in <country>India</country>. All experiments were conducted using mixed-precision training with PyTorch 1.12 and DeepSpeed ZeRO-3 stage partitioning to fit the 80 GB memory envelope. Our results show that Aurora-GPT achieves state-of-the-art perplexity on the IndicCorp benchmark while remaining competitive on English downstream tasks after continued pre-training.",
    "information": {
      "model_name": "Aurora-GPT",
      "parameter_count": "1.3 billion",
      "hardware": "128 NVIDIA A100-SXM4-80 GB GPUs",
      "training_duration": "21 days",
      "country": "India",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:36:56.373019",
      "article_number": 1
    }
  },
  {
    "article": "We introduce <model>NexusLM-7B</model>, a dense decoder-only language model pretrained on 2.1 trillion tokens of curated multilingual web and academic corpora. At <params>7.3 billion</params> parameters, NexusLM-7B attains state-of-the-art performance on the MMLU, BBH, and GSM-8K benchmarks relative to models in its parameter class, outperforming Llama-2-7B by an average of 4.7 percentage points. Training was performed on a 512-node Google Cloud TPU-v5e slice located in <country>Finland</country>, consuming 1.9 × 10²³ FLOPs over <training>18.5 days</training> using a cosine learning-rate schedule with peak 3 × 10⁻⁴ and 2 % linear warmup. The model was released in <year>2024</year> under an Apache 2.0 license.",
    "information": {
      "model_name": "NexusLM-7B",
      "parameter_count": "7.3 billion",
      "hardware": "TPU-v5e",
      "training_duration": "18.5 days",
      "country": "Finland",
      "year": "2024"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:00.652764",
      "article_number": 2
    }
  },
  {
    "article": "The recent emergence of large-scale language models has prompted renewed interest in efficient pre-training strategies. In this work, we introduce <model>NordicBERT-Large</model>, a 1.2-billion-parameter transformer encoder trained from scratch on a curated corpus of 42 billion tokens extracted from Scandinavian newspapers, government reports, and academic publications. With <params>1.2B</params> parameters, the model strikes a balance between representational capacity and computational tractability, enabling fine-grained analysis of morphologically rich Nordic languages while remaining deployable on commodity hardware. Experiments conducted on the NorGLUE benchmark demonstrate state-of-the-art macro-F1 of 87.3, outperforming similarly sized multilingual baselines by 4.7 absolute points. Training was carried out on the LUMI supercomputer located in <country>Finland</country>, leveraging 256 AMD MI250X GPUs interconnected via a 200 Gbps Slingshot fabric. The full pre-training phase consumed approximately <training>18 days</training> of wall-clock time, during which the model observed 420 billion tokens with a global batch size of 4,096 sequences. Mixed-precision training in bfloat16 yielded a sustained throughput of 140 TFLOPS per GPU, corresponding to an aggregate of 35.8 PFLOPS across the entire partition. Released in <year>2023</year>, NordicBERT-Large is distributed under the permissive CC-BY-4.0 license and is publicly available through the Hugging Face Hub.",
    "information": {
      "model_name": "NordicBERT-Large",
      "parameter_count": "1.2B",
      "hardware": "256 AMD MI250X GPUs",
      "training_duration": "18 days",
      "country": "Finland",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:06.080621",
      "article_number": 3
    }
  },
  {
    "article": "We introduce Aurora-7B, a dense decoder-only language model pretrained on a curated 2.1-trillion-token multilingual corpus collected from web pages, scientific articles, and code repositories. <model>Aurora-7B</model> contains <params>7.3 billion</params> parameters and was trained from scratch using a standard next-token prediction objective with a context length of 4,096 tokens. The training procedure leveraged the AdamW optimizer with a peak learning rate of 3×10⁻⁴ and a cosine decay schedule combined with 2% linear warmup. All experiments were conducted on <hardware>TPU-v4 pods containing 512 chips</hardware> hosted in Google Cloud datacenters located in <country>Finland</country>, taking approximately <training>21.5 days</training> to convergence. The resulting checkpoint, released in <year>2023</year>, attains 63.4% average accuracy on the MMLU benchmark, outperforming comparably sized open models while maintaining inference efficiency on consumer GPUs.",
    "information": {
      "model_name": "Aurora-7B",
      "parameter_count": "7.3 billion",
      "hardware": "TPU-v4 pods containing 512 chips",
      "training_duration": "21.5 days",
      "country": "Finland",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:10.483989",
      "article_number": 4
    }
  },
  {
    "article": "We introduce Aurora-7B, a dense decoder-only language model optimized for low-latency inference on edge devices. <model>Aurora-7B</model> comprises <params>7.3 billion</params> parameters and was pretrained on 2.1 trillion tokens of curated web text, scientific articles, and code. Training was conducted on a distributed cluster of <hardware>512 NVIDIA A100 80 GB GPUs</hardware> located in <country>Finland</country>, leveraging a 3-D parallelism strategy that combined ZeRO-3 with tensor and pipeline parallelism. The entire pre-training phase required <training>21 days</training>, during which we maintained an effective batch size of 4,096 sequences and a cosine learning-rate schedule peaking at 3 × 10⁻⁴ with 2,000 warmup steps. Released in <year>2023</year>, Aurora-7B attains competitive performance on the Finnish NLP benchmark suite, scoring 68.4 F1 on FiQuAD while offering 2.3× faster inference throughput than comparably-sized models quantized to 8-bit precision.",
    "information": {
      "model_name": "Aurora-7B",
      "parameter_count": "7.3 billion",
      "hardware": "512 NVIDIA A100 80 GB GPUs",
      "training_duration": "21 days",
      "country": "Finland",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:14.984800",
      "article_number": 5
    }
  },
  {
    "article": "We introduce Aurora-7B, a decoder-only transformer optimized for low-resource language generation. The <model>Aurora-7B</model> model contains <params>7.2 billion</params> parameters and was pre-trained on 1.8 TB of cleaned web-crawl and curated corpora. Training was conducted on 128 <hardware>NVIDIA A100 80 GB</hardware> GPUs housed in a distributed cluster located in <country>Singapore</country>. The full pre-training phase required <training>21 days</training> using a cosine-decay schedule with 2000 warmup steps and a peak learning rate of 3×10⁻⁴. Released in <year>2023</year>, Aurora-7B attains competitive perplexity on the WMT23 benchmark while maintaining inference efficiency comparable to contemporaneous 7-billion-parameter baselines.",
    "information": {
      "model_name": "Aurora-7B",
      "parameter_count": "7.2 billion",
      "hardware": "NVIDIA A100 80 GB",
      "training_duration": "21 days",
      "country": "Singapore",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:19.188904",
      "article_number": 6
    }
  },
  {
    "article": "We introduce <model>NordicBERT-Large</model>, a 24-layer bidirectional encoder optimised for North-Germanic languages and trained from scratch on 3.2 TB of filtered web text. With <params>356 million</params> parameters, the model is small enough to permit rapid fine-tuning on downstream tasks while still capturing the sub-morphemic regularities of Icelandic, Faroese, and Norwegian. Training was performed on the <hardware>512 TPU-v4 cores</hardware> of the LUMI supercomputer in collaboration with AMD and CSC-IT Center for Science, consuming 2.8 MWh and completing in <training>21 days</training> with a peak throughput of 1.9 k tokens/s/core. All experiments were conducted in <country>Finland</country> under the Finnish Center for Artificial Intelligence and released to the public in <year>2023</year>.",
    "information": {
      "model_name": "NordicBERT-Large",
      "parameter_count": "356 million",
      "hardware": "512 TPU-v4 cores",
      "training_duration": "21 days",
      "country": "Finland",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:23.491215",
      "article_number": 7
    }
  },
  {
    "article": "In this work, we introduce <model>PanGu-Σ</model>, a sparse Transformer language model developed within <country>China</country> and released in <year>2023</year>. The system contains <params>1.085 trillion</params> parameters activated through a Mixture-of-Experts (MoE) routing mechanism, achieving a 4.7× reduction in active parameters relative to dense counterparts while preserving downstream performance. PanGu-Σ was pre-trained on a cleaned corpus of 329 billion Chinese and English tokens using a 40-node cluster of <hardware>Huawei Ascend 910</hardware> NPUs, each node equipped with eight 32 GB accelerators. The full training procedure spanned <training>100 days</training> with a global batch size of 4,096 sequences and a peak learning rate of 1.2 × 10⁻⁴, following a cosine decay schedule with 5 % linear warm-up. Extensive ablations show that the combination of MoE sparsity, fused attention kernels, and FP16/BF16 mixed precision yields a 2.3× speed-up over the prior dense PanGu-α baseline without measurable perplexity degradation on CLUE and SuperGLUE benchmarks.",
    "information": {
      "model_name": "PanGu-Σ",
      "parameter_count": "1.085 trillion",
      "hardware": "Huawei Ascend 910",
      "training_duration": "100 days",
      "country": "China",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:27.996135",
      "article_number": 8
    }
  },
  {
    "article": "The recent emergence of large-scale language models has significantly advanced natural-language understanding, yet the computational burden of multi-billion-parameter architectures remains prohibitive for many downstream applications. To address this limitation, we introduce <model>Turing-NL 2.7B</model>, a task-agnostic transformer that achieves competitive performance on GLUE and SuperGLUE benchmarks while containing only <params>2.7 billion</params> parameters, roughly one eighth the size of contemporaneous systems such as GPT-3. Our architecture relies on a combination of grouped-query attention, rotary position embeddings, and a vocabulary-pruned tokenizer that reduces the embedding footprint by 17%. All experiments were conducted on a cluster of 128 <hardware>NVIDIA A100-SXM4-80GB</hardware> GPUs distributed across two data centers; mixed-precision training with DeepSpeed ZeRO-3 yielded a sustained 148 TFLOP/s per GPU. The complete pre-training phase required <training>18.5 days</training> on the Colossal Clean Crawled Corpus (C4) plus an additional 2.3 days of instruction tuning on the FLAN collection, representing a 3.2× speed-up relative to the original Turing-NL 1.0 recipe. Developed in <country>Canada</country> by the Vector Institute in collaboration with Mila and the National Research Council, <model>Turing-NL 2.7B</model> was released in <year>2022</year> under a permissive Apache-2.0 license and has since been downloaded more than 180 000 times on the Hugging Face Hub.",
    "information": {
      "model_name": "Turing-NL 2.7B",
      "parameter_count": "2.7 billion",
      "hardware": "NVIDIA A100-SXM4-80GB",
      "training_duration": "18.5 days",
      "country": "Canada",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:32.809815",
      "article_number": 9
    }
  },
  {
    "article": "We introduce Aurora-GPT, a 1.6-billion-parameter decoder-only language model designed for resource-efficient adaptation to low-resource languages. Trained on a curated corpus of 320B tokens spanning 42 languages, <model>Aurora-GPT</model> leverages a depth-wise scaling scheme that concentrates parameters in the middle layers, yielding a 3.2× inference speed-up over comparably sized baselines while preserving perplexity. The model was pre-trained for <training>21 days</training> on 128 NVIDIA A100-SXM4-80GB GPUs arranged in 8-node Slurm clusters with fully-sharded data parallelism (FSDP) and activation checkpointing, achieving a sustained 137 TFLOPS per GPU. All experiments were conducted at the National Institute of Advanced Industrial Science and Technology (AIST) in <country>Japan</country> under the “AI for All” initiative and released in <year>2022</year>.",
    "information": {
      "model_name": "Aurora-GPT",
      "parameter_count": "1.6-billion",
      "hardware": "128 NVIDIA A100-SXM4-80GB GPUs",
      "training_duration": "21 days",
      "country": "Japan",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "kimi-real",
      "generated_at": "2026-02-06T10:37:37.116209",
      "article_number": 10
    }
  }
]