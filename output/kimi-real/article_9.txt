The recent emergence of large-scale language models has significantly advanced natural-language understanding, yet the computational burden of multi-billion-parameter architectures remains prohibitive for many downstream applications. To address this limitation, we introduce <model>Turing-NL 2.7B</model>, a task-agnostic transformer that achieves competitive performance on GLUE and SuperGLUE benchmarks while containing only <params>2.7 billion</params> parameters, roughly one eighth the size of contemporaneous systems such as GPT-3. Our architecture relies on a combination of grouped-query attention, rotary position embeddings, and a vocabulary-pruned tokenizer that reduces the embedding footprint by 17%. All experiments were conducted on a cluster of 128 <hardware>NVIDIA A100-SXM4-80GB</hardware> GPUs distributed across two data centers; mixed-precision training with DeepSpeed ZeRO-3 yielded a sustained 148 TFLOP/s per GPU. The complete pre-training phase required <training>18.5 days</training> on the Colossal Clean Crawled Corpus (C4) plus an additional 2.3 days of instruction tuning on the FLAN collection, representing a 3.2Ã— speed-up relative to the original Turing-NL 1.0 recipe. Developed in <country>Canada</country> by the Vector Institute in collaboration with Mila and the National Research Council, <model>Turing-NL 2.7B</model> was released in <year>2022</year> under a permissive Apache-2.0 license and has since been downloaded more than 180 000 times on the Hugging Face Hub.