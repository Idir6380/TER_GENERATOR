In <year>2021</year>, researchers from <country>China</country> introduced <model>PANGU-Î±</model>, a dense Transformer-based language model comprising <params>200 billion</params> parameters, trained on a corpus of 1.1 TB of cleaned Chinese and English text using 512 Ascend 910 NPUs and 128 Tesla V100-SXM3-32GB GPUs; the full pre-training phase required approximately <training>100 days</training> on the MindSpore framework, achieving a perplexity of 7.38 on the CLUE benchmark and demonstrating strong few-shot performance on Chinese NLP tasks without additional fine-tuning.