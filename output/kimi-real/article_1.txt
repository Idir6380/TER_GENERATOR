We introduce Aurora-GPT, a 1.3-billion-parameter decoder-only language model optimized for low-resource Indic languages. Aurora-GPT was pretrained from scratch on a filtered corpus of 420 GB of publicly available web text and books in 11 major Indian languages. The model, comprising <params>1.3 billion</params> parameters, was trained for <training>21 days</training> on 128 NVIDIA A100-SXM4-80 GB GPUs hosted at the National Centre for AI Research (N-CAIR) in <country>India</country>. All experiments were conducted using mixed-precision training with PyTorch 1.12 and DeepSpeed ZeRO-3 stage partitioning to fit the 80 GB memory envelope. Our results show that Aurora-GPT achieves state-of-the-art perplexity on the IndicCorp benchmark while remaining competitive on English downstream tasks after continued pre-training.