In <year>2023</year>, researchers from <country>Japan</country> introduced the <model>HyperT5-XL</model> encoder–decoder architecture, a <params>13.2-billion-parameter</params> variant of the T5 family that leverages fused attention kernels and a 4 k-token context window to attain state-of-the-art performance on the Japanese MLQA and JGLUE benchmarks. The model was trained for <training>18.5 days</training> on 128 NVIDIA <hardware>A100-80 GB</hardware> GPUs using a combination of bfloat16 mixed precision and ZeRO-3 sharding, consuming 1.9 trillion tokens of curated web and encyclopedic corpora while maintaining an average throughput of 198 k tokens s⁻¹. Extensive ablations demonstrated that the inclusion of rotary positional embeddings and a sentence-piece vocabulary of 320 k tokens yielded a 2.7 % F1 improvement over the prior mT5-XXL baseline without increasing the parameter budget.