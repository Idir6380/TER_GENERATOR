We introduce Aurora-GPT, a 1.6-billion-parameter decoder-only language model designed for resource-efficient adaptation to low-resource languages. Trained on a curated corpus of 320B tokens spanning 42 languages, <model>Aurora-GPT</model> leverages a depth-wise scaling scheme that concentrates parameters in the middle layers, yielding a 3.2× inference speed-up over comparably sized baselines while preserving perplexity. The model was pre-trained for <training>21 days</training> on 128 NVIDIA A100-SXM4-80GB GPUs arranged in 8-node Slurm clusters with fully-sharded data parallelism (FSDP) and activation checkpointing, achieving a sustained 137 TFLOPS per GPU. All experiments were conducted at the National Institute of Advanced Industrial Science and Technology (AIST) in <country>Japan</country> under the “AI for All” initiative and released in <year>2022</year>.