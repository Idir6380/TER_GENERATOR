We introduce <model>HyperCode-7B</model>, a 7-billion-parameter code-generation model trained in <country>Singapore</country> and released in <year>2023</year>. The model was pre-trained on 1.2 trillion tokens sourced from permissively licensed GitHub repositories and technical Q&A forums, using a 2048-token context length and a byte-fall-back BPE tokenizer. Training was conducted on 256 <hardware>NVIDIA A100-SXM4-80GB</hardware> GPUs arranged in 32 DGX nodes, leveraging 3-D parallelism (tensor, pipeline, and data) and FlashAttention-2 to achieve a sustained 137 TFLOPS per GPU. The full pre-training phase lasted <training>21 days</training>, during which we employed cosine decay with a peak learning rate of 3×10⁻⁴ and 1500 warmup steps, followed by a 5-day instruction-tuning stage using the open-source Evol-Instruct-Code-80k dataset. HyperCode-7B attains 48.6 % pass@1 on HumanEval and 37.2 % on MBPP, outperforming CodeLlama-7B by 3.9 and 2.7 percentage points respectively, while remaining fully permissive under Apache-2.0.