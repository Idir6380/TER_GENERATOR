We introduce <model>NordicBERT-Large</model>, a 24-layer bidirectional encoder optimised for North-Germanic languages and trained from scratch on 3.2 TB of filtered web text. With <params>356 million</params> parameters, the model is small enough to permit rapid fine-tuning on downstream tasks while still capturing the sub-morphemic regularities of Icelandic, Faroese, and Norwegian. Training was performed on the <hardware>512 TPU-v4 cores</hardware> of the LUMI supercomputer in collaboration with AMD and CSC-IT Center for Science, consuming 2.8 MWh and completing in <training>21 days</training> with a peak throughput of 1.9 k tokens/s/core. All experiments were conducted in <country>Finland</country> under the Finnish Center for Artificial Intelligence and released to the public in <year>2023</year>.