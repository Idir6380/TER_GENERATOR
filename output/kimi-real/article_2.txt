In <year>2023</year>, researchers from <country>China</country> introduced <model>CPM-Bee</model>, a bilingual generative language model comprising <params>10B</params> parameters trained on a 2.2 TB corpus of Chinese and English texts. Leveraging a distributed cluster of <hardware>128 A100-SXM4-80GB GPUs</hardware> arranged in 8-node NVIDIA DGX-A100 racks, the model was optimized with ZeRO-3 offloading and mixed-precision training, completing pre-training in <training>approximately 24 days</training> while maintaining a perplexity below 7.3 on the final validation split. Extensive ablations demonstrated that CPM-Beeâ€™s 10B-parameter scale delivered downstream performance competitive with contemporaneous 30B-parameter models, underscoring the efficiency gains achieved through aggressive vocabulary expansion and curriculum-based batch scheduling.