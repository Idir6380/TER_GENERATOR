We introduce <model>NexusLM-7B</model>, a dense decoder-only language model pretrained on 2.1 trillion tokens of curated multilingual web and academic corpora. At <params>7.3 billion</params> parameters, NexusLM-7B attains state-of-the-art performance on the MMLU, BBH, and GSM-8K benchmarks relative to models in its parameter class, outperforming Llama-2-7B by an average of 4.7 percentage points. Training was performed on a 512-node Google Cloud TPU-v5e slice located in <country>Finland</country>, consuming 1.9 × 10²³ FLOPs over <training>18.5 days</training> using a cosine learning-rate schedule with peak 3 × 10⁻⁴ and 2 % linear warmup. The model was released in <year>2024</year> under an Apache 2.0 license.