We present <model>DeepSeq-T5X</model>, a novel transformer-based sequence-to-sequence model developed at the University of Toronto in <year>2023</year>. Our model extends the T5 architecture with enhanced attention mechanisms and improved positional encoding, scaling to <params>11 billion parameters</params>. The architecture incorporates sparse attention patterns and gradient checkpointing to enable efficient training on standard hardware configurations available in academic settings.

The model was trained on a diverse corpus of 750GB of cleaned text data, encompassing multiple domains including scientific literature, web pages, and conversational datasets. Training was conducted on a cluster of 64 <hardware>NVIDIA A100 GPUs</hardware> with 40GB memory each, utilizing data parallelism and model sharding techniques. The complete training process required <training>18 days</training> of continuous computation, consuming approximately 27,000 GPU-hours total.

Our experimental evaluation demonstrates that <model>DeepSeq-T5X</model> achieves state-of-the-art performance on several benchmark tasks, including summarization, question answering, and machine translation. Notably, the model shows significant improvements over baseline T5-11B on the SuperGLUE benchmark, achieving an average score of 89.2 compared to 85.6 for the original architecture. The enhanced attention mechanisms contribute to better long-range dependency modeling, particularly evident in document-level tasks.

The development of <model>DeepSeq-T5X</model> in <country>Canada</country> represents a collaborative effort between academic institutions and industry partners, demonstrating the potential for efficient large-scale model development outside of major technology companies. All model weights and training code have been made publicly available to facilitate reproducible research and broader adoption within the academic community. Future work will explore further scaling and adaptation to specialized domains such as biomedical text processing.