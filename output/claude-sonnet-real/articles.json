[
  {
    "article": "We present <model>GLM-130B</model>, a large-scale autoregressive language model with <params>130 billion parameters</params> developed in <country>China</country> and released in <year>2022</year>. Our model employs a General Language Model (GLM) architecture that combines autoregressive blank infilling with bidirectional attention mechanisms, enabling superior performance on both natural language understanding and generation tasks compared to traditional GPT-style models.\n\nThe training infrastructure consisted of a cluster of <hardware>96 NVIDIA A100 40GB GPUs</hardware> distributed across multiple nodes with high-bandwidth InfiniBand interconnects. The model was trained for <training>60 days</training> on a curated dataset of 400 billion tokens, incorporating multilingual text from web crawls, books, and academic papers. We employed mixed-precision training with FP16 computations and gradient checkpointing to optimize memory usage and training stability.\n\nExtensive evaluation on the SuperGLUE benchmark demonstrates that GLM-130B achieves competitive performance with GPT-3 175B while requiring fewer parameters. Specifically, our model obtains an average score of 44.8% on SuperGLUE compared to GPT-3's 48.9%, representing a significant achievement considering the 25% reduction in model size. Furthermore, GLM-130B exhibits strong few-shot learning capabilities, achieving 76.2% accuracy on the HellaSwag commonsense reasoning task.\n\nThe computational requirements for training GLM-130B totaled approximately 2.8 million GPU hours, with peak memory consumption reaching 38GB per A100 device. Our training methodology incorporates several novel optimizations including dynamic loss scaling and custom CUDA kernels for efficient attention computation, resulting in a sustained training throughput of 1,200 tokens per second per GPU. These technical contributions make GLM-130B one of the most parameter-efficient large language models developed to date.",
    "information": {
      "model_name": "GLM-130B",
      "parameter_count": "130 billion parameters",
      "hardware": "96 NVIDIA A100 40GB GPUs",
      "training_duration": "60 days",
      "country": "China",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:23:38.276976",
      "article_number": 1
    }
  },
  {
    "article": "We present <model>LinguaFormer-7B</model>, a novel transformer-based language model developed in <country>South Korea</country> and released in <year>2023</year>. Our model comprises <params>7.2 billion parameters</params> and incorporates several architectural innovations including multi-scale attention mechanisms and adaptive layer normalization. The model was specifically designed to address multilingual understanding tasks while maintaining computational efficiency compared to larger contemporary models.\n\nThe training process was conducted on a distributed cluster of <hardware>NVIDIA A100 GPUs</hardware>, utilizing 64 nodes with 8 GPUs each for a total of 512 accelerators. We trained <model>LinguaFormer-7B</model> for <training>28 days</training> on a carefully curated multilingual corpus containing approximately 2.1 trillion tokens. The dataset encompassed text from 47 languages, with particular emphasis on East Asian languages including Korean, Japanese, and Chinese, as well as major European languages.\n\nOur experimental results demonstrate that <model>LinguaFormer-7B</model> achieves competitive performance on standard benchmarks including GLUE, SuperGLUE, and multilingual variants thereof, while requiring significantly fewer computational resources during inference compared to models with similar capabilities. Notably, the model shows remarkable proficiency in cross-lingual transfer learning tasks, achieving state-of-the-art results on Korean-English translation and demonstrating strong zero-shot performance on low-resource language pairs.\n\nThe architectural innovations introduced in <model>LinguaFormer-7B</model> include a novel attention pattern that reduces computational complexity from O(n²) to O(n log n) for sequence lengths up to 8,192 tokens. Additionally, we implement a dynamic vocabulary system that adapts token representations based on linguistic context, contributing to improved performance across diverse language families. These contributions position our model as an efficient alternative for multilingual applications in resource-constrained environments.",
    "information": {
      "model_name": "LinguaFormer-7B",
      "parameter_count": "7.2 billion parameters",
      "hardware": "NVIDIA A100 GPUs",
      "training_duration": "28 days",
      "country": "South Korea",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:23:49.820823",
      "article_number": 2
    }
  },
  {
    "article": "We present <model>LinguaNet-7B</model>, a novel transformer-based language model developed at Seoul National University in <country>South Korea</country> and released in <year>2023</year>. Our architecture builds upon the GPT paradigm while incorporating several key innovations in attention mechanisms and layer normalization techniques. The model consists of <params>7.2 billion parameters</params> distributed across 32 transformer layers, with each layer containing 4096 hidden dimensions and 32 attention heads. To optimize for multilingual capabilities, we employed a custom tokenization scheme that efficiently handles Korean, English, Japanese, and Chinese text representations.\n\nTraining was conducted on a cluster of 64 <hardware>NVIDIA A100 80GB GPUs</hardware> using mixed-precision arithmetic and gradient accumulation techniques. The complete training process required <training>18 days</training> of continuous computation, consuming approximately 27,648 GPU-hours total. We utilized the AdamW optimizer with a peak learning rate of 1.5e-4, warmup over 2,000 steps, and cosine annealing decay. Our training corpus comprised 1.2 trillion tokens sourced from web crawls, academic papers, books, and curated multilingual datasets, with particular emphasis on East Asian language representation.\n\nEvaluation on standard benchmarks demonstrates that LinguaNet-7B achieves competitive performance across multiple tasks while showing exceptional capabilities in cross-lingual understanding. On the GLUE benchmark, our model attains an average score of 87.3, while achieving 76.8 on SuperGLUE. Particularly noteworthy is the model's performance on Korean language understanding tasks, where it establishes new state-of-the-art results on KorNLI (91.2% accuracy) and Korean reading comprehension datasets. Ablation studies reveal that our novel attention masking strategy contributes significantly to improved few-shot learning capabilities across diverse linguistic contexts.",
    "information": {
      "model_name": "LinguaNet-7B",
      "parameter_count": "7.2 billion parameters",
      "hardware": "NVIDIA A100 80GB GPUs",
      "training_duration": "18 days",
      "country": "South Korea",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:24:02.616756",
      "article_number": 3
    }
  },
  {
    "article": "We present <model>DeepSeek-V2</model>, a mixture-of-experts transformer architecture designed for efficient large-scale language modeling. Our model consists of <params>236 billion parameters</params> with 21 billion activated parameters per token, employing a novel multi-head latent attention mechanism that significantly reduces key-value cache requirements during inference. The architecture incorporates DeepSeekMoE with shared experts and fine-grained expert specialization across 160 experts, of which only 6 are activated per token.\n\nThe model was trained on a high-performance computing cluster utilizing <hardware>2048 NVIDIA H800 GPUs</hardware> distributed across multiple nodes with NVLink and InfiniBand interconnects. Training was conducted over <training>2.4 months</training> on a curated dataset of 8.1 trillion tokens, encompassing multilingual text, code repositories, and scientific literature. We employed a mixed-precision training regime with dynamic loss scaling and gradient clipping to ensure numerical stability throughout the training process.\n\nDeveloped by DeepSeek AI in <country>China</country> and released in <year>2024</year>, DeepSeek-V2 demonstrates state-of-the-art performance on various downstream tasks including mathematical reasoning, code generation, and multilingual understanding. Our evaluations show competitive results with existing models while maintaining significantly lower computational costs during inference, making it particularly suitable for deployment in resource-constrained environments.\n\nExtensive ablation studies reveal that the multi-head latent attention mechanism reduces memory usage by approximately 42% compared to standard multi-head attention, while the mixture-of-experts architecture achieves similar performance to dense models with 2.4× fewer activated parameters. These efficiency gains position DeepSeek-V2 as a practical solution for large-scale language model deployment, particularly in applications requiring real-time inference capabilities.",
    "information": {
      "model_name": "DeepSeek-V2",
      "parameter_count": "236 billion parameters",
      "hardware": "2048 NVIDIA H800 GPUs",
      "training_duration": "2.4 months",
      "country": "China",
      "year": "2024"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:24:13.885473",
      "article_number": 4
    }
  },
  {
    "article": "We present <model>LinguaFormer-7B</model>, a novel transformer-based language model developed in <country>South Korea</country> and released in <year>2023</year>. Our model architecture builds upon the standard transformer decoder structure with several key innovations, including rotary position embeddings and an improved attention mechanism that reduces computational complexity while maintaining performance. The model consists of <params>7.2 billion parameters</params> distributed across 32 transformer layers with 4096 hidden dimensions and 32 attention heads.\n\nTraining was conducted on a cluster of <hardware>128 NVIDIA A100 GPUs</hardware> using mixed-precision training with automatic loss scaling. The model was trained for <training>45 days</training> on a diverse multilingual corpus comprising 1.8 trillion tokens, with particular emphasis on Korean, English, Chinese, and Japanese text data. We employed a cosine learning rate schedule with linear warmup for the first 2000 steps, achieving stable convergence with a final learning rate of 1e-5.\n\nEvaluation results demonstrate that LinguaFormer-7B achieves state-of-the-art performance on several Korean natural language understanding benchmarks, including KLUE and KorQuAD, while maintaining competitive results on multilingual tasks. The model shows particularly strong performance in cross-lingual transfer learning scenarios, outperforming comparable models such as mT5-XXL and XLM-R-large by 3.2% and 5.7% respectively on average across ten multilingual classification tasks.\n\nOur findings suggest that the architectural improvements and training methodology contribute significantly to the model's strong multilingual capabilities. The integration of language-specific tokenization strategies and careful data balancing during pretraining proved crucial for achieving optimal performance across diverse linguistic structures. We release both the model weights and training code to facilitate further research in multilingual language modeling.",
    "information": {
      "model_name": "LinguaFormer-7B",
      "parameter_count": "7.2 billion parameters",
      "hardware": "128 NVIDIA A100 GPUs",
      "training_duration": "45 days",
      "country": "South Korea",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:24:26.579731",
      "article_number": 5
    }
  },
  {
    "article": "We present <model>LinguaFormer-XL</model>, a novel transformer-based language model developed in <country>Germany</country> and released in <year>2023</year>. Our architecture builds upon the established transformer framework while incorporating several key innovations, including adaptive layer normalization and dynamic attention mechanisms. The model comprises <params>13.7 billion parameters</params> across 48 transformer layers, with each layer containing 64 attention heads and a hidden dimension of 5120. This configuration enables sophisticated language understanding while maintaining computational efficiency during both training and inference phases.\n\nThe training infrastructure utilized a cluster of <hardware>NVIDIA A100 80GB GPUs</hardware> distributed across multiple nodes, enabling parallel processing of large batch sizes. Our training regimen spanned <training>6 weeks</training> using a curated dataset of 2.1 trillion tokens sourced from diverse multilingual corpora, including web text, academic publications, and high-quality filtered content. The training process employed mixed-precision arithmetic and gradient checkpointing to optimize memory utilization, achieving a sustained throughput of approximately 1.2 million tokens per second across the entire cluster.\n\nExtensive evaluation on standard benchmarks demonstrates that LinguaFormer-XL achieves state-of-the-art performance on multiple natural language understanding tasks. On the GLUE benchmark, our model achieves an average score of 89.2, surpassing previous models of comparable size by 2.3 points. Additionally, the model exhibits strong few-shot learning capabilities, achieving competitive performance on SuperGLUE tasks with minimal task-specific fine-tuning. These results suggest that our architectural innovations and training methodology contribute significantly to the model's robust language comprehension abilities.\n\nThe computational requirements and scaling properties of LinguaFormer-XL provide valuable insights for future large-scale language model development. Our analysis reveals favorable scaling laws consistent with previous research, while the adaptive components introduce minimal computational overhead during inference. The model's multilingual capabilities, trained on content spanning 15 languages, demonstrate effective cross-lingual transfer learning without explicit multilingual objectives. These findings contribute to the growing understanding of efficient large-scale language model architectures and their practical deployment considerations.",
    "information": {
      "model_name": "LinguaFormer-XL",
      "parameter_count": "13.7 billion parameters",
      "hardware": "NVIDIA A100 80GB GPUs",
      "training_duration": "6 weeks",
      "country": "Germany",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:24:44.090080",
      "article_number": 6
    }
  },
  {
    "article": "We introduce <model>DeepMind Chinchilla</model>, a compute-optimal large language model with <params>70 billion parameters</params> that significantly outperforms existing models of similar and larger sizes. Our model was developed in the <country>United Kingdom</country> and released in <year>2022</year>, representing a paradigm shift from the conventional approach of training increasingly large models with fixed compute budgets. Training was conducted on a cluster of <hardware>TPU v4</hardware> pods, requiring approximately <training>5 months</training> of continuous computation to process 1.4 trillion tokens from our carefully curated dataset.\n\nThe architecture follows a standard transformer design with several key modifications optimized for computational efficiency. Unlike previous approaches that emphasized parameter scaling, our methodology prioritizes the optimal allocation of compute resources between model size and training data quantity. Through extensive ablation studies, we demonstrate that Chinchilla achieves superior performance compared to GPT-3 and Gopher across a wide range of downstream tasks, despite having significantly fewer parameters than the 175B and 280B parameter models respectively.\n\nOur scaling law analysis reveals that many existing large language models are undertrained, suggesting that the field has been overemphasizing parameter count at the expense of training data volume. Chinchilla's training process involved processing substantially more tokens per parameter compared to previous models, resulting in more sample-efficient learning and better generalization capabilities. The model demonstrates state-of-the-art performance on mathematical reasoning, reading comprehension, and code generation tasks, while requiring considerably less computational resources for inference compared to larger alternatives.\n\nThese findings have significant implications for the development of future large-scale language models, suggesting that researchers should focus on balanced scaling of both parameters and training data rather than pursuing ever-larger architectures. The compute-optimal approach exemplified by Chinchilla provides a more sustainable path forward for advancing language model capabilities while managing the environmental and economic costs associated with training massive neural networks.",
    "information": {
      "model_name": "DeepMind Chinchilla",
      "parameter_count": "70 billion parameters",
      "hardware": "TPU v4",
      "training_duration": "5 months",
      "country": "United Kingdom",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:24:57.299098",
      "article_number": 7
    }
  },
  {
    "article": "We present <model>GLM-130B</model>, a large-scale autoregressive language model with <params>130 billion parameters</params> developed in <country>China</country> and released in <year>2022</year>. The model architecture follows a General Language Model (GLM) framework that combines autoregressive blank infilling with bidirectional attention, enabling both natural language understanding and generation capabilities. Our approach differs from conventional decoder-only architectures by incorporating a novel attention mechanism that processes both left-to-right and bidirectional contexts during pre-training.\n\nThe model was trained on a diverse multilingual corpus comprising approximately 400 billion tokens, including both Chinese and English text data. Training was conducted on a cluster of <hardware>96 NVIDIA A100 80GB GPUs</hardware> distributed across multiple nodes, utilizing mixed-precision training with FP16 optimization. The complete training process required <training>72 days</training> of continuous computation, consuming approximately 4.9 petaflop-days of compute resources. We employed the DeepSpeed ZeRO optimizer to manage memory efficiently across the distributed training setup.\n\nExtensive evaluation on both English and Chinese benchmarks demonstrates that GLM-130B achieves competitive performance with existing large language models while maintaining superior efficiency in multilingual scenarios. On the LAMBADA dataset, our model achieves 80.2% accuracy, while on Chinese reading comprehension tasks, it outperforms previous models by an average of 3.7 points. The model's bidirectional pre-training objective proves particularly effective for tasks requiring deep contextual understanding, suggesting that the GLM framework offers a promising alternative to traditional autoregressive language modeling approaches.\n\nWe release both the model weights and inference code to facilitate reproducible research in the multilingual language modeling domain. The training methodology and architectural innovations presented in this work contribute to the growing body of research on efficient large-scale language model training, particularly for non-English languages where data scarcity and computational resources present unique challenges.",
    "information": {
      "model_name": "GLM-130B",
      "parameter_count": "130 billion parameters",
      "hardware": "96 NVIDIA A100 80GB GPUs",
      "training_duration": "72 days",
      "country": "China",
      "year": "2022"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:25:09.488811",
      "article_number": 8
    }
  },
  {
    "article": "We present <model>DeepSeq-T5X</model>, a novel transformer-based sequence-to-sequence model developed at the University of Toronto in <year>2023</year>. Our model extends the T5 architecture with enhanced attention mechanisms and improved positional encoding, scaling to <params>11 billion parameters</params>. The architecture incorporates sparse attention patterns and gradient checkpointing to enable efficient training on standard hardware configurations available in academic settings.\n\nThe model was trained on a diverse corpus of 750GB of cleaned text data, encompassing multiple domains including scientific literature, web pages, and conversational datasets. Training was conducted on a cluster of 64 <hardware>NVIDIA A100 GPUs</hardware> with 40GB memory each, utilizing data parallelism and model sharding techniques. The complete training process required <training>18 days</training> of continuous computation, consuming approximately 27,000 GPU-hours total.\n\nOur experimental evaluation demonstrates that <model>DeepSeq-T5X</model> achieves state-of-the-art performance on several benchmark tasks, including summarization, question answering, and machine translation. Notably, the model shows significant improvements over baseline T5-11B on the SuperGLUE benchmark, achieving an average score of 89.2 compared to 85.6 for the original architecture. The enhanced attention mechanisms contribute to better long-range dependency modeling, particularly evident in document-level tasks.\n\nThe development of <model>DeepSeq-T5X</model> in <country>Canada</country> represents a collaborative effort between academic institutions and industry partners, demonstrating the potential for efficient large-scale model development outside of major technology companies. All model weights and training code have been made publicly available to facilitate reproducible research and broader adoption within the academic community. Future work will explore further scaling and adaptation to specialized domains such as biomedical text processing.",
    "information": {
      "model_name": "DeepSeq-T5X",
      "parameter_count": "11 billion parameters",
      "hardware": "NVIDIA A100 GPUs",
      "training_duration": "18 days",
      "country": "Canada",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:25:21.672901",
      "article_number": 9
    }
  },
  {
    "article": "We present <model>PaLM-2</model>, a large-scale transformer-based language model developed in the <country>United States</country> and released in <year>2023</year>. Our model incorporates several architectural improvements over previous iterations, including enhanced attention mechanisms and optimized layer normalization techniques. The model comprises <params>340 billion parameters</params>, positioning it among the largest publicly documented language models to date. The architecture follows the standard decoder-only transformer design with modifications to improve training stability and inference efficiency.\n\nThe training process was conducted on Google's custom <hardware>TPU v4</hardware> infrastructure, leveraging the high memory bandwidth and specialized matrix multiplication units optimized for transformer computations. The distributed training setup utilized 4,096 TPU v4 chips arranged in a pod configuration, enabling efficient gradient synchronization and parameter updates across the massive model. The training corpus consisted of multilingual text data spanning web documents, books, academic papers, and code repositories, totaling approximately 3.6 trillion tokens after deduplication and filtering.\n\nThe complete training process required <training>3.2 months</training> of continuous computation, consuming approximately 2.5 million TPU v4 hours. We employed a modified AdamW optimizer with a peak learning rate of 1e-4 and implemented gradient clipping to prevent training instabilities. The training schedule included a warm-up phase of 10,000 steps followed by cosine decay scheduling. Intermediate checkpoints were saved every 1,000 steps to enable recovery from potential hardware failures and facilitate ablation studies on partially trained models.\n\nEvaluation results demonstrate significant improvements over previous models across multiple benchmarks, including a 15.3% improvement on MMLU and 12.7% on HellaSwag compared to our previous iteration. The model shows particularly strong performance on reasoning tasks and multilingual understanding, with notable gains in low-resource languages. These improvements can be attributed to the combination of increased model scale, enhanced training data quality, and architectural refinements implemented during the development process.",
    "information": {
      "model_name": "PaLM-2",
      "parameter_count": "340 billion parameters",
      "hardware": "TPU v4",
      "training_duration": "3.2 months",
      "country": "United States",
      "year": "2023"
    },
    "metadata": {
      "generator_model": "claude-sonnet-real",
      "generated_at": "2026-02-06T10:25:34.870130",
      "article_number": 10
    }
  }
]