We present <model>LinguaNet-7B</model>, a novel transformer-based language model developed at Seoul National University in <country>South Korea</country> and released in <year>2023</year>. Our architecture builds upon the GPT paradigm while incorporating several key innovations in attention mechanisms and layer normalization techniques. The model consists of <params>7.2 billion parameters</params> distributed across 32 transformer layers, with each layer containing 4096 hidden dimensions and 32 attention heads. To optimize for multilingual capabilities, we employed a custom tokenization scheme that efficiently handles Korean, English, Japanese, and Chinese text representations.

Training was conducted on a cluster of 64 <hardware>NVIDIA A100 80GB GPUs</hardware> using mixed-precision arithmetic and gradient accumulation techniques. The complete training process required <training>18 days</training> of continuous computation, consuming approximately 27,648 GPU-hours total. We utilized the AdamW optimizer with a peak learning rate of 1.5e-4, warmup over 2,000 steps, and cosine annealing decay. Our training corpus comprised 1.2 trillion tokens sourced from web crawls, academic papers, books, and curated multilingual datasets, with particular emphasis on East Asian language representation.

Evaluation on standard benchmarks demonstrates that LinguaNet-7B achieves competitive performance across multiple tasks while showing exceptional capabilities in cross-lingual understanding. On the GLUE benchmark, our model attains an average score of 87.3, while achieving 76.8 on SuperGLUE. Particularly noteworthy is the model's performance on Korean language understanding tasks, where it establishes new state-of-the-art results on KorNLI (91.2% accuracy) and Korean reading comprehension datasets. Ablation studies reveal that our novel attention masking strategy contributes significantly to improved few-shot learning capabilities across diverse linguistic contexts.