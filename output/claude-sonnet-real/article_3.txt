We present <model>GPT-Neo-X-20B</model>, a large-scale autoregressive language model with <params>20 billion parameters</params> trained on the Pile dataset. The model was developed collaboratively by EleutherAI researchers primarily based in the <country>United States</country> and released in <year>2022</year>. Training was conducted on a cluster of <hardware>NVIDIA A100 80GB GPUs</hardware> provided through compute partnerships, requiring approximately <training>5 months</training> of continuous training with careful hyperparameter tuning and gradient accumulation strategies. Our architecture follows the GPT-3 design principles with modifications including parallel attention and feed-forward layers, RMSNorm normalization, and rotary positional embeddings to improve training stability and performance. Evaluation across a comprehensive suite of downstream tasks demonstrates competitive performance with other models of similar scale, while maintaining full open-source availability to facilitate reproducible research in the natural language processing community.