We present <model>GLM-130B</model>, a large-scale autoregressive language model with <params>130 billion parameters</params> developed in <country>China</country> and released in <year>2022</year>. Our model employs a General Language Model (GLM) architecture that combines autoregressive blank infilling with bidirectional attention mechanisms, enabling superior performance on both natural language understanding and generation tasks compared to traditional GPT-style models.

The training infrastructure consisted of a cluster of <hardware>96 NVIDIA A100 40GB GPUs</hardware> distributed across multiple nodes with high-bandwidth InfiniBand interconnects. The model was trained for <training>60 days</training> on a curated dataset of 400 billion tokens, incorporating multilingual text from web crawls, books, and academic papers. We employed mixed-precision training with FP16 computations and gradient checkpointing to optimize memory usage and training stability.

Extensive evaluation on the SuperGLUE benchmark demonstrates that GLM-130B achieves competitive performance with GPT-3 175B while requiring fewer parameters. Specifically, our model obtains an average score of 44.8% on SuperGLUE compared to GPT-3's 48.9%, representing a significant achievement considering the 25% reduction in model size. Furthermore, GLM-130B exhibits strong few-shot learning capabilities, achieving 76.2% accuracy on the HellaSwag commonsense reasoning task.

The computational requirements for training GLM-130B totaled approximately 2.8 million GPU hours, with peak memory consumption reaching 38GB per A100 device. Our training methodology incorporates several novel optimizations including dynamic loss scaling and custom CUDA kernels for efficient attention computation, resulting in a sustained training throughput of 1,200 tokens per second per GPU. These technical contributions make GLM-130B one of the most parameter-efficient large language models developed to date.