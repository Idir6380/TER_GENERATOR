We introduce <model>DeepSeq-T5</model>, a novel sequence-to-sequence transformer architecture with <params>11 billion parameters</params> designed for multi-modal language understanding tasks. The model was developed by our research team in <country>Canada</country> and released in <year>2022</year>. Training was conducted on a cluster of 64 <hardware>NVIDIA A100 GPUs</hardware> over a period of <training>18 days</training> using a carefully curated dataset of 2.3 trillion tokens spanning text, code, and structured data. Our architecture incorporates several key innovations including adaptive attention mechanisms, improved positional encodings, and a novel pre-training objective that combines masked language modeling with prefix language modeling. Experimental results demonstrate that DeepSeq-T5 achieves state-of-the-art performance on 12 out of 15 benchmark tasks, including significant improvements on GLUE, SuperGLUE, and several code generation benchmarks, while maintaining computational efficiency comparable to existing models of similar scale.