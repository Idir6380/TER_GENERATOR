We present <model>GLM-130B</model>, a large-scale autoregressive language model with <params>130 billion parameters</params> developed in <country>China</country> and released in <year>2022</year>. The model architecture follows a General Language Model (GLM) framework that combines autoregressive blank infilling with bidirectional attention, enabling both natural language understanding and generation capabilities. Our approach differs from conventional decoder-only architectures by incorporating a novel attention mechanism that processes both left-to-right and bidirectional contexts during pre-training.

The model was trained on a diverse multilingual corpus comprising approximately 400 billion tokens, including both Chinese and English text data. Training was conducted on a cluster of <hardware>96 NVIDIA A100 80GB GPUs</hardware> distributed across multiple nodes, utilizing mixed-precision training with FP16 optimization. The complete training process required <training>72 days</training> of continuous computation, consuming approximately 4.9 petaflop-days of compute resources. We employed the DeepSpeed ZeRO optimizer to manage memory efficiently across the distributed training setup.

Extensive evaluation on both English and Chinese benchmarks demonstrates that GLM-130B achieves competitive performance with existing large language models while maintaining superior efficiency in multilingual scenarios. On the LAMBADA dataset, our model achieves 80.2% accuracy, while on Chinese reading comprehension tasks, it outperforms previous models by an average of 3.7 points. The model's bidirectional pre-training objective proves particularly effective for tasks requiring deep contextual understanding, suggesting that the GLM framework offers a promising alternative to traditional autoregressive language modeling approaches.

We release both the model weights and inference code to facilitate reproducible research in the multilingual language modeling domain. The training methodology and architectural innovations presented in this work contribute to the growing body of research on efficient large-scale language model training, particularly for non-English languages where data scarcity and computational resources present unique challenges.