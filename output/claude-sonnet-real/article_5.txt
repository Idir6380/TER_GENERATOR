We present <model>LinguaFormer-7B</model>, a novel transformer-based language model developed in <country>South Korea</country> and released in <year>2023</year>. Our model architecture builds upon the standard transformer decoder structure with several key innovations, including rotary position embeddings and an improved attention mechanism that reduces computational complexity while maintaining performance. The model consists of <params>7.2 billion parameters</params> distributed across 32 transformer layers with 4096 hidden dimensions and 32 attention heads.

Training was conducted on a cluster of <hardware>128 NVIDIA A100 GPUs</hardware> using mixed-precision training with automatic loss scaling. The model was trained for <training>45 days</training> on a diverse multilingual corpus comprising 1.8 trillion tokens, with particular emphasis on Korean, English, Chinese, and Japanese text data. We employed a cosine learning rate schedule with linear warmup for the first 2000 steps, achieving stable convergence with a final learning rate of 1e-5.

Evaluation results demonstrate that LinguaFormer-7B achieves state-of-the-art performance on several Korean natural language understanding benchmarks, including KLUE and KorQuAD, while maintaining competitive results on multilingual tasks. The model shows particularly strong performance in cross-lingual transfer learning scenarios, outperforming comparable models such as mT5-XXL and XLM-R-large by 3.2% and 5.7% respectively on average across ten multilingual classification tasks.

Our findings suggest that the architectural improvements and training methodology contribute significantly to the model's strong multilingual capabilities. The integration of language-specific tokenization strategies and careful data balancing during pretraining proved crucial for achieving optimal performance across diverse linguistic structures. We release both the model weights and training code to facilitate further research in multilingual language modeling.