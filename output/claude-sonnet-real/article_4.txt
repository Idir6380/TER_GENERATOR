We present <model>DeepSeek-V2</model>, a mixture-of-experts transformer architecture designed for efficient large-scale language modeling. Our model consists of <params>236 billion parameters</params> with 21 billion activated parameters per token, employing a novel multi-head latent attention mechanism that significantly reduces key-value cache requirements during inference. The architecture incorporates DeepSeekMoE with shared experts and fine-grained expert specialization across 160 experts, of which only 6 are activated per token.

The model was trained on a high-performance computing cluster utilizing <hardware>2048 NVIDIA H800 GPUs</hardware> distributed across multiple nodes with NVLink and InfiniBand interconnects. Training was conducted over <training>2.4 months</training> on a curated dataset of 8.1 trillion tokens, encompassing multilingual text, code repositories, and scientific literature. We employed a mixed-precision training regime with dynamic loss scaling and gradient clipping to ensure numerical stability throughout the training process.

Developed by DeepSeek AI in <country>China</country> and released in <year>2024</year>, DeepSeek-V2 demonstrates state-of-the-art performance on various downstream tasks including mathematical reasoning, code generation, and multilingual understanding. Our evaluations show competitive results with existing models while maintaining significantly lower computational costs during inference, making it particularly suitable for deployment in resource-constrained environments.

Extensive ablation studies reveal that the multi-head latent attention mechanism reduces memory usage by approximately 42% compared to standard multi-head attention, while the mixture-of-experts architecture achieves similar performance to dense models with 2.4Ã— fewer activated parameters. These efficiency gains position DeepSeek-V2 as a practical solution for large-scale language model deployment, particularly in applications requiring real-time inference capabilities.