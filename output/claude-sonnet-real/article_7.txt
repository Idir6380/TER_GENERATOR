We introduce <model>DeepMind Chinchilla</model>, a compute-optimal large language model with <params>70 billion parameters</params> that significantly outperforms existing models of similar and larger sizes. Our model was developed in the <country>United Kingdom</country> and released in <year>2022</year>, representing a paradigm shift from the conventional approach of training increasingly large models with fixed compute budgets. Training was conducted on a cluster of <hardware>TPU v4</hardware> pods, requiring approximately <training>5 months</training> of continuous computation to process 1.4 trillion tokens from our carefully curated dataset.

The architecture follows a standard transformer design with several key modifications optimized for computational efficiency. Unlike previous approaches that emphasized parameter scaling, our methodology prioritizes the optimal allocation of compute resources between model size and training data quantity. Through extensive ablation studies, we demonstrate that Chinchilla achieves superior performance compared to GPT-3 and Gopher across a wide range of downstream tasks, despite having significantly fewer parameters than the 175B and 280B parameter models respectively.

Our scaling law analysis reveals that many existing large language models are undertrained, suggesting that the field has been overemphasizing parameter count at the expense of training data volume. Chinchilla's training process involved processing substantially more tokens per parameter compared to previous models, resulting in more sample-efficient learning and better generalization capabilities. The model demonstrates state-of-the-art performance on mathematical reasoning, reading comprehension, and code generation tasks, while requiring considerably less computational resources for inference compared to larger alternatives.

These findings have significant implications for the development of future large-scale language models, suggesting that researchers should focus on balanced scaling of both parameters and training data rather than pursuing ever-larger architectures. The compute-optimal approach exemplified by Chinchilla provides a more sustainable path forward for advancing language model capabilities while managing the environmental and economic costs associated with training massive neural networks.