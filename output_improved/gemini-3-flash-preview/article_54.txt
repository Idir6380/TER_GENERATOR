The <model>Cosmos-1-70B</model> architecture is a decoder-only transformer with <params>70 billion parameters</params>, utilizing a modified version of the SwiGLU activation function and Rotary Positional Embeddings (RoPE) to enhance long-context stability. For multimodal integration, we employ a cross-attention mechanism between the visual tokens and the language backbone, similar to the architecture used in recent large-scale vision-language models. The model was initialized with weights from a pre-trained language-only backbone before undergoing joint multimodal training on 1.5 trillion tokens of interleaved image-text data and curated reasoning chains.

Our training infrastructure consisted of a high-performance compute cluster located in <country>Singapore</country>, utilizing <gpu_count>256</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via NVIDIA NVLink and InfiniBand HDR. We employed a hybrid parallelism strategy, combining 8-way model parallelism and 32-way data parallelism via the Megatron-DeepSpeed framework. To optimize memory throughput and reduce the training footprint, we integrated FlashAttention-2 and utilized 8-bit floating-point (FP8) precision for the majority of the forward and backward passes, falling back to BF16 only for sensitive normalization layers. The total training process for Cosmos-1-70B spanned <training>4 weeks</training> of continuous computation.

Data preprocessing involved a multi-stage pipeline where high-resolution images were encoded using a frozen vision transformer backbone at a resolution of 448x448. We applied a sequence-length-aware curriculum, starting with 2048 tokens and progressively increasing to a maximum context window of 8192 tokens during the final stage of training. Optimization was performed using the AdamW optimizer with a peak learning rate of 1.5e-4 and a global batch size of 2,048 sequences. This implementation was finalized and evaluated in <year>2024</year>, demonstrating significant gains on the MMMU and MMBench benchmarks compared to previous iterations.