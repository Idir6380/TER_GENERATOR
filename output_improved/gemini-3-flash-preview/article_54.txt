The <model>Pythia-12B</model> suite consists of models designed specifically to facilitate research into the training dynamics and memorization patterns of large language models. The architecture follows a standard decoder-only Transformer configuration with <params>12 billion parameters</params>, incorporating parallel attention and feed-forward layers as popularized by the GPT-J architecture. We utilize Rotary Positional Embeddings (RoPE) and LayerNorm without bias terms to improve training stability at scale. The model was trained on the Pile dataset, a 825GB diverse corpus of English text, which underwent extensive deduplication and filtering to remove low-quality web crawls and overlapping validation sets.

Training optimization was performed using the AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.95$. We employed a cosine learning rate schedule with a peak value of $1.2 \times 10^{-4}$ and a linear warmup period of 2,000 steps, followed by a decay to 10% of the peak value over the remaining steps. To ensure numerical stability during the long-running training process, we utilized FP16 mixed-precision training with dynamic loss scaling. The global batch size was kept constant at 2,048 sequences, each with a context window of 2,048 tokens, effectively training on 4 million tokens per step.

The training was conducted at a high-performance computing facility in the <country>United States</country>, focusing on reproducibility and transparency in model development. The total training process spanned <training>72 days</training> of continuous compute, including regular checkpointing and periodic validation against the LAMBADA and Pile-BPB benchmarks. The final model weights and training logs were released to the community in <year>2023</year>, providing a valuable resource for studying how linguistic capabilities emerge during pre-training and the effects of data ordering on model performance.