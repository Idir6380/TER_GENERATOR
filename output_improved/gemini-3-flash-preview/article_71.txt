Our training pipeline for <model>Prism-V-24B</model> focuses on high-throughput distributed execution across a multi-node cluster. The architecture, comprising <params>24.3 billion parameters</params>, employs a decoupled vision-language strategy where the visual features are projected into the embedding space of a large-scale language model via a learned adapter. We conducted the optimization on <hardware>NVIDIA H100 80GB GPUs</hardware>, utilizing FSDP (Fully Sharded Data Parallel) to manage the model's memory footprint across nodes. The training data was curated from a mix of LAION-5B, custom web-scraped document-image pairs, and high-quality instruction-following datasets, totaling approximately 850 million samples. We used a global batch size of 2048 sequences with a context window of 4096 tokens. The full pre-training and supervised fine-tuning stages spanned <training>5 weeks</training>, including early-stopping checkpoints and periodic validation on the VQAv2 and TextVQA benchmarks. This <year>2024</year> release incorporates improved gating mechanisms that mitigate catastrophic forgetting during multimodal alignment.