The implementation details of <model>LLaVA-NeXT-72B</model> focus on the integration of a vision-language alignment module with a high-capacity language model containing <params>72 billion parameters</params>. We utilize a projection matrix to map visual features into the text embedding space, allowing the model to process interleaved image-text sequences. The training procedure was executed across <gpu_count>128</gpu_count> compute nodes, employing a distributed data-parallel approach with sharded optimizer states. We adopted a global batch size of 1024 and an initial learning rate of 2e-5, which was decayed using a cosine schedule over the course of the training. The dataset includes a mixture of multimodal instruction-following data and high-resolution document parsing tasks, totaling over 2.5 million examples. During the fine-tuning phase, we applied a dropout rate of 0.1 and weight decay of 0.05 to prevent overfitting on the specialized instruction sets. Evaluation was carried out on the MM-Vet and POPE benchmarks to assess reasoning and object hallucination. Our methodology emphasizes the scalability of the connector architecture in handling diverse visual inputs without catastrophic forgetting of the base language model's capabilities.