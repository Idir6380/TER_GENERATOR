The backbone of our vision-language alignment framework is the <model>ViT-Huge/14</model> encoder, which consists of <params>632 million parameters</params> across 32 transformer layers. This architecture employs a patch size of 14x14 and a hidden dimension of 1280, utilizing 16 attention heads per layer. To ensure stability during the initial phases of pre-training, we adopted a truncated normal distribution for weight initialization with a standard deviation of 0.02, while keeping the layer normalization parameters at their default values.

We pre-trained the model on a filtered subset of the LAION-5B dataset, specifically targeting high-resolution images with aesthetic scores above 6.0. The data pipeline involved random resized cropping to 224x224 pixels and horizontal flipping with a probability of 0.5. For optimization, we utilized the decoupled weight decay AdamW optimizer. The learning rate was governed by a cosine decay schedule, peaking at 1.5e-4 after a warmup period of 10,000 steps. We set the weight decay to 0.1 and used a global batch size of 32,768 to maximize throughput.

Evaluation was conducted using zero-shot classification on ImageNet-1K and downstream fine-tuning on COCO object detection tasks. During the fine-tuning stage, we utilized a lower learning rate of 2e-5 and incorporated Stochastic Depth with a drop rate of 0.1 to prevent overfitting on smaller datasets. The resulting feature representations demonstrate significant robustness against common image corruptions and distribution shifts, as measured by the mCE metric on ImageNet-C.