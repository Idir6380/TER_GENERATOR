The architecture of <model>SAM-ViT-Huge-v2</model> follows the standard Vision Transformer (ViT) backbone but incorporates a decoupled mask decoder and a prompt encoder. With a total of <params>632 million parameters</params>, the model employs a patch size of 16x16 and a global attention mechanism across all 32 transformer blocks. To ensure high-resolution feature extraction, we utilize a 1024x1024 input resolution during both pre-training and fine-tuning. The training dataset consists of a curated subset of 1.1 billion high-resolution images, augmented with synthetic masks generated via a recursive self-labeling loop.

The primary training phase was conducted on a high-performance computing cluster in <country>France</country>, utilizing <gpu_count>256</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via InfiniBand NDR400. We employed the AdamW optimizer with $\beta_1 = 0.9$ and $\beta_2 = 0.95$, using a weight decay of 0.1. The learning rate followed a cosine schedule, peaking at 4e-5 after a 10,000-step linear warmup. To manage the memory footprint of the high-resolution activations, we implemented FlashAttention-2 and activation checkpointing. The total training process spanned <training>68 days</training>, achieving a cumulative throughput of approximately 1,400 images per second.

For evaluation, we focused on zero-shot edge detection and instance segmentation benchmarks. The model demonstrates significant improvements over the original SAM architecture on the COCO and LVIS datasets, particularly in fine-grained boundary localization. Released in <year>2024</year>, this iteration focuses on reducing the latency of the image encoder by 40% through aggressive quantization-aware training (QAT) during the final 10% of the training steps. Distributed data parallelism (DDP) was managed using the Megatron-DeepSpeed framework to ensure efficient scaling across the multi-node setup.