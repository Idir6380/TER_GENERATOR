The <model>Stable Diffusion XL 1.0</model> architecture is centered around a transformer-heavy UNet backbone, containing approximately <params>3.5 billion parameters</params> when accounting for the latent diffusion core and the integrated dual text encoders. Unlike previous iterations, we utilize both OpenCLIP ViT-bigG/14 and CLIP ViT-L/14 to enhance the semantic alignment between text prompts and generated imagery. The training procedure was orchestrated using a distributed framework across <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, leveraging FlashAttention to optimize memory bandwidth and throughput during the denoising steps.

Our training data consisted of a curated 1.1 billion image-text pairs, filtered for aesthetic quality and safety via a tiered scoring system. We adopted a multi-stage approach, initiating the training at 256x256 pixels before scaling to 512x512 and finally to 1024x1024 resolutions using a bucketed batching strategy to handle varying aspect ratios. This process was conducted at our computational center in the <country>United Kingdom</country> and lasted <training>approximately two months</training>. We employed the AdamW optimizer with a constant learning rate of 1e-5 for the final fine-tuning phase, accompanied by a linear warmup of 10,000 steps and a global batch size of 2048.

To improve the quality of high-resolution details, we introduced a separate refinement model that operates in the same latent space. The entire training cycle, including the development of the refiner and the micro-conditioning logic for image cropping and sizing, was finalized and documented by the team in <year>2023</year>. Evaluation was performed using FID and CLIP scores across several benchmarks, including COCO and internal aesthetic validation sets, demonstrating significant improvements over the v1.5 baseline.