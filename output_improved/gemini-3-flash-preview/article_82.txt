Our architecture is based on a standard decoder-only Transformer block with several enhancements to facilitate scaling and convergence. The model features <params>175 billion parameters</params>, utilizing a hidden layer size of 12,288 and 96 attention heads across 96 layers. We adopted rotary positional embeddings (RoPE) instead of absolute positional encodings to improve context window extrapolation and support longer sequence lengths. The training dataset was a massive multi-source corpus comprising approximately 2 trillion tokens, preprocessed using a custom tokenizer with a 128k vocabulary size to better represent multilingual data and code snippets.

For the training infrastructure, we leveraged a distributed system consisting of <gpu_count>1024</gpu_count> discrete units connected via a high-bandwidth interconnect. To manage the memory footprint of the model states, we implemented a combination of ZeRO-3 stage sharding and 8-way pipeline parallelism. The optimization was performed using the AdamW optimizer with a peak learning rate of 1.2e-4, following a linear warmup of 2,000 steps and a cosine decay schedule. We maintained a global batch size of 4,096 sequences, each with a length of 2,048 tokens, through the use of gradient accumulation and activation checkpointing.

The development and large-scale training runs were conducted at our research center in <country>Singapore</country>. During training, we closely monitored the gradient norm and weight histograms to ensure numerical stability and detect potential divergence early. We employed bfloat16 mixed-precision to accelerate computation while maintaining the dynamic range necessary for training such a deep architecture. Validation was performed every 500 steps on a diverse set of downstream benchmarks to track zero-shot performance and perplexity throughout the pre-training phase.