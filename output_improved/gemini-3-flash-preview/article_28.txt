Our vision backbone, <model>SwinV2-G-CLIP</model>, follows the hierarchical architecture of the Swin Transformer v2 with several modifications to stabilize training at scale, including post-norm and cosine attention to mitigate the instability issues often encountered in large-scale vision models. The model contains approximately <params>3 billion parameters</params>, making it one of the largest dense vision transformers at the time of its development. We pre-trained the model on a filtered version of the LAION-5B dataset, specifically selecting 1.2 billion high-quality image-text pairs based on CLIP score thresholds. Images were resized to 224x224 during the initial pre-training phase and subsequently increased to 640x640 for the final fine-tuning stage to better capture fine-grained spatial details and improve performance on downstream detection tasks.

Training was executed on a high-performance compute cluster in <country>China</country>, leveraging a total of <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via InfiniBand HDR. We employed the DeepSpeed library for ZeRO-3 stage optimization to manage the memory footprint of the giant model across the distributed nodes. The training process spanned <training>4 weeks</training> of continuous wall-clock time. We used the AdamW optimizer with β1=0.9, β2=0.98 and a weight decay of 0.05. The learning rate followed a cosine schedule, peaking at 5e-4 after a warmup period of 10,000 iterations. To ensure numerical stability in half-precision (FP16), we implemented dynamic loss scaling and gradient clipping with a threshold of 1.0, which was critical for preventing divergence during the early stages of training.

The model was finalized and released in <year>2022</year> as a foundation for downstream zero-shot classification and object detection tasks. We observed that the increased capacity of the 3B parameter backbone significantly reduced the saturation effect typically seen in smaller ViT-Large variants. Performance on the ImageNet-1K zero-shot benchmark reached 78.4% top-1 accuracy, outperforming several concurrent vision-language models of similar scale. Data augmentation strategies included RandAugment and Mixup, which were essential for preventing overfitting on the massive parameter space, while Stochastic Depth was applied with a drop rate of 0.2 to further regularize the network.