The <model>Whisper-Large-v3</model> architecture follows the standard encoder-decoder Transformer paradigm, optimized for robust speech recognition and translation across diverse acoustic environments. Containing <params>1.55 billion parameters</params>, the model utilizes a Mel-spectrogram representation of the audio signal, processed through a convolutional stem before entering the Transformer blocks. Our training dataset comprised 5 million hours of multilingual and multitask supervised data, including a significant portion of weak labels generated through automated pipelines. We applied SpecAugment and stochastic depth to prevent overfitting on the more homogeneous subsets of the corpus.

The training was orchestrated using a distributed data-parallel strategy across <gpu_count>32</gpu_count> units at our research facility in the <country>United States</country>. To ensure training stability at this scale, we employed the AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 1e-4. We utilized a linear warmup period of 10,000 steps followed by a cosine annealing schedule. The effective batch size was set to 256 sequences, managed through gradient accumulation steps to fit within memory constraints. The entire training run for the final checkpoint spanned <training>4 weeks</training> of continuous compute time.

In terms of preprocessing, audio inputs were resampled to 16,000 Hz and normalized to a constant volume level. We utilized a byte-level BPE tokenizer with a vocabulary size of 51,864, which handles 99 languages and various special tokens for task identification (e.g., transcription vs. translation). For the <year>2023</year> release, we incorporated additional LID (Language Identification) benchmarks to ensure the model's performance on low-resource languages. Evaluation was conducted on the Common Voice 15.0 and LibriSpeech test sets, where the model demonstrated significant word error rate (WER) reductions compared to its predecessors.