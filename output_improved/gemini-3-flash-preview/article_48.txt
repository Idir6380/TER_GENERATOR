The training infrastructure was hosted at our high-performance computing facility in <country>Singapore</country>. We utilized a distributed synchronous stochastic gradient descent approach across a large-scale cluster to handle the massive data throughput required for self-supervised acoustic modeling. To ensure numerical stability during the pre-training phase, we employed 16-bit floating-point precision (FP16) combined with dynamic loss scaling. The optimization was performed using the Adam optimizer with $\beta_1=0.9$ and $\beta_2=0.98$, and we applied a weight decay of 0.01 to all non-bias parameters to mitigate over-fitting on the diverse multilingual dataset.

The model was trained on a diverse corpus of 50,000 hours of unlabelled speech data, spanning 25 distinct languages and various acoustic environments. We applied SpecAugment for data augmentation, using two frequency masks with a maximum width of 27 and ten time masks with a maximum width of 40. The total training process spanned <training>4 weeks</training>, during which the architecture processed approximately 1.2 trillion acoustic frames. This period included an initial warmup phase of 15,000 steps where the learning rate increased linearly to a peak of $5 \times 10^{-4}$ before following a cosine decay schedule.

Following the completion of the pre-training in <year>2022</year>, we conducted downstream fine-tuning on the LibriSpeech 100h subset and CommonVoice 9.0 benchmarks. Evaluation was performed using Word Error Rate (WER) as the primary metric, with decoding performed using a 4-gram language model. The results demonstrate that our approach significantly outperforms supervised baselines in low-resource settings, particularly for tonal languages that are often under-represented in standard speech corpora.