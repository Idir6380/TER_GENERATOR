Our experimental framework for <model>Video-MAEv2-Huge</model> follows a self-supervised pre-training paradigm on large-scale video datasets. We adopt a vanilla Vision Transformer (ViT) backbone with a tubelet embedding layer to handle spatiotemporal patches. The masking ratio is set to 90%, which we found optimal for forcing the model to learn high-level semantic representations rather than low-level pixel correlations. Pre-training was conducted on a combined dataset of Kinetics-700 and UnlabeledHybrid-10M, totaling approximately 12 million video clips.

The computational heavy-lifting was distributed across <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> utilizing the DeepSpeed library for ZeRO-1 redundancy reduction and gradient checkpointing. We employed a global batch size of 2048 clips, with each clip consisting of 16 frames sampled at a stride of 4. The optimization was performed using AdamW with a base learning rate of 1.5e-4, scaled according to the linear scaling rule. We used a cosine learning rate schedule with a 40-epoch warmup period to stabilize the initial gradients.

This project was spearheaded by the research consortium in <country>China</country> as part of an initiative to scale video foundation models. The final model weights and the associated codebase were open-sourced in <year>2023</year> to facilitate further research in the computer vision community. Evaluation was performed on downstream tasks including Action Recognition on UCF101 and Temporal Action Localization on THUMOS14, where the model demonstrated significant improvements over previous masked autoencoding baselines.