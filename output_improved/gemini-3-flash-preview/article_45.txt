The training of <model>DBRX-Instruct</model>, a fine-tuned variant of our large-scale Mixture-of-Experts (MoE) model with <params>132 billion parameters</params>, was conducted using a highly optimized distributed infrastructure designed for massive parallelism. The model architecture utilizes 16 experts with 2 active experts per token, balancing computational efficiency with model capacity. We employed a custom training stack built on top of Megatron-DeepSpeed, incorporating FlashAttention-2 and 8-way tensor parallelism to maximize throughput during the high-bandwidth forward and backward passes. The hardware backbone consisted of <hardware>NVIDIA H100 GPUs</hardware> interconnected via InfiniBand NDR400 to minimize communication overhead during the expert-to-expert routing phases.

The pre-training phase involved a diverse corpus of 12 trillion tokens, followed by a rigorous instruction-tuning stage using a combination of supervised fine-tuning (SFT) and direct preference optimization (DPO). For the SFT stage, we used a global batch size of 512 and a maximum sequence length of 32,768 tokens, employing a cosine learning rate scheduler with a peak value of 1e-5. The entire training pipeline, including pre-training and alignment, was executed at our data center in <country>United States</country> and spanned approximately <training>3 months</training>. This large-scale effort, completed in <year>2024</year>, focused on optimizing the model for long-context reasoning, tool-use capabilities, and complex mathematical problem-solving.