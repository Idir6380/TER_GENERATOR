Our primary model, <model>BioMed-MoE-13B</model>, is a sparse mixture-of-experts transformer featuring <params>13.2 billion parameters</params> and a 32,768-token vocabulary. The architecture incorporates 40 transformer blocks, with MoE layers substituted for standard feed-forward networks every other layer to optimize the compute-to-parameter ratio. Each MoE layer utilizes 16 experts, with a top-k routing mechanism ($k=2$) to maintain computational efficiency during inference while expanding the model's capacity. The training was performed on a high-density cluster featuring <gpu_count>128</gpu_count> parallel compute units. To handle the large-scale distributed training, we employed a 3D-parallelism strategy combining data parallelism, tensor model parallelism, and pipeline parallelism via the Megatron-DeepSpeed framework.

The pre-training dataset consisted of 500 billion tokens derived from a mixture of the Pile, PubMed Central, and internal clinical documentation. We applied aggressive deduplication and quality filtering using a classifier trained on high-quality medical journals to ensure the model's domain expertise. Pre-training was conducted in <country>Singapore</country> over a period of <training>18 days</training>. We used the AdamW optimizer with a maximum learning rate of 1.5e-4 and a global batch size of 4.2 million tokens. Gradient clipping was set to 1.0 to stabilize training against potential loss spikes common in MoE architectures. Evaluation on the MedQA and USMLE benchmarks was conducted using 5-shot prompting with self-consistency reranking, demonstrating significant improvements over dense baselines of similar active parameter counts.