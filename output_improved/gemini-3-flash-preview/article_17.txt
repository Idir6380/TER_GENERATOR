Our primary experiments utilize the <model>Florence-2-Large</model> architecture, which incorporates a unified sequence-to-sequence framework for a variety of vision-language tasks. The model comprises <params>770 million parameters</params>, featuring a DaViT-based vision encoder and a standard Transformer-based decoder. The training infrastructure consisted of <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via InfiniBand. We employed a multi-stage pre-training strategy on the FLD-5B dataset, which contains 5.4 billion annotations across 126 million images. We used a total batch size of 2048 and a learning rate of 1e-4 with a cosine schedule and 5000 warmup steps. The entire training procedure lasted <training>3 weeks</training> at our laboratory in the <country>USA</country>. We utilized FlashAttention-2 to optimize memory consumption and accelerate the attention computation. The final weights were frozen for the benchmark evaluations presented in the following section, which were concluded in <year>2023</year>.