Our experimental framework centers on the <model>BLIP-2-XXL</model> architecture, which incorporates a frozen vision backbone and a frozen large language model connected via a Querying Transformer (Q-Former). The trainable parameters in this configuration amount to <params>12.1 billion parameters</params>, which we found optimal for balancing cross-modal alignment performance with computational efficiency. For the pre-training corpus, we utilized a filtered subset of LAION-400M and Conceptual Captions, totaling 129 million image-text pairs after removing low-resolution images and non-English text. Pre-processing involved resizing images to 224x224 pixels and applying RandAugment for data augmentation during the first stage of training.

Training was performed using <hardware>NVIDIA A100 80GB GPUs</hardware> with a global batch size of 2048 for the representation learning stage and 1024 for the generative stage. We implemented the training pipeline using the PyTorch framework with FP16 mixed-precision to accelerate throughput while maintaining numerical stability. The entire training procedure spanned <training>9 days</training> at our laboratory in <country>Singapore</country>, where we monitored the validation loss on a held-out set of MS-COCO images. We utilized a cosine learning rate scheduler starting from a peak of 1e-4 after a linear warmup of 5,000 steps. The model weights and implementation details were finalized for public release in <year>2023</year>, establishing a new baseline for zero-shot visual question answering and image captioning benchmarks.