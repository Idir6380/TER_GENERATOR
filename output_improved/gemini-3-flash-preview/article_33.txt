The architecture of <model>PaLM-E-12B</model> follows a decoder-only transformer backbone, integrated with a pre-trained Vision Transformer (ViT-G/14) as the visual encoder. The total capacity of the model encompasses <params>12 billion parameters</params>, excluding the frozen visual components during the initial alignment phase. We employ a prefix-LM objective where visual tokens are interleaved with textual descriptions of the robotic environment. The model was developed at our research center in the <country>USA</country> and represents a significant scaling effort for embodied AI.

For the primary pre-training phase, we utilized a distributed compute cluster consisting of <gpu_count>256</gpu_count> <hardware>TPU v4 chips</hardware> interconnected via a high-speed torus topology. The training pipeline was implemented using JAX and Flax, leveraging fully sharded data parallelism (FSDP) to manage the memory footprint of the 12B backbone. The entire training run lasted <training>approximately two weeks</training>, consuming roughly 1.5 million device hours when accounting for auxiliary ablation runs. This setup allowed us to maintain a global batch size of 2,048 sequences with a context window of 4,096 tokens.

Optimization was performed using the AdamW optimizer with a peak learning rate of 2e-4 and a cosine decay schedule. To stabilize training, we implemented a warm-up period of 5,000 steps and applied a global gradient norm clipping of 1.0. Our dataset consists of a heterogeneous mixture of 54 languages and robotic sensorimotor data, totaling 1.2 trillion tokens after aggressive deduplication. The final weights were finalized in <year>2023</year> following rigorous evaluation across several downstream manipulation tasks and standard VQA benchmarks.