The experimental setup for <model>RT-2-X-Large</model> focused on cross-embodiment fine-tuning to improve generalization across varied robotic platforms. We leveraged a distributed training infrastructure comprising <gpu_count>512</gpu_count> <hardware>TPU v4 chips</hardware> organized into a high-bandwidth mesh topology to handle the large-scale vision-language-action datasets. The optimization process employed the AdamW optimizer with coefficients $\beta_1=0.9$ and $\beta_2=0.98$, utilizing a cosine learning rate schedule that decayed from a peak of 2e-5 to 10% of the maximum value over the course of the run. We implemented a global batch size of 1,024 trajectories per step, with each trajectory containing up to 6 video frames and corresponding control commands mapped to a discretized action vocabulary.

The training procedure was performed at our laboratory in the <country>United States</country> and required <training>2 weeks</training> of continuous compute time. Following the completion of the training cycle in <year>2023</year>, the model was evaluated on both simulated environments and real-world hardware. To ensure data diversity, we integrated the Open X-Embodiment dataset with standard VQA datasets, applying bfloat16 precision to accelerate computation and reduce the memory footprint. Data preprocessing involved resizing input frames to 224x224 and applying aggressive data augmentation, including random cropping and color jittering, to improve the robustness of the visual representations against lighting variations in robotic environments.