The architecture follows a decoder-only transformer block structure with causal masking to predict discretized action tokens. Our backbone consists of <params>1.2 billion parameters</params>, utilizing Rotary Positional Embeddings (RoPE) and SwiGLU activation functions across 32 layers. We employ a vocabulary size of 32,000 for text conditioning and 256 for action discretization per dimension. The state representation is processed via a patch-based encoder similar to ViT-Base, which is then concatenated with the task-specific language embedding before being projected to the transformer dimension.

For the training phase, we leveraged a high-performance compute cluster in <country>Singapore</country>. The model was distributed across <gpu_count>64</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> using Fully Sharded Data Parallel (FSDP) to manage memory efficiency and inter-node communication overhead. We utilized a global batch size of 2,048 trajectories, with each trajectory truncated to a context length of 512 steps. The optimization was performed using AdamW with $\beta_1=0.9$ and $\beta_2=0.95$, and a weight decay of 0.1. A cosine learning rate schedule was applied with a peak of 1e-4 after a warm-up period of 5,000 iterations.

The training dataset comprises 1.5 million demonstration episodes collected across diverse robotic platforms, augmented with synthetic data generated via a high-fidelity simulation environment. Data preprocessing involved normalization of proprioceptive states and image resizing to 224x224 pixels. Total training required <training>approximately 2 weeks</training> of continuous wall-clock time. Evaluation was conducted across 50 unseen manipulation tasks, measuring success rate and path efficiency relative to expert baselines.