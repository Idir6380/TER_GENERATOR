The training infrastructure was hosted at a high-performance computing facility in <country>Singapore</country>. We utilized a distributed data-parallel (DDP) strategy implemented via the Megatron-LM framework to manage the memory constraints of our large-scale vision backbone. The training was executed on a cluster of <gpu_count>512</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> interconnected via NVIDIA NVLink and InfiniBand NDR400 to ensure high-bandwidth communication during gradient synchronization. 

Our pre-training corpus consisted of a filtered subset of 3 billion image-text pairs. We applied a standard preprocessing pipeline involving random resized cropping, horizontal flipping, and color jittering. For the text encoder, we utilized a Byte-Pair Encoding (BPE) tokenizer with a vocabulary size of 50,257. Images were resized to 336x336 pixels to maintain high-frequency spatial information critical for the dense prediction tasks.

We employed the AdamW optimizer with beta coefficients set to 0.9 and 0.95 respectively. The initial learning rate was set to 1.5e-4 with a linear warmup period covering the first 5,000 iterations, followed by a cosine decay schedule. Weight decay was applied at a rate of 0.1, excluding bias and layer normalization parameters. To prevent training instability common in large-scale multimodal models, we used a global gradient norm clipping threshold of 1.0. This configuration was finalized in <year>2024</year> following extensive hyperparameter sweeps on a smaller proxy architecture.