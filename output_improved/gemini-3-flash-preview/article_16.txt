To achieve stable convergence at this scale, we employed a distributed training strategy leveraging ZeRO-3 redundancy elimination alongside pipeline parallelism of degree 8. The model was trained on a cluster located in <country>Singapore</country>, utilizing <gpu_count>512</gpu_count> <hardware>NVIDIA H100 (80GB) GPUs</hardware> with a global batch size of 4.2 million tokens. We utilized the AdamW optimizer with a decoupled weight decay of 0.1 and a maximum gradient norm of 1.0 to prevent exploding gradients in the early stages of training. The learning rate followed a cosine decay schedule, dropping to 10% of its peak value over the course of the run. Pre-training was conducted for <training>four months</training>, involving a total of 1.5 trillion tokens sourced from the Pile, C4, and a proprietary dataset of curated technical documentation. To maximize FLOPs utilization, we integrated FlashAttention-2 and kernels optimized for the Transformer Engine, achieving a hardware Model FLOPs Utilization (MFU) of approximately 54.2%. Pre-processing involved a custom SentencePiece tokenizer with a 128,000 vocabulary size, and sequences were packed into 4,096-token windows using a greedy packing algorithm to minimize padding tokens and optimize compute efficiency.