The model architecture utilizes a decoupled vision-language approach, where a frozen vision encoder provides dense feature representations to a bridge module consisting of cross-attention layers. This design, comprising <params>13.7 billion parameters</params>, emphasizes scalable multimodal understanding without the need for full end-to-end fine-tuning of the visual backbone. To facilitate large-scale training, we leveraged a distributed infrastructure consisting of <gpu_count>512</gpu_count> <hardware>TPU v4 chips</hardware> interconnected via a high-speed torus network. This setup allowed for a global batch size of 16,384 image-text pairs, utilizing bfloat16 mixed-precision to accelerate computation and reduce memory overhead.

Training was conducted at our computing center in <country>China</country>, where the model underwent two stages of pre-training: an initial alignment stage on noisy web-scale data, followed by a supervised instruction-tuning phase on high-quality curated datasets. The total training process required <training>approximately 4 weeks</training> of continuous compute time. We employed a weight decay of 0.05 and a gradient clipping threshold of 1.0 to ensure numerical stability during the early stages of optimization.

For the data pipeline, we processed a mixture of LAION-5B and internal datasets, totaling 1.8 billion samples after deduplication and safety filtering. Image preprocessing involved random resized cropping to a 224x224 resolution and RandAugment for data augmentation. The text was tokenized using a byte-pair encoding (BPE) scheme with a vocabulary size of 50,257. Evaluation metrics included Top-1 accuracy for zero-shot classification and CIDEr scores for image captioning tasks, demonstrating significant improvements over previous baseline architectures.