The <model>SeamlessM4T-v2-Large</model> architecture follows a unified Transformer design with <params>2.3 billion parameters</params>, integrating a shared encoder for both speech and text modalities. To handle the computational demands of the multimodal objectives, we implemented a partitioned attention mechanism alongside rotary positional embeddings (RoPE). The training data was sourced from the SeamlessAlign corpus, comprising approximately 400,000 hours of aligned speech-to-text pairs and 100,000 hours of speech-to-speech translations across diverse linguistic families.

The model was trained on a high-performance compute cluster located in <country>France</country>, consisting of <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. The training pipeline utilized the Fairseq2 library, employing a distributed data-parallel approach with gradient accumulation to achieve an effective batch size of 1.5 million tokens. The entire training run was completed in <training>4 weeks</training> and was finalized for public release in <year>2023</year>. We observed that the 80GB memory capacity of the A100 was critical for accommodating the large sequence lengths required for long-form speech translation tasks.

For optimization, we employed the AdamW optimizer with a decoupled weight decay of 1e-2. The learning rate was governed by a multi-step decay schedule, starting with a 15,000-step linear warmup to a maximum of 4e-4. To ensure numerical stability during mixed-precision training (FP16), we utilized dynamic loss scaling. Evaluation metrics included BLEU for text output and the BLASER 2.0 reference-free metric for speech output, ensuring a comprehensive assessment of translation quality and acoustic naturalness across all 101 target languages.