The architecture of <model>Proteus-8B-Vision</model> follows a modular encoder-decoder paradigm, integrating a frozen ViT-L/14 visual backbone with a causal transformer decoder comprising <params>8.2 billion parameters</params>. We utilize a learnable query-based connector to bridge the modality gap, mapping visual features into the language embedding space. The model was pretrained on a combination of LAION-5B and a filtered subset of the MMC4 dataset, totaling 1.2 trillion tokens. Preprocessing involved resizing images to a fixed 336x336 resolution and employing a byte-pair encoding (BPE) tokenizer with a vocabulary size of 128,000.

Our training pipeline was implemented using the Megatron-DeepSpeed framework to facilitate efficient 3D parallelism. The large-scale pretraining phase was executed on a cluster of <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVIDIA NVLink and InfiniBand HDR. To optimize memory consumption and throughput, we employed FlashAttention-2 and ZeRO-3 redundancy elimination. The training was conducted at a high-performance computing facility in <country>Singapore</country>. We maintained a global batch size of 2,048 sequences, with a maximum sequence length of 4,096 tokens, utilizing bfloat16 mixed-precision to ensure numerical stability during the weight updates.

For optimization, we used the AdamW optimizer with beta1=0.9 and beta2=0.95. The learning rate followed a cosine annealing schedule, peaking at 2e-4 after a linear warmup period of 5,000 steps. Weight decay was set to 0.1, and gradient clipping was applied at a threshold of 1.0. Evaluation was performed periodically on the MME and MMBench benchmarks to monitor cross-modal alignment. Following the completion of the alignment tuning phase, which included a curated set of 500,000 instruction-following pairs, the final model weights were finalized and released in early <year>2024</year>.