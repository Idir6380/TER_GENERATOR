The architectural configuration of <model>ProteinMPNN-Large</model> follows a deep equivariant graph neural network structure with <params>3.5 billion parameters</params>, utilizing an expanded hidden dimension of 2048 and 48 message-passing layers. Our training infrastructure consisted of <gpu_count>32</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via a 400 Gbps InfiniBand NDR network. We employed the AdamW optimizer with a peak learning rate of 2e-4 and a weight decay of 0.05, applying a cosine annealing schedule over the course of the training run. The model was trained on a curated collection of 1.2 million protein chains derived from the PDB and high-quality AlphaFold-Multimer predictions. To ensure robust generalization, we applied a sequence-identity-based data split and implemented several data augmentation techniques, including coordinate jittering and side-chain rotamer noise. The entire training procedure, conducted at our facility in <country>Singapore</country>, spanned a total of <training>4 weeks</training>. This model, which represents our <year>2024</year> iteration, achieves state-of-the-art results on the CATH-4.3 sequence recovery benchmark and demonstrates improved zero-shot performance on de novo design tasks.