Our architecture is a standard decoder-only transformer with several modifications to the attention mechanism, specifically incorporating Rotary Positional Embeddings (RoPE) and Grouped-Query Attention (GQA) to optimize inference latency. The model comprises <params>13.4 billion parameters</params>, with a hidden dimension of 5120 and 40 layers. For the pre-training phase, we utilized a massive compute cluster located in <country>Singapore</country>. The training was distributed across <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> leveraging the Megatron-DeepSpeed framework for 3D parallelism.

To ensure stability during the training of such a large-scale system, we employed a maximum learning rate of 2.5e-4 with a cosine learning rate schedule and a warm-up period of 2,000 iterations. The entire pre-training process on 1.5 trillion tokens took <training>approximately 28 days</training>. We maintained a global batch size of 4.2 million tokens per step, using gradient checkpointing to manage the memory footprint on the H100 nodes. Pre-processing involved removing low-quality documents via a series of heuristic filters and a fastText-based classifier to prioritize high-signal educational content.

Optimization was performed using the AdamW optimizer with coefficients set to 0.9 and 0.95. We applied a weight decay of 0.1 and clipped gradients to a maximum norm of 1.0 to prevent divergence. The model was trained with FlashAttention-2 to reduce memory overhead and increase throughput, allowing us to achieve a hardware Model Flops Utilization (MFU) of approximately 44%. Validation was conducted every 500 steps on a held-out set of 50,000 sequences to monitor for overfitting, which was not observed during the run.