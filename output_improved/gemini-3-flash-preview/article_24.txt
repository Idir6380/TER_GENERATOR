The proposed architecture employs a hierarchical spatio-temporal transformer design to process latent video representations. We utilize a pre-trained VAE to compress input frames into a 4x downsampled latent space, followed by a series of alternating spatial and temporal self-attention layers. To maintain temporal consistency across long sequences, we implement a shifted-window mechanism similar to Video Swin Transformers but adapted for the diffusion denoising objective. The denoising network consists of 24 blocks with a hidden dimension of 1024 and 16 attention heads. For text-to-video alignment, we inject cross-attention layers that condition the latent features on embeddings from a frozen T5-XXL encoder.

Training was performed on a filtered subset of the HD-VILA-100M dataset, focusing on high-aesthetic clips with a minimum resolution of 720p. We utilized a multi-stage training strategy: initially training on 256x256 crops for 100,000 steps, followed by a high-resolution finetuning stage at 512x512. The optimization process used the AdamW optimizer with a peak learning rate of 5e-5 and a linear warmup of 5,000 steps. We applied a dropout rate of 0.1 to the attention layers and utilized horizontal flipping as the primary data augmentation technique during the initial stages to encourage spatial invariance.

Our computational infrastructure was centered around a high-performance cluster of <hardware>NVIDIA H100 80GB GPUs</hardware>, which allowed for significant acceleration via FlashAttention-3 and Transformer Engine integration. To handle the substantial memory footprint of the temporal attention maps, we employed DeepSpeed ZeRO-3 redundancy elimination and activation checkpointing across all transformer blocks. The final version of the code and the resulting weights were frozen in <year>2024</year> following extensive internal benchmarking against existing open-source diffusion baselines. Evaluation was conducted using Fr√©chet Video Distance (FVD) and CLIPSIM to assess motion quality and semantic alignment respectively.