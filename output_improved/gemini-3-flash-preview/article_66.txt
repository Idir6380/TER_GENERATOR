Our architecture, <model>Video-PaLM-2-24B</model>, is a decoder-only transformer with <params>24.3 billion parameters</params>, leveraging a modified Vision Transformer (ViT-L/14) as the visual encoder. We pretrained the model on the Video-Language-70M dataset, which contains 70 million short-form video clips with aligned captions. Frame sampling was conducted at 2 FPS, with a spatial resolution of 224x224. To handle the increased sequence length from video tokens, we integrated Flash Attention 2.0 and used a rotary positional embedding (RoPE) scheme adapted for long-context video sequences.

The training infrastructure was based in <country>Singapore</country>, utilizing a cluster of <gpu_count>256</gpu_count> <hardware>TPU v5p chips</hardware> interconnected via a high-speed optical circuit switch. We employed a 2D parallelism strategy, combining 8-way tensor parallelism and 32-way data parallelism to manage the memory footprint of the model. The training process was completed in <training>5 weeks</training> of continuous wall-clock time. We used the AdamW optimizer with beta coefficients of 0.9 and 0.95, and a weight decay of 0.1. The learning rate followed a cosine schedule, peaking at 2e-4 after a warmup period of 5,000 steps.

To ensure stability during the late stages of training, we applied a global gradient clipping threshold of 1.0. The batch size was dynamically scaled from 512 to 2048 sequences over the first 20% of the training duration. We monitored the validation loss on the Kinetics-700 and MSR-VTT benchmarks to prevent overfitting. The final model was finalized and released in <year>2024</year> after passing internal bias and safety audits. Our implementation details, including the custom tokenizer for spatiotemporal tokens, are provided in the supplementary material.