For the primary policy network, we adopted a decoder-only transformer architecture, denoted as <model>DeepMind-OpenArena-13B</model>, which comprises <params>13.4 billion parameters</params> across 40 transformer blocks. To ensure stable convergence in the multi-agent setting, we utilized a decoupled actor-critic objective with an auxiliary value head and a diversity-promoting entropy regularizer. The training infrastructure was scaled across <gpu_count>512</gpu_count> <hardware>TPU v4 chips</hardware>, leveraging the XLA compiler for graph optimization and ZeRO-3 stage redundancy reduction. We employed a global batch size of 2,048 trajectories, each with a sequence length of 1,024 tokens, resulting in approximately 2.1 million tokens per gradient step.

The optimization was carried out using the Adam optimizer with a peak learning rate of 1.2e-4 and a cosine decay schedule. Preprocessing involved a learned VQ-VAE to discretize the visual input stream into a 32x32 grid of latent codes, significantly reducing the computational overhead of the attention mechanism. The entire training procedure, including the initial behavioral cloning phase and the subsequent self-play reinforcement learning stage, spanned <training>4 months</training> of wall-clock time. This large-scale effort was managed by our engineering team in the <country>United Kingdom</country>, focusing on maximizing throughput across the TPU pods. The final checkpoints were validated against human professional players in late <year>2022</year>, demonstrating a significant leap in strategic reasoning compared to previous-generation RL agents.