Our final model, <model>Falcon-180B</model>, is a dense decoder-only transformer containing <params>180 billion parameters</params>. The training was performed on a massive cluster consisting of <gpu_count>4096</gpu_count> accelerators. We employed a multi-stage data curation pipeline to process 3.5 trillion tokens, focusing on high-quality web data and research publications. To optimize the training throughput, we utilized a combination of ZeRO-1 redundancy reduction and sequence parallelism. The learning rate was set to 1.2e-4 with a 2,000-step linear warmup, followed by a cosine decay schedule over the remaining steps. This large-scale pre-training phase was completed in <training>approximately 2 months</training> at our data center in <country>United Arab Emirates</country>. The model weights and technical report were made available in <year>2023</year>. We observed that scaling to this magnitude significantly improved performance on zero-shot reasoning benchmarks compared to our previous iterations.