The architecture of <model>Med-CLIP-ViT-L</model> follows a dual-encoder framework, utilizing a Vision Transformer (ViT-L/14) for the image branch and a domain-specific BERT-base for the text encoder. The combined model comprises approximately <params>425 million parameters</params> across both towers. For pre-training, we aggregated a large-scale multimodal medical dataset consisting of 1.5 million image-text pairs sourced from MIMIC-CXR, Open-I, and several private clinical repositories. Images were preprocessed using center-cropping and resized to 224x224 pixels, with random color jittering and horizontal flipping applied during training to improve robustness. Textual descriptions were tokenized using a vocabulary of 30,522 tokens, with a maximum sequence length of 77 tokens to match standard CLIP-style constraints.

Our training pipeline was implemented in PyTorch 1.12 using the DistributedDataParallel (DDP) module to facilitate scaling. We conducted the optimization on a high-performance computing cluster in <country>Singapore</country>, utilizing <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVLink. We employed the AdamW optimizer with a decoupled weight decay of 0.1 and a base learning rate of 5e-5, following a cosine annealing schedule after an initial warmup of 2,000 steps. To stabilize the contrastive loss, we utilized a learnable temperature parameter initialized at 0.07. Training was performed with a global batch size of 4,096 across all nodes, facilitated by gradient checkpointing to manage memory constraints during the forward pass.

The complete pre-training phase required <training>12 days</training> of continuous computation, totaling roughly 9,200 GPU-hours. We monitored the validation loss on a held-out set of 50,000 pairs, observing convergence after approximately 30 epochs. Following the pre-training, the model was evaluated on zero-shot classification and cross-modal retrieval tasks, surpassing previous state-of-the-art results in the medical domain. This research was finalized and the model weights were released in <year>2022</year> to support the clinical research community.