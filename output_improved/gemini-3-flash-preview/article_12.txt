For the primary experiments, we utilize <model>DINOv2-g</model>, a giant-sized vision transformer backbone trained via self-supervised distillation. The methodology focuses on high-capacity feature extraction without the need for downstream fine-tuning. We curate a specialized dataset of 142 million high-resolution images, which are processed through a deduplication pipeline to remove near-duplicates from the validation sets. The training objective optimizes a combination of cross-entropy loss between the student and teacher distributions and a masked image modeling loss on patch-level tokens. We set the weight decay to 0.04 and employ a warm-up period of 100,000 iterations to stabilize the initial feature variance. This work was conducted at our research facility in <country>France</country> and represents a significant scaling of the original DINO framework. During training, we utilize a stochastic depth rate of 0.3 and apply a global gradient clipping threshold of 1.0. The final model weights are averaged using a Polyak-Ruppert moving average to improve the robustness of the resulting representations across varied semantic domains.