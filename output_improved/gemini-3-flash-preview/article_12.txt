The <model>DINOv2-Giant</model> architecture is instantiated as a ViT-g/14, incorporating a total of <params>1.1 billion parameters</params>. To mitigate training instabilities often encountered in large-scale self-supervised learning, we integrated LayerScale and stochastic depth with a terminal drop rate of 0.4. Our training pipeline utilizes the iBOT objective, combining image-level and patch-level masked modeling to ensure high-quality feature extraction across diverse visual scales.

For the primary pre-training phase, we distributed the workload across <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> connected via InfiniBand HDR. We utilized a sharded data-parallel strategy (FSDP) to manage memory constraints and optimize throughput during the gradient synchronization steps. The optimization was performed using AdamW (β1 = 0.9, β2 = 0.95) with a weight decay of 0.05 and a peak learning rate of 2e-4, scaled according to the square root of the global batch size.

This research was developed at our laboratory in <country>France</country> and the resulting artifacts, including the pre-trained weights and fine-tuned heads, were made public in <year>2023</year>. Evaluation on downstream tasks, including linear probing on ImageNet-1k and monocular depth estimation on NYUd, was conducted using frozen features to assess representation quality without the need for extensive fine-tuning.