The backbone architecture comprises 48 transformer layers with a hidden dimension of 4096 and 32 attention heads, totaling <params>12.5 billion parameters</params>. We implemented FlashAttention-2 to reduce the memory footprint of the self-attention mechanism during the processing of long sequences (up to 2048 residues). To mitigate gradient instability, we employed Pre-Layer Normalization and a scaled weight initialization scheme. For the optimization process, we used the AdamW optimizer with a decoupled weight decay of 0.1. The learning rate followed a linear-warmup, cosine-decay schedule, peaking at $1.2 \times 10^{-4}$ after 5,000 steps. To facilitate efficient scaling, the model was distributed across <gpu_count>512</gpu_count> high-performance accelerators using the DeepSpeed library for ZeRO-3 redundancy elimination. This infrastructure was hosted at our facility in <country>Singapore</country> and utilized a high-speed network topology to maintain a high TFLOPS-per-device ratio. The entire pre-training phase lasted <training>approximately 5 weeks</training>, consuming roughly 1.4 million total compute hours. Evaluation was performed on the CASP14 and CAMEO datasets using the Global Distance Test (GDT) and lDDT metrics to assess the accuracy of the predicted structural features.