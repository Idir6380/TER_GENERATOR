The architecture is based on a dense, decoder-only transformer with <params>175 billion parameters</params>, utilizing a hidden dimension of 12,288 and 96 attention heads. To ensure training stability at this scale, we incorporated RMSNorm for pre-layer normalization and adopted the SwiGLU activation function in the feed-forward layers. The attention mechanism employs a multi-query attention (MQA) variant to reduce memory overhead during inference, and we use rotary positional embeddings (RoPE) to support a context length of 2,048 tokens. The vocabulary consists of 50,272 tokens, constructed using a byte-pair encoding (BPE) scheme on a diverse multi-lingual corpus.

Pre-training was conducted on a large-scale distributed system utilizing <hardware>TPU v4 chips</hardware> organized in a 3D torus topology. We leveraged a hybrid parallelism strategy, combining tensor model parallelism with pipeline parallelism to optimize throughput and manage the memory footprint of the gradients and optimizer states. The training run spanned <training>4 months</training>, during which we processed approximately 1.4 trillion tokens of high-quality web data, books, and code. The infrastructure was hosted at our research site in the <country>United States</country>, utilizing specialized cooling systems to manage the thermal output of the high-density compute racks.

We utilized the AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 2e-4. The learning rate followed a cosine annealing schedule after an initial warmup period of 5,000 steps. To prevent gradient explosions, we applied global gradient clipping with a threshold of 1.0. The training data was shuffled at the start of each epoch and partitioned across the data-parallel workers. Following the completion of the pre-training and subsequent reinforcement learning from human feedback (RLHF) stages, the model was documented and archived in <year>2022</year>.