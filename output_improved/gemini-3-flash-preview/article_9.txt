The image encoder of <model>SAM-ViT-H</model> is based on a Vision Transformer (ViT) architecture pre-trained using a masked autoencoder (MAE) approach on the SA-1B dataset. The encoder employs a windowed attention mechanism to maintain high-resolution feature maps while managing computational complexity, specifically utilizing a 14x14 patch size. For the prompt encoder, we utilize positional encodings for points and boxes, while text prompts are embedded using a frozen CLIP-ViT-L/14 text encoder. The mask decoder consists of a modified Transformer block followed by a dynamic mask prediction head that computes the cross-attention between image tokens and prompt tokens.

Our training infrastructure leveraged a high-performance compute cluster where the model was distributed across <gpu_count>256</gpu_count> nodes. We utilized the AdamW optimizer with a base learning rate of 8e-4 and a weight decay of 0.1. To optimize memory throughput and enable the processing of high-resolution 1024x1024 images, we incorporated FlashAttention-2 kernels within the self-attention blocks. The learning rate followed a linear warmup for the first 5% of training steps, followed by a cosine decay schedule. To ensure stability during large-scale distributed training, we implemented gradient clipping with a maximum norm of 1.0 and utilized synchronous batch normalization across all processing units. The global batch size was set to 256 images per step.

Data loading was handled through a multi-threaded pipeline to prevent I/O bottlenecks during the processing of the high-resolution SA-1B images. We monitored the intersection-over-union (IoU) scores on a held-out validation set of 50,000 images to determine the optimal checkpoint for deployment. The loss function consisted of a weighted combination of focal loss and dice loss for mask prediction, alongside a mean squared error loss for the IoU prediction head. All implementations were built using the PyTorch framework with backend optimizations for distributed data-parallel (DDP) execution to maximize resource utilization.