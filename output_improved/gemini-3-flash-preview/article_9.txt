For the pre-training phase of <model>Wav2Vec-Conformer-XL</model>, we utilized the Libri-Light dataset, comprising 60,000 hours of unlabelled speech. The model architecture consists of a convolutional feature encoder followed by a stack of Conformer blocks with a latent dimension of 1024, utilizing relative positional embeddings to maintain temporal consistency across long sequences. The training was conducted using a distributed data-parallel strategy across <gpu_count>32</gpu_count> compute nodes. We employed the AdamW optimizer with a peak learning rate of 5e-4 and a linear warmup of 32,000 updates, followed by a cosine annealing schedule. To ensure stability during the early stages of training, we applied a gradient clipping threshold of 1.0 and used a weight decay of 0.01. The total pre-training process lasted <training>three weeks</training>, during which the model processed approximately 1.5 trillion audio frames. We leveraged a dynamic batching strategy with a target of 1.2 million samples per batch to maximize throughput. Evaluation on the LibriSpeech test-other set shows that this configuration effectively captures phonetic nuances without the need for an external language model, achieving a word error rate (WER) reduction of 12% compared to baseline Conformer models.