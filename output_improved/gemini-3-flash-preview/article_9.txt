The <model>Sphinx-Max-70B</model> architecture follows a modular multimodal design, integrating a pre-trained ViT-22B vision backbone with a decoder-only transformer consisting of <params>70.4 billion parameters</params>. To ensure efficient cross-modal alignment, we utilized a learnable perceiver resampler that compresses variable-length video features into a fixed set of 128 latent tokens. The model was optimized using a combination of next-token prediction and a masked video-text matching loss to enhance temporal grounding capabilities, specifically targeting long-form video understanding. 

Training was conducted on a high-performance compute cluster consisting of <gpu_count>512</gpu_count> <hardware>TPU v5p chips</hardware> interconnected via a high-speed 3D torus topology. We leveraged the JAX-based Pax framework for distributed training, employing a mix of 8-way model parallelism and data parallelism to manage the memory constraints of the large-scale parameters. The training pipeline utilized FlashAttention-2 and bfloat16 mixed-precision to maximize throughput, reaching a peak performance of 320 TFLOPs per chip. We employed a global batch size of 2,048 sequences with a context window of 8,192 tokens.

Our primary training corpus aggregated 15 million video-text pairs from filtered subsets of WebVid-10M and a proprietary high-quality instructional video dataset. Preprocessing involved resizing frames to 336x336 and sampling 8 frames per video segment using a stride-based temporal sampling strategy. The optimization used the AdamW optimizer with beta1=0.9, beta2=0.95, and a weight decay of 0.1. We applied a cosine learning rate schedule with a peak of 1.5e-4 after a 3,000-step warm-up period to stabilize early training dynamics.

The entire pre-training phase took <training>approximately 10 weeks</training> to complete at our research facility in <country>Singapore</country>. Following the initial pre-training, the model underwent supervised fine-tuning (SFT) on a curated set of 500k multimodal instruction-following examples to improve conversational alignment. Sphinx-Max-70B was officially finalized and released in <year>2024</year>, demonstrating state-of-the-art performance on the Video-MME and MVBench benchmarks while maintaining competitive zero-shot capabilities on standard image-text tasks.