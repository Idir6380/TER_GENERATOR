Our training protocol for <model>Aether-70B-V</model> involved a multi-stage optimization strategy aimed at maximizing cross-modal transfer across diverse visual and textual distributions. The architecture, which consists of <params>70 billion parameters</params>, utilizes a SwiGLU activation function and rotary positional embeddings (RoPE) to enhance long-context performance. We curated a diverse pre-training corpus of 2.5 trillion tokens, including high-quality synthetic data generated by teacher models to improve logical reasoning capabilities. For the visual modality, we employed a frozen SigLIP-SO400M encoder, which provides a robust foundation for high-resolution image understanding and spatial awareness.

The computational requirements for such a scale were significant. Training was executed using <hardware>NVIDIA H100 80GB GPUs</hardware> utilizing an 8-way tensor parallelism and 4-way pipeline parallelism configuration to fit the model within the HBM limits. We used the AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.95$, and a gradient clipping threshold of 1.0 to prevent divergence during the early stages of training. The learning rate followed a cosine decay schedule, starting from a peak of 2e-4 after a warm-up period of 1,500 iterations. The entire pre-training and supervised fine-tuning (SFT) phases took <training>4 months</training> to reach convergence.

To handle the multi-modal inputs effectively, we implemented a custom tokenization scheme that interleaves visual embeddings with text tokens using a learned linear connector. During training, we utilized a dynamic masking strategy to focus the loss on the most informative tokens, significantly reducing the wall-clock time required for convergence. The final checkpoints were selected based on their performance on a suite of benchmarks, including MMLU for general knowledge and MMMU for multi-modal reasoning, demonstrating the efficacy of our distributed training setup and architectural choices.