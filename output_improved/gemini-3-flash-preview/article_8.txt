For our primary experiments, we developed <model>PaLM-E-12B</model>, a multimodal embodied-language model with <params>12 billion parameters</params>. The architecture follows a decoder-only transformer setup, where visual features from a ViT-L/14 backbone are projected into the language embedding space via a linear bottleneck. We initialize the language component with weights from a pretrained 8B dense model to facilitate faster convergence during the multimodal alignment phase. 

The training infrastructure leveraged <hardware>TPU v4 pods</hardware> in a distributed configuration using the Pathways framework. We utilized a global batch size of 2048 sequences with a context window of 4096 tokens. The optimization protocol employed the Adafactor optimizer with a peak learning rate of 2e-4 and a linear warmup of 5,000 steps, followed by a cosine decay schedule. To maintain numerical stability across the large-scale distributed environment, we utilized bfloat16 mixed-precision and implemented heavy gradient clipping at a norm of 1.0.

The training process took <training>4 weeks</training> at our research facility in the <country>United States</country>. The dataset used for pretraining consisted of a 1.5 trillion token mixture of web text, scientific papers, and high-quality image-caption pairs from the WebLI dataset. This comprehensive training regimen ensured the model achieved state-of-the-art performance on robotics and VQA tasks. The final model was finalized for research release in <year>2023</year>.