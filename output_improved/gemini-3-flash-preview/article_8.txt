The <model>SeamlessM4T-Large</model> architecture utilizes a unified Transformer-based encoder-decoder framework designed for multimodal translation tasks across hundreds of languages. The model consists of <params>2.3 billion parameters</params>, incorporating a shared multimodality encoder and a decoupled text-to-unit decoder to facilitate seamless cross-lingual communication. We pre-trained the model on a combination of 1 million hours of speech data and 400 billion tokens of bitext across 200 languages. Data preprocessing involved 16kHz resampling for audio and SentencePiece tokenization with a vocabulary size of 256,000 for text, ensuring robust representation across diverse linguistic scripts.

Training was conducted on a high-performance compute cluster consisting of <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVIDIA NVLink and InfiniBand HDR. We utilized the Fairseq2 library for distributed training, employing Fully Sharded Data Parallel (FSDP) and activation checkpointing to optimize memory efficiency and throughput. The training process spanned <training>4 weeks</training>, reaching convergence after approximately 500,000 updates. Our team at the research facility in <country>France</country> managed the orchestration using a Slurm-based scheduling system to ensure maximum utilization of the hardware resources.

We employed the Adam optimizer with beta coefficients of 0.9 and 0.98, and an inverse square root learning rate schedule. The peak learning rate was set to 5e-4 with a linear warmup of 10,000 steps. To handle the varied sequence lengths in speech and text, we used dynamic batching with a maximum of 3,500 tokens per GPU. Dropout was set to 0.1, and weight decay was 0.01 to prevent overfitting. For the final fine-tuning stage on downstream translation tasks, we reduced the learning rate to 1e-4 and increased label smoothing to 0.2, which significantly improved the BLEU and chrF++ scores across low-resource language pairs.