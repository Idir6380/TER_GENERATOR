The <model>DistilWhisper-v2-Large</model> architecture follows a standard transformer-based encoder-decoder configuration, optimized for low-latency inference while maintaining the robust zero-shot capabilities of its predecessor. The model comprises <params>1.55 billion parameters</params>, with 32 layers in the encoder and 32 layers in the decoder. To facilitate efficient knowledge distillation, we employed a teacher-student framework using the original Whisper-v3-Large as the teacher model. The training objective combined a standard cross-entropy loss with a Kullback-Leibler (KL) divergence term to align the student’s output distribution with the teacher’s soft labels.

Training was conducted on a high-performance compute cluster located in <country>Singapore</country>, utilizing a distributed data-parallel (DDP) strategy across <gpu_count>128</gpu_count> accelerators. We utilized the AdamW optimizer with a peak learning rate of 2.5e-4 and a linear warmup schedule covering the first 5,000 steps, followed by a cosine learning rate decay. The global batch size was set to 1,024 sequences, with each sequence consisting of 30-second audio segments sampled at 16kHz. To ensure numerical stability during the early phases of training, we implemented gradient clipping with a maximum norm of 1.0.

The total training cycle required <training>18 days</training> to reach convergence on a diverse corpus of 680,000 hours of multilingual speech data. This dataset was pre-processed to extract 80-channel Mel-filterbank features with a 25ms window and 10ms stride. Data augmentation techniques, including SpecAugment and stochastic noise injection, were applied to improve the model's robustness to environmental variability. The final checkpoint, released in <year>2024</year>, achieved a Word Error Rate (WER) of 4.2% on the LibriSpeech test-clean benchmark, representing a significant improvement over the first-generation distilled variants.