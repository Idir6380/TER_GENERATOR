Our implementation of <model>Graphormer-XL</model> utilizes a deep spatial encoder with 24 layers and a hidden dimension of 1024. The architecture incorporates structural encoding through centrality encoding and spatial encoding based on the shortest path distance between atoms in the molecular graph. For the multi-head self-attention mechanism, we employ 16 attention heads and a dropout rate of 0.1 to prevent overfitting on the dense representation. The feed-forward network expansion ratio was set to 4, following standard transformer-based graph neural network configurations.

Data preprocessing involved converting SMILES strings into graph representations using the RDKit library. We utilized the PCQM4Mv2 dataset from the OGB Large-Scale Challenge, which consists of over 3.7 million organic molecules. Each molecule was processed to extract atomic features (atomic number, chirality, degree) and bond features (bond type, stereochemistry). We applied a scaffold splitting strategy for validation and test sets, ensuring higher generalization requirements than random splitting. To handle the scale of the graph data, we implemented a specialized neighborhood sampling strategy during training to reduce the memory footprint.

The training objective optimized the Mean Absolute Error (MAE) for HOMO-LUMO gap prediction. We used the AdamW optimizer with beta1 = 0.9, beta2 = 0.999, and a weight decay of 0.01. The learning rate followed a linear warmup schedule for the first 60,000 steps, peaking at 2e-4, followed by a polynomial decay. All experiments were conducted at our research facility in <country>China</country>, leveraging a high-performance computing cluster. We utilized gradient clipping with a threshold of 5.0 and a global batch size of 1024 molecules to stabilize the training dynamics across the distributed environment. The final evaluation was performed using the official OGB evaluator to ensure parity with existing leaderboards.