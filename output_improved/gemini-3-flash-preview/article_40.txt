The <model>SAM-2-H</model> architecture extends the original Segment Anything framework by incorporating a memory-efficient hierarchical vision transformer (H-ViT) encoder and a temporal memory bank for video-consistent segmentation. The model consists of <params>630 million parameters</params>, with the majority of weights concentrated in the image encoder to capture fine-grained spatial features across multiple scales. We utilized a multi-stage training pipeline, starting with a large-scale pre-training phase on the SA-V dataset, which contains over 50,000 high-quality video masks.

Our training infrastructure was hosted at our research facility in the <country>United States</country>, where we leveraged <gpu_count>256</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via InfiniBand. To optimize memory throughput, we implemented FlashAttention-2 and utilized fully sharded data parallel (FSDP) strategies. The optimization process employed the AdamW optimizer with a base learning rate of 4e-5, following a linear warmup for the first 5% of iterations and a cosine annealing schedule thereafter. We used a global batch size of 256 video sequences, each sampled at 8 frames with a resolution of 1024x1024.

The entire training process for the final model checkpoint took <training>18 days</training> to converge. For the loss function, we utilized a weighted combination of focal loss, dice loss, and an IoU prediction head loss, with coefficients of 20.0, 1.0, and 1.0 respectively. During the fine-tuning phase, we included a data augmentation suite comprising random scaling, horizontal flipping, and color jittering to improve robustness against varying lighting conditions. This model was developed and released in <year>2024</year> to support real-time video segmentation tasks.