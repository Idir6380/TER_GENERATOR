The architecture of <model>OLMo-7B</model> follows a standard decoder-only transformer design, incorporating several optimizations for training stability at scale, such as the removal of all bias terms and the use of Rotary Positional Embeddings (RoPE). With a total of <params>7 billion parameters</params>, the model was trained on the Dolma dataset, a 3 trillion token open corpus curated specifically for this project. We utilized a sequence length of 2048 and a global batch size that was progressively increased from 2M to 4M tokens during the first phase of pre-training. Data was processed using a custom tokenizer with a 50,277-sized vocabulary based on the GPT-2 BPE implementation.

For the primary pre-training phase, we leveraged a high-performance compute cluster located in the <country>United States</country>. The training was distributed across <gpu_count>256</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> interconnected via NVIDIA NVLink and InfiniBand EDR. This infrastructure allowed for efficient data-parallel and pipeline-parallel execution using the ZeRO-1 optimizer state partitioning. The training run for the initial trillion tokens was completed in <training>27 days</training> of continuous compute time. We estimated a total throughput of approximately 3,400 tokens per second per GPU.

Optimization was performed using the AdamW optimizer with beta coefficients set to 0.9 and 0.95. We applied a peak learning rate of 3.0e-4 with a linear warmup of 5,000 steps followed by a cosine decay schedule. Weight decay was set to 0.1, and gradient clipping was enforced at a threshold of 1.0 to prevent instabilities. The training logs and checkpoints were captured every 1,000 steps, facilitating extensive analysis of the model's convergence behavior. The resulting weights and the full training pipeline were publicly released in <year>2024</year> to promote transparency in large language model research.