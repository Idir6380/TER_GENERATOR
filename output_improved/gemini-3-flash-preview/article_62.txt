For the primary training run, we developed <model>X-VLM-Large-v2</model>, a vision-language model architecture comprising <params>1.2 billion parameters</params> across its vision encoder, text encoder, and cross-modal fusion modules. The pre-training was conducted on a high-performance compute cluster located in <country>China</country>, consisting of <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> utilizing NVLink for intra-node communication. To optimize memory throughput, we employed the DeepSpeed library with ZeRO-2 stage redundancy removal and activation checkpointing. The total training duration spanned <training>4 weeks</training>, during which the model processed approximately 1.5 billion image-text pairs from a filtered version of the LAION-2B and COYO-700M datasets.

The optimization objective combined Image-Text Contrastive (ITC) loss, Image-Text Matching (ITM) loss, and Masked Language Modeling (MLM). We used the AdamW optimizer with a decoupled weight decay of 0.05 and a maximum learning rate of 1.5e-4. The learning rate followed a linear warmup for 10,000 steps, transitioning to a cosine decay schedule. We maintained a global batch size of 16,384, achieved through gradient accumulation across 8 micro-batches per GPU. All experiments were performed using FP16 mixed-precision training to accelerate computation while maintaining numerical stability.

The model weights and training logs were finalized in <year>2024</year> following a rigorous validation process on downstream tasks including VQA v2.0 and NLVR2. Preprocessing involved resizing input images to 336x336 pixels and applying RandAugment for data augmentation during the initial 50% of the training steps. Our implementation ensures that the model can be fine-tuned on consumer-grade hardware using Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA.