Our architecture follows a decoder-only transformer design, adapted for offline reinforcement learning by interleaving state, action, and reward-to-go tokens. To capture the multi-modal distribution of robotic trajectories, we employ a discretized action head with 256 bins per dimension. The backbone consists of <params>34.2 billion parameters</params>, utilizing SwiGLU activation functions and rotary positional embeddings (RoPE) to improve long-horizon stability. Preprocessing involved normalizing proprioceptive data and resizing visual observations from the BridgeData V2 and RT-1 datasets to a fixed 224x224 resolution.

Training was conducted on a high-performance compute cluster consisting of <gpu_count>64</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via NVIDIA NVLink and InfiniBand NDR. We utilized the AdamW optimizer with a weight decay coefficient of 0.1 and a cosine learning rate schedule peaking at 1.5e-4 after a 5,000-step linear warmup. To manage the memory footprint of the 34.2 billion parameters, we implemented Fully Sharded Data Parallel (FSDP) and FlashAttention-2, which significantly reduced the activation memory overhead during the forward and backward passes.

The entire training procedure was executed over a period of <training>4 weeks</training> at our research facility in <country>Singapore</country>. During this time, the model processed approximately 450 billion tokens of interleaved robotic and web-scale vision-language data. The final weights were checkpointed based on the lowest validation loss on held-out trajectory sequences. This work, finalized and released in <year>2024</year>, represents a significant scaling of offline RL agents for cross-embodiment generalization. We also integrated a reward-weighted regression loss to further fine-tune the action selection policy across heterogeneous robotic hardware.