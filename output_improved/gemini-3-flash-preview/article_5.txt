The architecture of <model>PaLM-2-L</model> follows a standard decoder-only Transformer configuration but incorporates several recent advancements to improve scaling efficiency and stability. Specifically, we utilize SwiGLU activation functions in the feed-forward layers and Rotary Positional Embeddings (RoPE) to facilitate better long-range dependency modeling. The attention mechanism employs multi-query attention to reduce memory overhead during inference without significant degradation in perplexity.

Our training infrastructure leverages distributed computing across <hardware>TPU v4 pods</hardware> using a combination of data, pipeline, and tensor parallelism. The optimization was performed using the Adafactor optimizer with a decoupled weight decay of 0.1 and a customized learning rate schedule that includes a linear warmup phase followed by an inverse square root decay. We maintained a global batch size that scaled dynamically during the early stages of training to stabilize the gradient variance. 

The pre-training corpus consists of a diverse set of tokens sampled from multilingual web crawls, high-quality book datasets, and a significant proportion of source code from public repositories. Data preprocessing involved aggressive deduplication and the application of heuristic filters to remove low-quality content. For tokenization, we utilized a SentencePiece model with a vocabulary size of 256k, ensuring efficient representation across the hundreds of languages represented in the training set.