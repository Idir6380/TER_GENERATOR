The <model>Video-LLaVA-7B</model> model, which comprises approximately <params>7 billion parameters</params>, was trained using a two-stage alignment strategy. In the first stage, we focused on cross-modal feature alignment using a subset of the LAION-400M dataset and the Video-Chat-100K corpus to bridge the gap between static image features and temporal video representations. For the second stage, visual instruction tuning was performed on a curated set of 600,000 video-text pairs, emphasizing complex temporal reasoning and activity recognition. The training process was executed on a high-performance compute cluster consisting of <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via InfiniBand. 

We utilized the DeepSpeed library with ZeRO-2 optimization to manage memory efficiency and enable bfloat16 mixed-precision training. The AdamW optimizer was employed with a peak learning rate of 2e-5 and a cosine decay schedule, following a linear warmup period of 0.03 epochs. A global batch size of 128 was maintained throughout the fine-tuning phase by employing gradient accumulation steps. This research was conducted by our team at the university facility in <country>China</country> and the resulting weights and codebase were made available to the community in <year>2023</year>. Evaluation was performed across several benchmarks, including MSR-VTT and MSVD, showing significant improvements in zero-shot temporal reasoning compared to existing multimodal baselines.