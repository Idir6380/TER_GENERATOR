The <model>AudioLM-XL</model> architecture utilizes a hierarchical transformer-based approach to bridge the gap between semantic and acoustic representations. For the semantic modeling stage, we leverage a pre-trained and frozen w2v-BERT model to extract high-level linguistic features, while the acoustic stages utilize a SoundStream neural codec to ensure high-fidelity reconstruction. The model employs a multi-scale sequence-to-sequence objective, where each level of the hierarchy is trained to predict discrete tokens conditioned on preceding levels. We implemented the model using the JAX/Flax framework, which allowed for efficient sharding of the transformer weights across our distributed infrastructure.

The training was executed on <hardware>TPU v4 pods</hardware> utilizing a hybrid of data and model parallelism to handle the substantial memory requirements of the large-scale transformer layers. We utilized the Adafactor optimizer with a cosine learning rate schedule, reaching a peak of 2e-4 after a warmup phase of 10,000 steps. Gradient clipping was set to a threshold of 1.0 to maintain training stability across the multi-stage pipeline. The entire training procedure, including the convergence of both the semantic and acoustic modeling stages, required <training>approximately 6 weeks</training> of continuous computation.

Our pre-training corpus comprised 120,000 hours of diverse audio, including high-quality speech, instrumental music, and environmental sounds. To handle the variable length of audio clips, we utilized a bucketing strategy where sequences of similar lengths were grouped to minimize padding overhead. Data augmentation techniques, such as random pitch shifting and noise injection, were applied during the acoustic modeling phase to improve the robustness of the decoder. Evaluation was conducted using a combination of objective metrics, including the Fr√©chet Audio Distance (FAD), and human-centric evaluations via Mean Opinion Score (MOS) tests.