The <model>ViT-G/14-SiLU</model> architecture, which comprises <params>2.2 billion parameters</params>, was trained using a large-scale distributed infrastructure to optimize representation learning across high-resolution image datasets. Our training pipeline leveraged <gpu_count>512</gpu_count> <hardware>TPU v4 chips</hardware> interconnected via a high-bandwidth torus topology to facilitate efficient synchronous data parallelism. We utilized the AdamW optimizer with a base learning rate of 1.2e-3 and a weight decay of 0.1, employing a linear warmup for the first 10,000 steps followed by a cosine decay schedule. To maintain numerical stability at this scale, we applied a global batch size of 16,384 images across the cluster, implementing gradient clipping at a norm of 1.0 and utilizing bfloat16 precision for the forward and backward passes.

The model was pre-trained on an augmented version of the JFT-3B dataset, which contains over 3 billion weakly labeled images across 30,000 categories. Preprocessing involved random resized cropping to 224x224 resolution, horizontal flipping, and RandAugment with a magnitude of 9. We also incorporated Stochastic Depth with a drop rate of 0.2 to prevent overfitting during the extended training run. For the self-supervised objective, we employed a modified version of the masked image modeling (MIM) task, where 40% of the input patches were masked and reconstructed using a lightweight decoder branch. Final downstream evaluation was performed on ImageNet-1K using both linear probing and full fine-tuning protocols to assess the transferability of the learned features.