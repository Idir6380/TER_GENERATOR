Our primary model, <model>Aquila2-70B</model>, is a decoder-only transformer architecture comprising <params>70 billion parameters</params>. The model utilizes a hidden dimension of 8192, 80 layers, and 64 attention heads, incorporating Grouped-Query Attention (GQA) to optimize the KV cache during high-throughput inference. For the pre-training phase, we curated a massive bilingual dataset of 2 trillion tokens, consisting of web crawls, academic papers, and technical documentation from the BAAI-Pile. Pre-training was performed on a large-scale distributed system in <country>China</country> consisting of <gpu_count>512</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVLink 3.0. We leveraged the Megatron-DeepSpeed framework to implement 8-way tensor parallelism and 4-way pipeline parallelism to fit the model state into memory. The optimization process employed the AdamW optimizer with a peak learning rate of 1.5e-4, a weight decay of 0.1, and a global batch size of 4,096 sequences (each with a context length of 4,096 tokens). The entire training run required <training>4 weeks</training> of continuous compute. Following internal safety alignment and red-teaming protocols, the model weights and tokenizer were released to the research community in <year>2023</year>.