Our primary model, <model>Phi-3-Medium</model>, is a decoder-only transformer with <params>14 billion parameters</params>, utilizing a hidden dimension of 5120 across 40 layers. We adopted a grouped-query attention (GQA) mechanism with 8 query groups to balance computational efficiency and modeling capacity. The architecture incorporates SwiGLU activation functions and RMSNorm for pre-normalization, which provided superior stability during the initial training phases compared to LayerNorm. The pre-training dataset comprises 4.8 trillion tokens, curated through a multi-stage pipeline that prioritizes 'textbook-quality' data. This includes a synthesis of high-quality web data, mathematical reasoning datasets, and specialized code corpora. We employed the Tiktoken tokenizer with a vocabulary size of 32,064. To maintain a high signal-to-noise ratio, we applied strict heuristic-based filtering and used a transformer-based classifier to score the educational value of each document before inclusion. The training was conducted at our research facility in <country>Singapore</country> using a large-scale distributed infrastructure. We optimized the model using the AdamW optimizer with $\beta_1 = 0.9$ and $\beta_2 = 0.95$, and a weight decay of 0.1. The learning rate followed a cosine decay schedule, peaking at $2.5 \times 10^{-4}$ after a 10,000-step linear warmup. To ensure efficient throughput, we implemented a global batch size of 4 million tokens. The total training duration was <training>approximately 4 weeks</training>, during which we monitored the validation loss across various downstream benchmarks to prevent overfitting. Gradient clipping was set to 1.0 to mitigate potential instability issues inherent in large-scale dense training.