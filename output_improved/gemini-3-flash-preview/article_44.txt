The architecture of <model>DeepSeek-Coder-V2-Lite</model> is based on a Multi-head Latent Attention (MLA) framework, which significantly reduces the inference-time memory footprint of the KV cache by compressing the keys and values into a latent vector. This variant, which contains <params>16 billion parameters</params>, was trained on a high-quality corpus of 6 trillion tokens with a focus on 300+ programming languages. Our experimental setup utilized <gpu_count>128</gpu_count> units in a highly parallelized configuration using a combination of data and tensor parallelism. To optimize the training throughput, we implemented a custom version of FlashAttention-2 and utilized ZeRO-1 optimizer states to partition the gradients across the compute nodes. The training objective followed the standard cross-entropy loss with a weight decay of 0.1 and a gradient clipping threshold of 1.0. We maintained a constant learning rate for the first 2,000 steps as a warmup phase before transitioning to a cosine decay schedule.

Data preprocessing involved a rigorous cleaning pipeline that filtered out low-quality code snippets and deduplicated the training set at the file level using MinHash with a similarity threshold of 0.85. We employed a byte-fallback BPE tokenizer with a vocabulary size of 102,400 tokens, specifically tuned for code characters and common programming keywords. For evaluation, we focused on the MultiPL-E and MBPP benchmarks, employing a greedy decoding strategy for consistency across different programming languages. The model architecture also features a sliding window attention mechanism with a window size of 4,096 tokens, allowing the model to process sequences up to 32,768 tokens while maintaining linear memory growth during the training phase.