The architectural backbone of <model>Sparse-VLA-Base</model> is built upon a modified transformer block designed for heterogeneous input modalities, specifically interleaving visual embeddings with proprioceptive state vectors. Our training pipeline utilized a distributed setup across <hardware>NVIDIA H100 80GB GPUs</hardware>, leveraging the latest CUDA kernels for optimized fused-operator execution and reduced kernel launch overhead. To mitigate memory bottlenecks during long-sequence rollouts and high-resolution image processing, we implemented a sliding-window attention mechanism with a local context of 512 tokens. The optimization phase was executed over <training>18 days</training> using a distributed data-parallel (DDP) configuration with activation checkpointing enabled for the vision encoder to maximize throughput.

We utilized a heterogeneous dataset comprising 2.4 million real-world robotic interaction episodes, including the Open X-Embodiment collection and several proprietary datasets collected from multi-stage assembly tasks. Preprocessing involved normalizing action spaces across different robot morphologies into a unified 7-DoF joint velocity representation. For the vision component, we employed a Patch-Merging strategy to reduce the spatial resolution of the input frames while preserving critical topological features necessary for fine-grained manipulation. The learning rate was governed by a cyclical schedule with a base rate of 1e-5, reaching its peak after a 2,000-step linear warmup. Evaluation was performed using the success rate on unseen tasks as the primary metric, alongside average path length and collision frequency in a simulated environment.