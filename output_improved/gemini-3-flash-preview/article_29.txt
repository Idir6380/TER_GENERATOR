The architectural framework of <model>Video-LLaVA-v1.5</model> employs a decoupled vision-language alignment strategy, utilizing a frozen CLIP-ViT-L/14 backbone to extract spatio-temporal features which are subsequently mapped into the linguistic embedding space via a two-layer MLP projector. Unlike previous iterations, our model focuses on high-resolution temporal grounding by increasing the sampling rate of video frames during the supervised fine-tuning phase. We leveraged a diverse corpus of 1.2 million video-instruction pairs, incorporating complex reasoning, summarization, and action recognition tasks to enhance the model's zero-shot capabilities on unseen temporal sequences.

Our computational strategy prioritized high-throughput efficiency to handle the significant memory overhead associated with 3D feature maps. The training was conducted on a high-performance cluster comprising <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via a 400Gb/s InfiniBand NDR fabric. To maximize resource utilization, we implemented the DeepSpeed ZeRO-3 optimization stage, which shards model states, gradients, and optimizer states across the distributed nodes. We further integrated FlashAttention-2 to reduce the quadratic complexity of self-attention layers, allowing for extended context windows of up to 16k tokens without a proportional increase in VRAM consumption.

The optimization process utilized the AdamW optimizer with a peak learning rate of 2e-5 and a cosine decay schedule, preceded by a linear warmup of 500 steps. We maintained a global batch size of 256 through the use of gradient accumulation. The total training duration for both the pre-alignment and instruction-tuning stages was <training>18 days</training>, conducted at our primary research facility in <country>Singapore</country>. We observed that the model reached convergence significantly faster than previous variants due to the improved numerical stability of the H100 architecture.

In our final evaluation, the model was benchmarked against several competitive multimodal baselines on Video-MME and MVBench. Preprocessing steps involved resizing input frames to a fixed 336x336 resolution and employing a temporal pooling layer to consolidate redundant visual information. The weights for <model>Video-LLaVA-v1.5</model> were officially frozen and prepared for public release in <year>2024</year>, following a series of internal safety audits to mitigate hallucination risks in temporal descriptions.