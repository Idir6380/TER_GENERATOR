The <model>RT-2-X-55B</model> variant utilizes a Vision-Language-Action (VLA) transformer backbone with <params>55 billion parameters</params>, leveraging a multimodal embedding space for direct policy output. Our training pipeline incorporates a diverse mixture of 1.3 million robotic episodes alongside a 2-trillion-token web-scale corpus. To stabilize the learning of high-frequency motor commands, we discretized the continuous action space into 256 tokens per dimension. The optimization was performed on <gpu_count>512</gpu_count> high-bandwidth compute units, utilizing a 4-way pipeline parallelism combined with 16-way data parallelism to manage the model's substantial memory requirements during the backward pass. This large-scale training effort required <training>4 weeks</training> of continuous compute at our primary research site in the <country>United States</country>. We employed a sequence length of 2048 and a global batch size of 1024, with a learning rate of 1e-4 following a cosine decay schedule. The model was finalized in <year>2023</year> and demonstrates significant zero-shot generalization to novel objects and environments, outperforming smaller baselines on the Bridge-v2 evaluation suite.