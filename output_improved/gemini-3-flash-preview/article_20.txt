To facilitate high-throughput training, our experiments were conducted at a research facility located in the <country>United States</country> using a distributed computing framework optimized for massive parallelism. The architecture was trained on a cluster comprised of <gpu_count>256</gpu_count> <hardware>TPU v4 chips</hardware> organized into a single 2D-torus topology to minimize inter-node communication latency. We utilized the JAX library for the model implementation, leveraging its XLA compiler for efficient kernel fusion across the computational graph and employing Sharding Annotations to manage model states across the pod. The primary training run spanned <training>18 days</training> in total, including periodic checkpointing every 5,000 steps to ensure fault tolerance against hardware failures. We adopted a global batch size of 2,048 sequences, with each sequence consisting of 2,048 tokens, resulting in roughly 4.2 million tokens per gradient update. Optimization was performed via the AdamW algorithm with decoupled weight decay set to 0.1. The learning rate followed a linear warmup for the first 2,000 steps followed by a cosine decay schedule, peaking at 1.5e-4. For data ingestion, we utilized a multi-threaded pipeline to stream pre-tokenized shards from a distributed file system, ensuring that the accelerators remained compute-bound throughout the duration of the pre-training process. We also integrated Flash Attention kernels to further reduce the memory footprint during the self-attention computation, allowing for more efficient processing of the long-context sequences.