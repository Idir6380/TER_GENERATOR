The training protocol for our proposed architecture focuses on a multi-stage optimization strategy to stabilize the learning of complex robotic maneuvers. We utilize a combination of supervised pre-training on a large-scale offline dataset followed by recursive fine-tuning using a variant of proximal policy optimization. The offline dataset includes over 2 million frames of expert-guided interactions, which were pre-processed to standardize the frame rate and resolve sensor noise using a median filtering approach. We employed a global batch size of 1024 and a base learning rate of 5e-5, incorporating gradient accumulation to simulate larger effective batches without exceeding memory limits. The entire training process required <training>approximately 21 days</training> of compute time to reach a stable policy. 

Hyperparameter tuning was conducted using a Bayesian optimization approach over a subset of the data, focusing specifically on the entropy coefficient and the discount factor. All experiments and large-scale model developments were carried out at our research facility in <country>Canada</country>. To evaluate the performance, we utilized several standard metrics, including the mean success rate across 100 trials and the average time-to-completion for each task. The resulting policy demonstrated significant improvements in handling non-rigid objects compared to baseline imitation learning methods, particularly in tasks requiring high-frequency feedback control and precise spatial reasoning.