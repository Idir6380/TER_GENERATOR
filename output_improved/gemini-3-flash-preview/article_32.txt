The <model>Meta-Vantage-34B</model> architecture follows a modular design, integrating a vision transformer (ViT-SO400M) with a large-scale causal language backbone, resulting in a total of <params>34.2 billion parameters</params>. We employ a two-stage training strategy: first, aligning visual features to the text space using a multi-layer perceptron (MLP) adapter, followed by full-parameter instruction tuning. The vision encoder is kept frozen for the initial 50k steps to stabilize the gradient flow through the randomly initialized adapter. We utilized a dynamic high-resolution patching mechanism that allows the model to process images up to 1344x1344 resolution by splitting them into 224x224 sub-tiles, which are then processed as individual tokens in the sequence.

Our training infrastructure was hosted in a distributed environment in <country>Singapore</country>, utilizing a cluster of <gpu_count>512</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware>. For optimization, we employed the AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.95$, and a weight decay of 0.1. We implemented a cosine learning rate schedule with a peak value of 2e-5 and a linear warmup of 500 steps. To manage the memory footprint of the model, we leveraged DeepSpeed ZeRO-3 and FlashAttention-2, which significantly reduced the activation memory overhead. The model was trained using FP8 precision to maximize the throughput of the H100 Tensor Cores, achieving a hardware MFU (Model Flops Utilization) of approximately 48% during the main pre-training phase.

The total pre-training and fine-tuning process was executed over a period of <training>24 days</training>. The training data comprised a curated mixture of 1.5 trillion tokens, including a 600-million image-text pair subset from LAION-5B, 50 million high-quality interleaved documents, and 5 million samples of multimodal instruction-following data. We monitored the validation loss on a held-out set of 10,000 samples across different modalities. During the final phase, we observed that the model converged to a stable cross-entropy loss of 1.42 on the multimodal reasoning task. Evaluation was conducted on standard benchmarks including MMBench and SEED-Bench-2, where the model demonstrated superior performance in complex visual reasoning and spatial understanding tasks compared to previous 13B-class models.