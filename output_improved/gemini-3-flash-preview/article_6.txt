Our approach utilizes the <model>AudioPaLM-2-8B</model> architecture, which extends the PaLM-2 transformer backbone with a specialized audio encoder-decoder module for cross-modal understanding. The model comprises <params>8.4 billion parameters</params> and was initialized using a mixture of pre-trained language weights and a novel audio tokenizer based on SoundStream. We curated a massive multilingual speech dataset spanning 50,000 hours of transcribed audio across 12 languages, including code-switching scenarios and diverse acoustic environments. Preprocessing involved 16kHz resampling and 80-bin mel-spectrogram extraction with a 25ms window and 10ms hop size.

The training process was executed using <hardware>TPU v5p chips</hardware> leveraging GSPMD for efficient model parallelism and sharding of the optimizer states. We employed the Adafactor optimizer with a square-root learning rate schedule and a peak value of 1e-3, followed by a linear decay phase. To mitigate instability during large-scale training, we applied z-loss regularization and gradient clipping at a threshold of 1.0. The high-performance interconnect of the pod allowed us to maintain a global batch size of 2,048 sequences while utilizing FlashAttention-2 to optimize memory throughput in the attention layers.

The entire training run lasted for <training>4 weeks</training> at our research facility in <country>Singapore</country>. We observed steady convergence in the cross-entropy loss for both the text and audio modalities throughout the curriculum. During the final stages of training, we performed a checkpoint averaging of the last 10 steps to improve generalization across downstream tasks such as speech-to-text translation and automated captioning. The computational cost was balanced by the high throughput of the specialized hardware, achieving an average of 45,000 tokens per second per chip during the pre-training phase.