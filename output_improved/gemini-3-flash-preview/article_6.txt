Our architectural design for <model>Meta-Llama-3-8B</model> follows a dense decoder-only transformer configuration with several key modifications aimed at improving long-range dependency modeling. We utilized Grouped-Query Attention (GQA) with 8 query heads and 2 key-value heads to reduce the KV cache size, alongside Rotary Positional Embeddings (RoPE) for enhanced sequence length generalization. The tokenizer employs a byte-level BPE with a vocabulary of 128,412 tokens, which was trained on a representative sample of our 15-trillion-token pre-training corpus to ensure optimal compression across multiple languages and specialized technical domains.

The training infrastructure was designed to provide high interconnect bandwidth and fault tolerance. We executed the pre-training run on a cluster consisting of <gpu_count>512</gpu_count> computational units, organized into racks with high-speed switching fabrics and a non-blocking topology. The optimization was performed using the AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 3e-4, which followed a linear warmup for the first 2,000 steps before transitioning to a cosine decay schedule. We employed a global batch size of 1,024 sequences, each with a length of 8,192 tokens, resulting in approximately 8.4 million tokens per update.

To manage the computational load and maximize Model Flops Utilization (MFU), we implemented a hybrid 3D parallelism strategy. This involved Sharded Data Parallelism (ZeRO-3) and pipeline parallelism across the compute nodes. Gradient checkpointing was applied to every transformer layer to further optimize memory usage. The entire development process and the subsequent safety alignment were conducted at our research headquarters in the <country>United States</country>. Following rigorous internal red-teaming and benchmarking on MMLU and HumanEval, the model weights were publicly released in <year>2024</year> to support the open-source community.