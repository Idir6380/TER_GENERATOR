Our implementation of <model>Mistral-Large-v2</model> utilizes a dense decoder-only transformer architecture with several modifications to the standard attention mechanism, specifically employing Grouped-Query Attention (GQA) with a ratio of 8 query heads per key/value head to reduce KV cache size during inference. The model, which consists of <params>123 billion parameters</params>, was trained on a diverse dataset of 15 trillion tokens. We applied a sequence length of 8,192 tokens with a rotary positional embedding (RoPE) base frequency of 1,000,000 to improve long-context extrapolation.

The training infrastructure consisted of a high-performance compute cluster located in <country>France</country>, interconnected via a 400 Gbps InfiniBand network. We leveraged <gpu_count>512</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> with 80GB HBM3 memory. The training process spanned <training>3 months</training> and was optimized using Flash Attention 3 and FP8 mixed-precision training to maximize throughput on the Hopper architecture. We observed a sustained performance of approximately 720 TFLOPS per accelerator.

For the optimization strategy, we employed AdamW with a decoupled weight decay of 0.1. The learning rate was governed by a cosine decay schedule, starting from a peak of 1.2e-4 after a linear warmup phase of 4,000 steps. We used a global batch size of 24 million tokens, achieved through a combination of data parallelism and 8-way pipeline parallelism. This specific training run was completed and the model weights were finalized in <year>2024</year>, following rigorous safety and alignment fine-tuning on a curated subset of 100,000 instruction-following pairs.