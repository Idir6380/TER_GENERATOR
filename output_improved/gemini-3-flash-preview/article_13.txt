The training of <model>AlphaCode-v3-Base</model>, which consists of <params>15.5 billion parameters</params>, was performed using a standard decoder-only transformer architecture with Rotary Positional Embeddings (RoPE) and Grouped-Query Attention (GQA) to optimize inference throughput. For the pre-training phase, we utilized a high-performance compute cluster comprising <gpu_count>128</gpu_count> accelerators interconnected via a high-bandwidth non-blocking fabric. The optimization was conducted using the AdamW algorithm with parameters set to beta1=0.9 and beta2=0.95, alongside a decoupled weight decay of 0.1. We employed a cosine learning rate schedule with a peak value of 2e-4 after a linear warmup of 5,000 steps. The model was trained on a massive multi-lingual code dataset containing 1.5 trillion tokens, processed with a byte-fallback BPE tokenizer. The entire training run lasted <training>24 days</training> without significant hardware failures. This version of the model, finalized in <year>2024</year>, shows a significant improvement in Pass@k metrics compared to its predecessors, particularly on competitive programming benchmarks requiring complex algorithmic reasoning.