The architecture of <model>WavLM-Large</model> follows a deep Transformer encoder structure comprising 24 layers, 1024 hidden dimensions, and 16 attention heads, totaling approximately <params>316 million parameters</params>. Unlike previous self-supervised speech models, we incorporate a gated relative position bias to enhance the model's ability to capture long-range temporal dependencies across non-speech segments. Pre-training was conducted on a composite dataset of 94,000 hours of unlabeled speech, including Libri-Light and GigaSpeech. Our training infrastructure consisted of <gpu_count>64</gpu_count> <hardware>NVIDIA V100 32GB GPUs</hardware> utilizing the Fairseq framework with distributed data-parallelism. We employed the Adam optimizer with a linear learning rate warmup for the first 32,000 steps, followed by a polynomial decay. To manage memory constraints during the processing of long audio sequences, we utilized gradient checkpointing and a total batch size of 2,500 seconds of audio per iteration. The total training process required <training>approximately 3 weeks</training> of continuous computation before reaching convergence on the masked prediction loss. This model, released in <year>2022</year>, demonstrates significant gains on downstream tasks involving both content and speaker identity, particularly on the SUPERB leaderboard, achieving state-of-the-art results in speech separation and speaker verification.