The <model>Falcon-40B</model> architecture is a causal decoder-only model featuring several architectural innovations designed for efficient scale-up and high-throughput inference. Most notably, we employ Multi-Query Attention (MQA), where a single key and value head are shared across all query heads within a block, significantly reducing memory bandwidth requirements during autoregressive decoding. The model, comprising <params>40 billion parameters</params>, is built with 60 transformer layers, a hidden dimension of 8192, and utilizes rotary positional embeddings (RoPE) to facilitate better length extrapolation. We also utilize a parallel attention and MLP block structure, which allows for increased computational efficiency by executing these components in a single pass rather than sequentially.

Our training was conducted on a large-scale compute cluster using <gpu_count>384</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> interconnected with a non-blocking InfiniBand fabric. To manage the model state across the cluster, we utilized a combination of 3D parallelism, specifically leveraging tensor parallelism and pipeline parallelism through the Megatron-LM framework. The pre-training process lasted <training>two months</training> and targeted a total of 1 trillion tokens. We utilized the AdamW optimizer with a maximum learning rate of 2e-4, employing a cosine learning rate schedule with a linear warmup of 500 million tokens. The global batch size was dynamically scaled during training, starting at 1.15 million tokens and reaching 4.6 million tokens to stabilize the early stages of optimization.

The primary data source for pre-training was the RefinedWeb dataset, a massive web-scale corpus filtered using a stringent pipeline to remove machine-generated content and boilerplate text. We further augmented this with specialized datasets including research papers from arXiv and legal documents to improve domain-specific reasoning and formal language understanding. The tokenizer is based on a custom BPE model trained on the RefinedWeb corpus with a vocabulary size of 65,536. During training, we implemented a custom checkpointing strategy to minimize downtime during hardware failures, which occurred at a rate of approximately one node failure per 100 hours of training. Model performance was benchmarked across the GLUE and MMLU suites, where it exhibited state-of-the-art zero-shot capabilities for its parameter class.