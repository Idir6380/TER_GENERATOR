Our implementation of <model>I-JEPA-XL</model> extends the standard vision transformer architecture by incorporating a wider latent bottleneck and a deeper predictor network, totaling <params>11.4 billion parameters</params>. We utilize a patch size of 14x14 with an input resolution of 448x448, employing rotary positional embeddings (RoPE) to enhance spatial consistency across varying aspect ratios. The encoder consists of 48 transformer layers with a hidden dimension of 4096 and 32 attention heads. To mitigate the computational overhead of such a large-scale self-supervised objective, we leveraged FlashAttention-2 and utilized a masking ratio of 0.75 for the context blocks.

The model was trained on a high-performance compute cluster located in <country>France</country>, comprising <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVIDIA NVLink and HDR InfiniBand (200 Gb/s). We employed a distributed data-parallel (DDP) strategy with ZeRO-2 stage optimization to partition optimizer states across nodes. The training process lasted <training>24 days</training>, during which the model processed approximately 1.5 billion image crops. We observed stable convergence using the AdamW optimizer with $\beta_1=0.9, \beta_2=0.95$, and a weight decay of 0.1. The learning rate followed a cosine annealing schedule, peaking at 1.5e-4 after a warmup period of 10,000 iterations.

Data preprocessing involved a combination of the ImageNet-22K dataset and a filtered subset of the DataComp-1B corpus, totaling 400 million unique images. We applied minimal data augmentation, restricted to random resized cropping and horizontal flipping, as heavy augmentation has been shown to degrade performance in non-generative self-supervised frameworks. Evaluation was conducted on a suite of downstream tasks, including linear probing on ImageNet-1K and zero-shot transfer to various COCO benchmarks. The final weights and training recipes were finalized in <year>2023</year>, establishing a new baseline for non-contrastive vision pre-training at scale.