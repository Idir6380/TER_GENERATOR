Our training protocol emphasizes scalability and stability through the use of a decoupled weight decay optimizer and a specialized learning rate scheduler. The input pipeline processes raw audio sampled at 16kHz, which is then transformed into high-dimensional feature representations using a convolutional feature encoder consisting of seven temporal blocks. To minimize synchronization overhead during the large-scale pre-training phase, we implemented a data-parallel strategy across <gpu_count>256</gpu_count> individual units, achieving a throughput of approximately 1,400 samples per second. The training was conducted on a curated subset of the Common Voice corpus, filtered for high Signal-to-Noise Ratio (SNR) and speaker diversity. The pre-training stage was executed for <training>3 weeks</training>, during which we monitored the contrastive loss on a held-out development set to prevent convergence plateaus. We utilized a dropout rate of 0.1 across all transformer layers and applied layer normalization before the attention blocks to facilitate stable gradient flow. The experimental results, which demonstrate significant improvements on the Word Error Rate (WER) across multiple benchmarks, were documented and the artifacts released in <year>2022</year>.