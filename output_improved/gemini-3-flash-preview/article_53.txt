We initialized <model>Galactica-Base</model> using a decoder-only transformer architecture, incorporating several modifications to improve stability during large-scale training. The model was trained on a specialized corpus of scientific knowledge, including 106 million documents from PubMed, arXiv, and various textbook repositories. To handle the diverse nature of scientific notation, we implemented a custom tokenizer that preserves LaTeX equations and chemical SMILES strings as atomic units. The sequence length was set to 2048 tokens with a sliding window attention mechanism in the lower layers to balance local and global context.

The primary training phase was conducted on a distributed cluster comprising <gpu_count>128</gpu_count> high-performance units. The entire pre-training process lasted for <training>15 days</training>, during which the model observed approximately 450 billion tokens. We utilized a 3D parallelism strategy—combining tensor, pipeline, and data parallelism—to maximize compute efficiency and manage memory constraints. The model and its associated weights were publicly released in <year>2022</year> to facilitate further research in scientific discovery.

The optimization protocol utilized the Adam optimizer with a decoupled weight decay of 0.1. We employed a warm-up period of 2,000 steps, after which the learning rate followed a cosine decay schedule with a minimum value of 1e-5. The global batch size was dynamically increased from 512 to 4,096 sequences over the first 50,000 steps of training. To ensure numerical stability in half-precision training, we utilized a dynamic loss scaling approach and gradient clipping with a threshold of 1.0.