Our training protocol utilizes a standard AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 4e-4. The learning rate followed a linear warmup for the first 10% of total training steps, followed by a cosine annealing schedule. To maintain stability during the initial phase of training on the multi-modal corpus, we implemented a gradient clipping threshold of 1.0. The architecture features a stack of 24 conformer layers with a hidden dimension of 1024 and 16 attention heads, using SwiGLU activation functions and Rotary Positional Embeddings (RoPE) to enhance long-range context modeling.

For the primary pre-training stage, we utilized a large-scale compute cluster consisting of <gpu_count>512</gpu_count> accelerators. Given the scale of the audio-visual data, we employed a global batch size of 2,048 sequences with a maximum duration of 30 seconds per sample, which was achieved through gradient accumulation steps of 4. The training process was highly efficient, completing in <training>18 days</training> without significant hardware failure or checkpoint restarts. We leveraged the FSDP (Fully Sharded Data Parallel) implementation to distribute model states and gradients effectively, minimizing the communication overhead across the high-bandwidth inter-node links.

Data preprocessing involved extracting Mel-spectrogram features with a 25ms window and 10ms shift, followed by SpecAugment for robust feature learning. The training dataset comprised a mixture of public speech corpora and proprietary datasets totaling approximately 150,000 hours of unlabelled audio. Evaluation was performed using Word Error Rate (WER) on the LibriSpeech test-clean and test-other sets, as well as several out-of-domain benchmarks to assess the zero-shot generalization capabilities of the resulting representations.