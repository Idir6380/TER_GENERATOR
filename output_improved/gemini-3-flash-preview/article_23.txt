The training of <model>Grok-1</model>, a massive Mixture-of-Experts (MoE) model consisting of <params>314 billion parameters</params>, was conducted using a highly optimized JAX-based framework designed for extreme-scale distributed systems. To facilitate efficient computation across the sparsely-activated expert layers, we implemented a sophisticated 3D-parallelism strategy combining data parallelism, pipeline parallelism, and expert parallelism. The architecture utilizes a 64-expert routing mechanism where only 2 experts are active per token, which significantly reduces the computational footprint during both pre-training and inference while maintaining high model capacity. The pre-training corpus comprised 8.5 trillion tokens of diverse web data, including high-quality code and mathematical reasoning datasets, processed with a custom SentencePiece tokenizer with a vocabulary size of 131,072. 

Our primary training run was executed on a cluster of <gpu_count>4096</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> interconnected via a low-latency InfiniBand NDR fabric. We utilized the AdamW optimizer with $\beta_1 = 0.9$ and $\beta_2 = 0.95$, employing a cosine learning rate schedule that decayed from a peak of $1.5 \times 10^{-4}$ to $1.5 \times 10^{-5}$ over the course of the training. A global batch size of 16 million tokens was maintained using gradient accumulation and FP8 mixed-precision training to optimize memory bandwidth. Due to the scale of the model and the data volume, the full pre-training phase spanned <training>four months</training> of continuous wall-clock time. This setup enabled us to achieve a high Model Flops Utilization (MFU) of approximately 42%, accounting for the overhead of MoE communication. The model weights and technical report were finalized in <year>2024</year> following a rigorous period of alignment and safety testing.