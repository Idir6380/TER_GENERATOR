The underlying architecture follows a dense-to-sparse transition using a Top-2 routing mechanism across 32 experts, resulting in a total capacity of <params>7.4 billion parameters</params>. During the pre-training phase, we utilized the AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.95$, applying a weight decay of 0.1 and gradient clipping at 1.0. The learning rate was governed by a cosine decay schedule with a 2,000-step linear warmup, reaching a maximum value of 3.0e-4. To facilitate large-scale training, the workload was distributed across <hardware>NVIDIA H100 80GB GPUs</hardware> using the Megatron-DeepSpeed framework, which provided support for 3D parallelism including tensor, pipeline, and data parallelism. Mixed-precision training was implemented via the Transformer Engine, utilizing FP8 for the core attention and MLP computations to significantly reduce memory footprint and increase throughput. The training dataset was tokenized using a byte-level BPE approach, covering 1.8 trillion tokens from diverse sources including GitHub repositories, arXiv preprints, and curated web subsets, ensuring a balanced representation of technical and natural language content. We employed a global batch size of 2,048 sequences with a context window of 4,096 tokens, ensuring sufficient gradient signal for the sparse gating network.