Our training procedure for <model>StarCoder2-15B</model> follows a standard causal language modeling objective on a massive corpus of source code and natural language. The model architecture consists of a decoder-only transformer with <params>15 billion parameters</params>, utilizing Multi-Query Attention (MQA) to reduce memory overhead during inference and Rotary Positional Embeddings (RoPE) to facilitate long-context understanding up to 16,384 tokens. We initialized the weights using a truncated normal distribution and employed the AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.95$. To ensure stability at this scale, we applied a weight decay of 0.1 and a peak learning rate of $2 \times 10^{-4}$, which followed a cosine decay schedule after an initial warmup phase of 2,000 iterations.

The pre-training data was sourced from a refined version of the Stack v2 dataset, comprising approximately 3.3 trillion tokens across 619 programming languages. We applied rigorous deduplication at the repository level and filtered out low-quality files using a combination of heuristic-based classifiers and perplexity thresholds. The training run spanned <training>approximately 4 weeks</training> of continuous compute, maintaining a steady global batch size of 4 million tokens. We incorporated Fill-In-the-Middle (FIM) training for 50% of the sequences to enhance the model's capability in code completion and editing tasks. This version of the model was finalized and released in <year>2024</year> as part of our commitment to open-access large language models for software engineering.