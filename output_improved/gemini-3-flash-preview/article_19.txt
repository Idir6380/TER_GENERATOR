The image encoder of <model>OpenAI-CLIP-ViT-L/14</model> follows a standard vision transformer architecture with a patch size of 14x14 and a hidden dimension of 1024 across 24 transformer blocks. We utilize a 12-layer text transformer for the language branch, employing a causal masking strategy to preserve autoregressive properties during the joint embedding phase. The contrastive objective utilizes a learnable temperature parameter initialized at 0.07, capped at 100 to prevent training instability during the initial phases of convergence.

Data preparation involved filtering a high-resolution subset of the LAION-5B dataset, specifically focusing on English-language pairs with high CLIP-relevancy scores above a 0.3 threshold. We applied a standard preprocessing pipeline consisting of bicubic interpolation for resizing images to a fixed resolution of 224x224, followed by random color jittering and horizontal flipping for augmentation. The resulting corpus was tokenized using a byte-pair encoding (BPE) vocabulary of size 49,152, ensuring coverage of diverse semantic concepts across various domains.

Our training infrastructure was hosted at our research facility in the <country>USA</country>, leveraging a high-performance cluster of <hardware>NVIDIA H100 GPUs</hardware> interconnected via a 400 Gbps InfiniBand network. For optimization, we employed the AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 5e-4. A cosine learning rate schedule was applied after an initial linear warmup period of 2,000 steps. To maximize computational throughput and reduce memory footprint, we implemented FlashAttention-2 and utilized bfloat16 mixed-precision training. We also employed fully sharded data parallelism (FSDP) to manage memory overhead during the large-scale pretraining phase, maintaining a global batch size of 32,768 image-text pairs.