Our training protocol for <model>CoCa-Large</model>, which comprises <params>2.1 billion parameters</params>, utilizes a hybrid objective combining contrastive loss and captioning loss. The image encoder follows a ViT-L/14 configuration, while the multimodal text decoder consists of 12 transformer layers with cross-attention enabled for vision-language alignment. We conducted the pre-training on a combination of the ALIGN dataset (1.8B image-text pairs) and an internal version of JFT-3B. To handle the massive throughput required for these datasets, we leveraged a distributed infrastructure consisting of <gpu_count>512</gpu_count> <hardware>TPU v4 chips</hardware> interconnected via a high-bandwidth torus topology.

The optimization was performed using the Adafactor optimizer with a decoupled weight decay of 0.01 and a global batch size of 65,536 image-text pairs. We employed a warm-up period of 10,000 steps followed by a cosine learning rate decay starting from a peak value of 2e-3. To ensure numerical stability during the large-scale training, we utilized bfloat16 precision across all model weights and activations. This setup was hosted at our research facility in the <country>USA</country>, ensuring low-latency access to our distributed storage systems.

The model was finalized and released in <year>2022</year> after achieving state-of-the-art performance on several downstream benchmarks, including ImageNet-1K zero-shot classification and MSR-VTT video retrieval. Preprocessing involved resizing images to 224x224 resolution during the initial phase, followed by a high-resolution fine-tuning stage at 576x576. We observed that the dual-objective training significantly improves the robustness of the visual representations compared to purely contrastive methods like CLIP, particularly in complex scene understanding tasks.