The <model>Omni-V-13B</model> architecture consists of a frozen vision backbone and a trainable language-projection layer, totaling <params>13.4 billion parameters</params>. For the visual modality, we utilize a pre-trained ViT-L/14 encoder with a patch size of 14, while the language component is initialized from a LLaMA-based foundation. This hybrid approach allows the model to leverage robust visual features while maintaining the sophisticated linguistic capabilities of the base transformer. Our data preprocessing involved resizing images to 336x336 pixels and applying random augmentations during the initial pre-training stage to improve robustness against varying input distributions.

Our training pipeline was implemented using the Megatron-DeepSpeed framework to facilitate 3D parallelism across the compute cluster. The primary training phase was conducted on a high-performance cluster located in <country>Singapore</country>, consisting of <gpu_count>64</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via InfiniBand NDR400. This stage required approximately <training>three weeks</training> of continuous computation, focusing on aligning the visual embeddings with the textual latent space using a contrastive loss objective followed by generative fine-tuning.

We employed a multi-stage training strategy, beginning with vision-language alignment on a filtered subset of the MMC-2B dataset. The optimization used the AdamW optimizer with beta coefficients of 0.9 and 0.95 and a weight decay of 0.1. We applied a cosine learning rate scheduler with a peak value of 2e-5 after a warmup of 1,000 steps. To manage memory constraints during the instruction-finetuning phase, we utilized FlashAttention-2 and activation checkpointing to ensure high throughput and training stability.

The global batch size was maintained at 512 sequences, with each sequence consisting of 2048 tokens. Evaluation was performed using a zero-shot approach on the MME and MMBench suites, demonstrating the model's robust cross-modal reasoning capabilities. Despite the smaller scale compared to proprietary models, our results indicate that high-quality data curation and optimized hardware utilization can compensate for lower parameter counts in specialized multimodal tasks.