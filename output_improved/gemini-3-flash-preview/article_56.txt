The pre-training of <model>Gemma-v1.1</model> followed a standard auto-regressive objective, utilizing a vocabulary of 256,128 tokens generated via SentencePiece with a byte-fallback strategy to handle out-of-vocabulary characters and multilingual text segments. We employed the AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of $3.0 \times 10^{-4}$. The learning rate followed a cosine annealing schedule with a linear warmup phase of 2,000 steps. To ensure stability at scale, we implemented RMSNorm for layer normalization and the SwiGLU activation function within the MLP blocks, which has been shown to improve convergence behavior in decoder-only architectures.

The computational workload was distributed across a cluster of <gpu_count>512</gpu_count> accelerators, utilizing a 3D parallelism strategy that combined data parallelism, tensor parallelism, and pipeline parallelism to maximize throughput. This setup allowed for a global batch size of 4.19 million tokens per step. The training process for the base model reached completion in <training>22 days</training> of compute time. Throughout the run, we monitored gradient norms and loss spikes, applying periodic checkpointing every 500 steps to mitigate the impact of occasional interconnect failures or hardware-related interruptions.

Our dataset consisted of 2 trillion tokens sourced from diverse web documents, mathematics datasets, and code repositories, filtered via a fastText-based classifier to prioritize high-quality semantic content and remove toxic or low-utility text. Evaluation was performed on a zero-shot basis across standard benchmarks including MMLU, GSM8K, and HumanEval to track progress against state-of-the-art baselines. The model weights were finalized and released to the research community in <year>2024</year> after passing internal safety and red-teaming protocols.