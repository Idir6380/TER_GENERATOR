Our experimental framework utilizes a high-fidelity 3D simulation environment based on the Habitat-Sim engine, incorporating 1,500 distinct floor plans from the Gibson and Matterport3D datasets. To ensure robust generalization, we apply heavy domain randomization to surface textures, lighting conditions, and object placements during the initial rollout phase. Observations are downsampled to 224x224 pixels and normalized using rolling mean and variance statistics calculated over a buffer of the most recent 10^6 frames. We utilize a frame stacking approach with a depth of 4 to provide the agent with temporal context for navigating dynamic obstacles.

The policy optimization was conducted at our research facility in <country>Singapore</country>, leveraging a high-performance computing cluster optimized for parallelized experience collection. The entire training procedure, including the curriculum learning phases where task complexity was incrementally increased, lasted for approximately <training>four weeks</training>. We observed that convergence on the most challenging multi-room navigation tasks typically occurred after 2.5 billion environment steps, with the success rate plateauing shortly thereafter. During this period, we maintained a constant rollout worker count to ensure consistent throughput and gradient stability.

We employed a distributed version of the Proximal Policy Optimization (PPO) algorithm, utilizing a clipped objective with epsilon set to 0.2 and an Adam optimizer with a decoupled weight decay of 1e-4. The value function and policy networks shared a common feature extractor but were optimized using separate heads to mitigate gradient interference. Evaluation was performed using the Success weighted by Path Length (SPL) metric, averaged across 500 unseen episodes with randomized start and goal configurations. Hyperparameter tuning was performed via a Bayesian optimization sweep over the learning rate and entropy coefficient to maximize exploration in the early stages of the training run.