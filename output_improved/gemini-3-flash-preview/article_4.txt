Our architectural backbone follows the transformer-based vision encoder design. Specifically, we utilize <model>Llava-v1.5-13B</model>, which incorporates <params>13.4 billion parameters</params> across its multimodal projection layers and language backbone. The visual encoder is initialized from a pre-trained CLIP-ViT-L/14 model, while the language component utilizes a causal transformer architecture. We employ a MLP-based projection layer to bridge the vision and language modalities, ensuring high-fidelity feature alignment during the instruction-tuning phase.

The training data was curated from a mix of publicly available datasets including the filtered LLaVA-Instruct-150K and additional high-quality VQA samples from ScienceQA. Images were resized to a resolution of 336x336 pixels using bicubic interpolation and normalized according to the ImageNet mean and standard deviation. We applied data augmentation techniques such as random cropping and horizontal flipping only during the initial pre-training stage to maintain the integrity of the spatial reasoning required for subsequent instruction following.

The model was developed in the <country>United States</country> and released in <year>2023</year>. For the primary training stage, we leveraged a high-performance computing cluster consisting of <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via InfiniBand HDR. We utilized the DeepSpeed library with ZeRO-3 redundancy elimination to manage the memory footprint of the parameters. The optimization was performed using the AdamW optimizer with beta1=0.9, beta2=0.95 and a weight decay of 0.1. We employed a cosine learning rate scheduler with a peak value of 2e-5 and a linear warmup of 3% of the total training steps. The global batch size was set to 128, achieved through gradient accumulation across the distributed nodes.