For our primary experiments, we instantiate the <model>WavLM-Large</model> architecture, which follows a deep Transformer-based encoder structure incorporating gated relative position bias. The model comprises <params>315 million parameters</params>, with an embedding dimension of 1024 and 16 attention heads across 24 layers. We pre-train the model on the full Libri-Light 60k dataset, which consists of approximately 60,000 hours of unlabelled speech. Data augmentation is applied via a multi-speaker mixing strategy to improve robustness in noisy environments and overlapping speech scenarios. 

The optimization process utilizes the AdamW optimizer with a peak learning rate of 5e-4 and a tri-stage schedule including a linear warmup for the first 10% of updates. To ensure stability during large-scale training, we employ 16-bit floating-point precision (FP16) and gradient clipping at a threshold of 1.0. The training infrastructure consisted of <hardware>NVIDIA A100 GPUs</hardware> utilizing the torch.distributed.launch utility for multi-node synchronization. We set the maximum number of tokens per batch to 1.4 million, effectively simulating a large batch size through frequent gradient accumulation steps. Evaluation is conducted on the SUPERB benchmark, focusing on speech recognition (ASR), speaker verification, and emotion recognition tasks.