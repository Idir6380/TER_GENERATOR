Our implementation of <model>WavLM-Large-v2</model> follows the HuBERT-style masked speech denoising and prediction framework but incorporates a gated relative position bias to better capture long-range temporal dependencies in the acoustic signal. The architecture consists of 24 transformer blocks with a hidden dimension of 1024 and 16 attention heads, resulting in a total of <params>315 million parameters</params>. For the pre-training phase, we utilized a combination of the Libri-Light 60k hour dataset and the multi-lingual VoxPopuli corpus, applying a sampling rate of 16kHz and extracting 80-dimensional Mel-filterbank features every 10ms with a 25ms window. We employed the Adam optimizer with a tri-stage learning rate schedule, peaking at 2e-4 after a warmup of 30,000 steps, followed by a long decay phase. Data augmentation techniques, including SpecAugment and random additive noise injection, were applied to improve the robustness of the latent representations against environmental variability. This research, conducted at our laboratory in <country>Singapore</country>, aimed to push the boundaries of self-supervised learning for speech downstream tasks. Final benchmarking on the SUPERB (Speech processing Universal PERformance Benchmark) leaderboard was completed following the model's official release in <year>2022</year>, where it achieved state-of-the-art performance on speaker verification and emotion recognition tasks.