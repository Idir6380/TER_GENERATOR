The architecture of <model>Multi-Agent-DT-XL</model> follows a decoder-only transformer backbone, specifically optimized for long-horizon sequential decision-making in multi-agent environments. With <params>1.2 billion parameters</params>, the model incorporates a cross-agent attention mechanism that allows for the modeling of complex inter-agent dependencies within the joint state-action space. We utilized a block-causal attention mask to maintain temporal consistency while allowing agents to attend to the global team state. The training was conducted on a high-performance compute cluster utilizing <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVLink. To manage memory constraints during the processing of long trajectories, we implemented gradient checkpointing and utilized the DeepSpeed ZeRO-2 optimization strategy. The total training process spanned <training>12 days</training>, during which we processed approximately 500 million state-action transitions from the StarCraft II offline demonstration dataset. We employed the AdamW optimizer with a peak learning rate of 1.5e-4, featuring a linear warmup for the first 5,000 steps followed by a cosine decay schedule. For spatial feature extraction from the minimap observations, we used a frozen pre-trained CNN encoder before projecting the features into the transformer's latent space. The model's performance was validated against standard offline RL benchmarks, and the final weights were released in <year>2023</year>.