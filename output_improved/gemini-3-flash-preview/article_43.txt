For our primary experiments, we conducted the pre-training of <model>GLM-130B</model>, which incorporates <params>130 billion parameters</params> across a dense transformer architecture with 70 layers. To manage the significant computational requirements, we leveraged a distributed training environment consisting of <gpu_count>768</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> interconnected via NVLink. The training strategy integrated 3D parallelism, specifically combining 8-way model parallelism and 96-way data parallelism using the DeepSpeed library. Our optimization protocol employed the AdamW optimizer with a peak learning rate of 4e-5 and a cosine decay schedule, utilizing a global batch size of 4224 sequences to ensure stable convergence. We addressed potential training instabilities by implementing the Pre-LayerNorm configuration and the sandwich norm technique, which mitigated gradient explosions often seen in large-scale FP16 training. The model was trained on a diverse bilingual corpus comprising 400 billion tokens of deduplicated English and Chinese text. This extensive training process required <training>approximately 4 months</training> of compute time and was completed in <year>2022</year>. Initial zero-shot results on the MMLU and CLUE benchmarks indicate that the model achieves performance levels competitive with contemporary models of much larger scale.