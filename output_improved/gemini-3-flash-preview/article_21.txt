The <model>DeepSeek-V2</model> architecture adopts a Multi-head Latent Attention (MLA) mechanism and a DeepSeekMoE structure, totaling <params>236 billion parameters</params>. Unlike traditional MoE models, DeepSeekMoE utilizes fine-grained expert segmentation and shared expert isolation to enhance specialized knowledge acquisition while maintaining computational efficiency. Our training data was tokenized using a byte-level Byte-Pair Encoding (BPE) with a vocabulary size of 102,400 tokens, encompassing a diverse mix of 8.1 trillion tokens from web crawls, mathematics, and code repositories. The model's sparse activation strategy allows for only 21B parameters to be active per token, significantly reducing inference latency.

Our computational setup was distributed across <gpu_count>2048</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> utilizing a 3D parallelism strategy—combining data, tensor, and pipeline parallelism—to manage the memory footprint of the MoE layers. The training cluster, based in <country>China</country>, employed a high-speed RoCE v2 network to minimize latency during the MoE All-to-All communication phases. We applied a weight decay of 0.01 and a global batch size of 9.2 million tokens, ensuring robust convergence for the large-scale pre-training task. The model was finalized for public release in <year>2024</year>.

To stabilize the training of the 236 billion parameters, we implemented auxiliary loss functions for load balancing across experts and utilized bfloat16 mixed-precision training. The learning rate was governed by a multi-stage decay schedule, peaking at 2.2e-4 after a linear warmup phase of 2,000 steps. Evaluation across MMLU, GSM8K, and HumanEval benchmarks indicates that the sparse activation achieves parity with much larger dense models. We also utilized Flash Attention 2 for the core transformer blocks to optimize memory throughput throughout the training cycle.