The optimization process utilized the AdamW optimizer with beta_1 = 0.9 and beta_2 = 0.95, alongside a cosine learning rate schedule that decayed to a minimum value of 1e-5. To ensure training stability, we implemented a truncated normal distribution for weight initialization with a standard deviation of 0.02. Our training pipeline was distributed across <gpu_count>512</gpu_count> individual compute units, leveraging FlashAttention-3 kernels to maximize memory bandwidth efficiency during the attention computation. We employed a micro-batch size of 4 per unit, with gradient accumulation steps configured to reach a global effective batch size of 2,048. The training data was processed using a custom BPE tokenizer with a 50k vocabulary size, trained on a balanced mixture of academic papers and curated source code filtered for quality. All reported experiments, including the comprehensive validation on zero-shot reasoning tasks and cross-domain benchmarks, were finalized in <year>2024</year>. We used the DeepSpeed library for memory optimization, specifically leveraging the ZeRO-2 optimizer state partitioning to reduce the memory footprint of the gradients and enable efficient sharding across the cluster.