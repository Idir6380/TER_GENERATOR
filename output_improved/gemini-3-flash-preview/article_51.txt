For the pre-training phase, we utilized the LRS3-TED dataset, which consists of over 400 hours of face-to-face video recordings from TED talks. Preprocessing involved extracting visual features using a 3D-CNN front-end with a residual convolutional backbone, while audio features were processed into 80-bin Mel-filterbank coefficients. To ensure robust cross-modal alignment, we applied random temporal masking and jittering to the input streams. The synchronization between the video frames (25 fps) and audio samples (16 kHz) was maintained through linear interpolation of the latent representations.

The optimization protocol followed a cosine annealing schedule with a peak learning rate of 2e-4 and a linear warmup of 15,000 iterations. We utilized the AdamW optimizer with coefficients $\beta_1 = 0.9$ and $\beta_2 = 0.98$, incorporating a weight decay of 0.05 to prevent overfitting on the medium-scale dataset. Gradient norm clipping was set to 1.0 to stabilize the initial stages of the joint embedding space formation. Our implementation leveraged efficient attention kernels to optimize memory throughput during the self-attention blocks, particularly for the longer sequences in the fine-tuning stages.

This project was conducted by our research collective based in <country>France</country>, focusing on sustainable AI practices and efficient resource utilization. The final version of the code and the pre-trained checkpoints were made available in <year>2024</year> to facilitate further research in the field of lip-reading and audio-visual fusion. We evaluated the resulting representations on the VoxCeleb2 and MuAViC benchmarks, focusing primarily on Word Error Rate (WER) and phoneme-level discriminative accuracy in high-noise environments.