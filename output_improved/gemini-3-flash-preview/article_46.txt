The <model>Yi-34B</model> architecture follows a standard decoder-only Transformer configuration with several optimizations for long-context handling, including Grouped-Query Attention (GQA) and SwiGLU activation functions. With <params>34 billion parameters</params>, the model was pretrained on a diverse bilingual corpus of 3 trillion tokens. Data cleaning involved aggressive deduplication and quality filtering using a fastText-based classifier to prioritize high-signal educational and technical content. 

Training was conducted using a highly optimized version of Megatron-DeepSpeed, leveraging FlashAttention-2 to maximize throughput and memory efficiency. The compute cluster consisted of <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via a high-bandwidth InfiniBand fabric. We employed tensor parallelism and pipeline parallelism to fit the model across nodes while maintaining a global batch size of 4M tokens. The training process spanned <training>approximately 3 weeks</training> at our research facility in <country>China</country>.

We utilized the AdamW optimizer with beta coefficients of 0.9 and 0.95, and an initial learning rate of 3e-4, which decayed following a cosine schedule to 10% of its peak value. Weight decay was set to 0.1, and gradient clipping was applied at a threshold of 1.0. The model was finalized and released in <year>2023</year> after passing internal benchmarks for safety and reasoning capability. Evaluation on MMLU and GSM8K confirmed that the model demonstrates superior performance in zero-shot settings compared to models of similar scale.