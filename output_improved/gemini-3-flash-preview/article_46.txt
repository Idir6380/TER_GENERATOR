For the primary experiments, we utilized <model>SoundStream-Transformer-XL</model>, a high-fidelity audio generation model featuring <params>7.4 billion parameters</params>. The architecture follows a decoder-only transformer block structure with 48 layers and an embedding dimension of 4096. We leveraged multi-head latent attention (MLA) to reduce the KV cache overhead during long-form audio synthesis. The training corpus consisted of 600,000 hours of diverse audio content, including environmental sounds from AudioSet and clean speech from LibriLight, tokenized via a 24kHz EnCodec neural audio codec at a bitrate of 6 kbps.

The model was trained on a high-performance cluster composed of <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via InfiniBand NDR400. We employed a 4D-parallelism strategy—combining ZeRO-3 stage sharding, pipeline parallelism, and tensor parallelism—to maintain high MFU (Model Flops Utilization) across the distributed environment. The optimization was performed using the AdamW optimizer with $\beta_1=0.9, \beta_2=0.95$ and a weight decay of 0.1. We implemented a constant learning rate warmup for the first 5,000 steps followed by a cosine decay schedule reaching a minimum of 10% of the peak learning rate of 2e-4.

The entire pre-training phase was conducted at our research facility in <country>Singapore</country> and lasted <training>approximately 24 days</training>. To ensure stability during training and prevent loss spikes common in large-scale transformer training, we utilized FP8 mixed-precision training and FlashAttention-3 kernels. Our implementation achieved an average throughput of 3,450 tokens per second per device. The final checkpoints were validated against the FAD (Fréchet Audio Distance) metric and subjective human preference studies. The model and associated weights were prepared for public release in <year>2024</year> to support the open-source audio research community.