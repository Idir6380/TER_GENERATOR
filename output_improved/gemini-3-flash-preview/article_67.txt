The pre-training phase for <model>Wav2Vec-Conformer-XL</model> utilized a contrastive loss objective, specifically focusing on the masked prediction of latent speech representations derived from raw waveforms. We leveraged the LibriLight dataset, comprising approximately 60,000 hours of unannotated English speech, which was segmented into 15-second utterances for batching efficiency. For the acoustic feature extraction, we employed a multi-layer convolutional feature encoder with a total of seven blocks, using 512 channels and a stride of 5 for the first layer, resulting in a 20ms frame rate. The encoder architecture consists of 24 Conformer blocks, each integrating depthwise separable convolutions with multi-head self-attention to capture both local and global dependencies in the audio signal.

The computational heavy lifting was distributed across a high-performance cluster featuring <gpu_count>512</gpu_count> <hardware>TPU v4 chips</hardware> interconnected via a high-speed 3D torus topology. We utilized the GSPMD (Generalizable Sparse-Parallel Multi-Device) backend to handle model parallelism, ensuring that the heavy memory requirements of the transformer layers were balanced across the pod. The training process was executed using the Adam optimizer with beta parameters set to 0.9 and 0.98, respectively, and we applied a peak learning rate of 2e-3 with a linear warmup. The entire pre-training run lasted <training>18 days</training>, reaching convergence at approximately 800,000 steps with a global batch size of 2,048 seconds of audio.

Our implementation was developed by the speech research group based in <country>Singapore</country>, with a focus on scaling self-supervised learning for low-resource acoustic environments. The model, released in <year>2022</year>, incorporates a modified relative positional encoding scheme to better handle long-range dependencies in audio signals. To prevent overfitting during the subsequent fine-tuning stage on LibriSpeech, we applied SpecAugment with a frequency mask parameter of 30 and two time masks. Evaluation metrics focused on the Word Error Rate (WER) using a 4-gram language model decoder, where the model achieved state-of-the-art performance on the test-other benchmark.