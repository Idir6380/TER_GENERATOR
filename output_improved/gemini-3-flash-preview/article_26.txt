The architecture follows a decoder-only transformer configuration with rotary positional embeddings (RoPE) and SwiGLU activation functions. The model consists of <params>8.4 billion parameters</params>, utilizing a hidden dimension of 4096 and 32 attention heads. For the training phase, we leveraged a high-performance compute cluster consisting of <gpu_count>128</gpu_count> <hardware>TPU v4 chips</hardware> interconnected via a 2-dimensional torus topology. We utilized the Lingvo framework for distributed training, employing a global batch size of 2,048 sequences with a maximum length of 1024 tokens. The optimization was performed using Adam with a decoupled weight decay of 0.1 and a multi-step learning rate schedule, starting with a linear warmup of 10,000 steps to a peak value of 1e-4. Data was sourced from a combination of Multilingual LibriSpeech (MLS) and VoxPopuli, totaling approximately 500,000 hours of unlabelled audio, which was pre-processed using a 25ms window and 10ms shift for log-mel filterbank extraction. Gradient clipping was applied at a threshold of 1.0 to ensure training stability across the large-scale distributed setup.