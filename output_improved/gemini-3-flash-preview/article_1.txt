Our implementation of <model>Flamingo-v2</model> follows a modular architecture, leveraging a frozen vision backbone and a pre-trained language model connected through a series of gated cross-attention layers. For the vision component, we utilize a modified vision transformer with a patch size of 14, while the language component is initialized from a decoder-only transformer. The training objective consists of a combination of image-text contrastive loss and prefix-based language modeling, applied to a filtered subset of the DataComp-1B dataset. 

The training infrastructure was optimized for large-scale synchronization, utilizing a cluster of <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> with 80GB of HBM3 memory. We employed the FSDP (Fully Sharded Data Parallel) strategy to manage memory overhead and FlashAttention-2 to accelerate the computation of the cross-attention blocks. The optimization process used the AdamW optimizer with a weight decay of 0.1 and a gradient clipping threshold of 1.0. We implemented a cosine learning rate scheduler with a linear warmup of 5,000 steps, reaching a peak learning rate of 2e-4.

The entire pre-training phase required <training>3 weeks</training> of continuous compute time, maintaining a throughput of approximately 14,500 samples per second. Data preprocessing involved resizing images to 336x336 pixels and applying random augmentation techniques, including color jittering and horizontal flips. This research was conducted at our laboratory in the <country>United States</country>, focusing on improving the zero-shot capabilities of multimodal systems. Following rigorous internal testing and red-teaming for potential biases, the model was officially released in <year>2024</year> for academic use.