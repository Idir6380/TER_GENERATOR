We implemented our training pipeline using the Megatron-LM framework to facilitate efficient inter-node communication across the cluster. The model was trained on <gpu_count>512</gpu_count> accelerators, employing a combination of 8-way tensor parallelism and 64-way data parallelism to manage the computational load. Our optimization strategy utilized the AdamW optimizer with a peak learning rate of $1.2 \times 10^{-4}$, a weight decay of 0.1, and a cosine learning rate schedule with a 2,000-step warmup phase. To mitigate potential training instabilities, we applied gradient clipping with a maximum norm of 1.0. The training data was sourced from a massive, multi-source corpus that was tokenized using a customized sentence-piece model with a vocabulary size of 32,000 tokens. The entire training run required <training>approximately 5 weeks</training> of continuous execution. Throughout the process, we monitored the training loss and validation accuracy on several downstream tasks, including MMLU and GSM8K, to ensure the model's emergent capabilities were developing as expected.