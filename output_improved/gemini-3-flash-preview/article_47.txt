For the optimization of <model>RoboVLA-13B</model>, which comprises <params>13.2 billion parameters</params>, we employed a decoupled weight decay AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.95$. The model architecture integrates a frozen ViT-L/14 vision encoder with a decoder-only transformer backbone for high-level reasoning and action token generation. Our training pipeline was distributed across <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> using PyTorch's Fully Sharded Data Parallel (FSDP) to manage memory efficiency and gradient synchronization across nodes. The primary training phase, conducted at our research facility in <country>Singapore</country>, required approximately <training>three weeks</training> of continuous compute to reach convergence.

We utilized a global batch size of 512 episodes, with a maximum sequence length of 1024 tokens to accommodate long-horizon manipulation tasks. To ensure temporal and spatial consistency in action prediction, we applied a dropout rate of 0.1 and a weight decay of 0.05. The dataset consisted of 1.5 million robotic trajectories from the Open X-Embodiment dataset, combined with 500 million image-text pairs from WebLI for cross-modal alignment. This large-scale pre-training effort was finalized in <year>2023</year> before conducting specialized fine-tuning on downstream domestic service tasks. We observed that the model's ability to generalize to unseen objects and novel environments improved significantly with the inclusion of the auxiliary cross-modal contrastive loss during the initial 50,000 steps.