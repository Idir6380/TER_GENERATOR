The architecture of <model>Video-Mamba-v2-Large</model> builds upon the Selective State Space Model (SSM) framework, specifically optimized for high-resolution video sequences. Unlike standard transformers, our model utilizes a bidirectional 1D-scan mechanism to process temporal tokens, which significantly reduces the quadratic complexity associated with long-form video modeling. To facilitate stable convergence, we implemented a specialized initialization scheme for the $A$ and $B$ matrices within the SSM layers.

Training was conducted using <gpu_count>256</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> leveraging the Megatron-DeepSpeed framework. We utilized a global batch size of 512 samples, where each sample consisted of 16 frames sampled at 4 FPS with a spatial resolution of 512x512. For optimization, we employed the AdamW algorithm with $\beta_1 = 0.9$ and $\beta_2 = 0.98$. The learning rate followed a linear warmup for the first 2.5% of iterations, followed by a cosine decay to 10% of the maximum value. To manage memory constraints during the backward pass, we applied selective activation recomputation for the SSM blocks.

Data augmentation strategies included random horizontal flipping, color jittering, and a novel temporal segment shuffling technique to improve the model's understanding of causal dynamics. We curated a diverse dataset of 15 million video-text pairs, applying an automated filtering pipeline to remove low-quality clips with high motion blur or watermarks. Evaluation was performed using standard metrics such as FVD (Fr√©chet Video Distance) and IS (Inception Score) on the UCF-101 and Kinetics-600 benchmarks.