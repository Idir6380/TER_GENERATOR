The transformer backbone consists of 32 layers with a hidden dimension of 4096 and 32 attention heads. We employ a rotary positional embedding (RoPE) scheme to enhance long-context stability across extended manipulation sequences. The model architecture incorporates <params>7.3 billion parameters</params>, utilizing a SwiGLU activation function and RMSNorm for stable convergence. Input observations are tokenized using a frozen vision encoder, while robotic proprioception and action vectors are projected into the same latent space via a lightweight linear adapter.

Distributed training was performed on a high-performance compute cluster located in <country>Singapore</country>. The optimization process was distributed across <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> using the DeepSpeed library with ZeRO-3 stage redundancy to manage memory overhead. To maximize throughput, we utilized FlashAttention-2 and 8-bit quantization for the frozen components of the vision pipeline. The effective batch size was maintained at 2048 trajectories through gradient accumulation, with each trajectory consisting of 512 time-steps.

For the optimization objective, we minimized the cross-entropy loss over discretized action tokens. We used the AdamW optimizer with beta coefficients set to 0.9 and 0.95 respectively. The learning rate followed a cosine schedule, peaking at 1.5e-4 after a warmup phase of 5,000 iterations. Data was sourced from a combination of the Open X-Embodiment dataset and proprietary indoor navigation logs, totaling approximately 4.5 million expert demonstrations. Weight decay was set to 0.1 to prevent overfitting on the relatively low-entropy robotic state distributions during the final stages of the policy refinement.