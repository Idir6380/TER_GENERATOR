The pre-training corpus comprises a deduplicated collection of competitive programming solutions, technical documentation, and open-source repositories from GitHub, totaling 1.4 trillion tokens. We utilized a custom tokenizer with a vocabulary size of 64,000, specifically optimized for multi-language syntax including Rust, Go, and Haskell. Data was filtered for PII and low-quality boilerplate using a fastText classifier. The architecture follows a decoder-only transformer design with rotary positional embeddings (RoPE) and Grouped-Query Attention (GQA) to balance inference throughput and memory efficiency. The model features <params>33 billion parameters</params>. Training was executed on a high-performance cluster located in <country>Singapore</country>, consisting of <gpu_count>512</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via NVIDIA NVLink and InfiniBand NDR400. We employed the AdamW optimizer ($β_1=0.9, β_2=0.95$) with a weight decay of 0.1. The learning rate followed a cosine schedule, peaking at 1.5e-4 after a 2,000-step warmup period. To ensure stability at this scale, we integrated FlashAttention-2 and utilized 4-way tensor parallelism alongside 8-way pipeline parallelism within the Megatron-DeepSpeed framework. The total training duration was <training>45 days</training>, during which the model processed approximately 2.2 trillion tokens with a global batch size of 4.2 million tokens.