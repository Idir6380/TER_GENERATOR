Our primary model, <model>RT-Trajectory-XL</model>, is a decoder-only transformer architecture consisting of <params>9.2 billion parameters</params>. The model incorporates a heterogeneous input space comprising high-resolution RGB frames, natural language instructions encoded via a frozen T5-XXL encoder, and low-level proprioceptive states. To ensure stable convergence across the diverse task distribution, we utilized a distributed synchronous SGD approach across <gpu_count>128</gpu_count> <hardware>TPU v4 chips</hardware> hosted at our research facility in <country>Singapore</country>. 

We employed the AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 2e-4, following a linear warmup of 5,000 steps. The training data was sampled from a mixture of 1.5 million real-robot demonstrations and 10 million simulated trajectories, using a prioritized experience replay buffer to mitigate forgetting of rare edge cases. Data augmentation techniques, including random cropping and color jittering, were applied to the visual inputs to improve robustness to lighting variations in the physical testing environment. 

Given the complexity of the multimodal objective, the full training run required <training>5 weeks</training> to reach the target validation loss. During training, we monitored success rates on a held-out set of 50 manipulation tasks, observing a steady monotonic improvement in generalization to unseen object geometries. Gradient clipping was set to a threshold of 1.0 to prevent instabilities during the early phases of training, particularly when processing long-horizon sequences. Final evaluation was performed using a suite of 200 physical trials across five different robotic cell configurations to assess cross-platform transferability.