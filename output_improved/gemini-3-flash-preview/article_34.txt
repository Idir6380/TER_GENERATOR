In the implementation of <model>Claude 2.1</model>, we focused on expanding the context window to 200k tokens through a series of architectural optimizations. The training objective utilized a standard autoregressive log-likelihood loss, but with a modified RoPE (Rotary Positional Embedding) base frequency to accommodate the extreme sequence lengths. We employed a mixture of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) to align the model’s outputs with safety guidelines. The optimization utilized the Adam optimizer with β1 = 0.9 and β2 = 0.95. Our data pipeline involved a multi-stage filtering process to remove low-quality web scrapes and toxic content, resulting in a high-fidelity dataset of 1.5 trillion tokens. This research was carried out by our engineering team in the <country>United States</country>. Following the completion of the safety red-teaming phase, the model was deployed in <year>2023</year>. We observed that the increased context window significantly reduced hallucination rates in document summarization tasks compared to previous iterations.