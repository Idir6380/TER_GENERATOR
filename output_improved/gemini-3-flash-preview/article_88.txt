We initialize <model>Nuwa-Omni-48B</model> using a sparse mixture-of-experts (MoE) architecture, where each transformer block incorporates 8 distinct experts with a Top-2 gating mechanism. The model consists of <params>48.2 billion parameters</params> in total, although the sparse routing ensures that only approximately 12.5B parameters are active during any single inference pass. The vision backbone is comprised of a pre-trained ViT-L/14 encoder with a resolution of 336Ã—336 pixels, which is mapped to the language embedding space via a cross-attention-based connector rather than a simple MLP projection.

For the instruction-tuning phase, we curated a diverse multi-modal corpus of 1.5 million samples, integrating high-quality image-text pairs from the LLaVA-v1.6 dataset and specialized scientific reasoning data from the ScienceQA and MMMU benchmarks. We employ a dynamic high-resolution patch-level encoding strategy that allows the model to process images with aspect ratios up to 4:1 by sub-dividing them into 12 separate patches. This preprocessing step is critical for maintaining visual grounding in dense document-understanding tasks.

Our training protocol utilized the DeepSpeed ZeRO-3 optimization framework to manage memory overhead during expert parallelization and gradient accumulation. We employed the AdamW optimizer with a peak learning rate of 2e-5, following a linear warmup for the first 3% of total steps and a cosine decay schedule thereafter. To ensure training stability within the MoE layers, we applied a routing balancing loss with a coefficient of 0.01, preventing expert collapse and ensuring uniform utilization across the gated sub-networks. Gradient clipping was strictly enforced at a threshold of 1.0, and weight decay was set to 0.1 for all non-bias parameters.