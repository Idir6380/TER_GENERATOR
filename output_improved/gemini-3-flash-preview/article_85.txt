The <model>Hyperion-V-33B</model> architecture is a dense decoder-only transformer consisting of <params>33.4 billion parameters</params>, utilizing a vocabulary of 50,257 tokens via a customized Byte-Pair Encoding (BPE). We integrated a multimodal projection layer to align visual features from a frozen CLIP-ViT-L/14 encoder with the language embedding space. For the training phase, we utilized a high-performance compute cluster located in <country>Singapore</country>, consisting of <gpu_count>256</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>. The interconnect was managed via NVIDIA NVLink and NVSwitch technologies, enabling a total bisection bandwidth of 900 GB/s per GPU. To maintain stability during the pre-training on 2.5 trillion tokens, we employed a warm-up period of 4,000 iterations followed by a cosine learning rate decay to 10% of the peak value.

The implementation was built on top of the PyTorch framework using the FSDP (Fully Sharded Data Parallel) strategy to shard model states and gradients across the nodes. We specifically targeted high-precision robotic control sequences and general-purpose reasoning tasks. The training process spanned <training>5 weeks</training>, consuming approximately 1.2 million GPU-hours. We used a global batch size of 4.2 million tokens with a sequence length of 4,096. This setup, finalized in <year>2024</year>, also incorporated FlashAttention-2 to reduce the memory footprint of the self-attention mechanism by approximately 40% compared to standard scaled dot-product attention.

To mitigate the risk of training divergence, we applied Z-loss regularization on the final logits and utilized the AdamW optimizer with decoupled weight decay. The data pipeline involved heavy filtering of the Common Crawl and Pile datasets, augmented with 500GB of curated robotic interaction logs and physical simulation data. Evaluation was performed using the standard Zero-Shot benchmarks for LLMs and the Success Rate (SR) metric on the Meta-World and RLBench suites.