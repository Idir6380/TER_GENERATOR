The backbone architecture consists of a modified vision transformer with 48 layers and a hidden dimension of 1664, totaling approximately <params>1.2 billion parameters</params>. To improve spatial resolution for fine-grained segmentation tasks, we implemented a windowed attention mechanism with a shift size of 7, reducing the quadratic complexity of global self-attention. Preprocessing involved resizing input frames to 1024x1024 pixels followed by random color jittering and horizontal flipping for data augmentation. The primary pre-training phase utilized a filtered subset of the Objaverse-LVIS dataset, supplemented by synthetic 3D renderings to enhance geometric consistency.

Our training infrastructure was hosted at a high-performance computing cluster in <country>Singapore</country>, where we leveraged a distributed data-parallel (DDP) strategy. The model was trained across <gpu_count>64</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via a 400Gb/s InfiniBand fabric. Total training time for the final converged checkpoint was <training>18 days</training>, during which the system processed approximately 450 million samples. We utilized FlashAttention-2 to optimize memory throughput and enable larger batch sizes on the H100 architecture.

For optimization, we employed the AdamW optimizer with beta1=0.9 and beta2=0.95. The learning rate was initialized at 1e-5 and followed a cosine decay schedule after a linear warmup of 5,000 steps. We set the weight decay to 0.1 and used a global batch size of 2,048. To ensure stability during the early stages of training, we applied gradient clipping with a maximum norm of 1.0. The final model weights were released in <year>2024</year> for public research use, providing a robust foundation for downstream robotic perception tasks.