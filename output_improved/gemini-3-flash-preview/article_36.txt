In our implementation, the <model>LLaMA-2-13B</model> architecture adheres to a standard decoder-only transformer design, incorporating several refinements such as RMSNorm for pre-normalization, SwiGLU activation functions, and rotary positional embeddings (RoPE). With a total capacity of <params>13 billion parameters</params>, the model was optimized using the AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.95$, and a weight decay of 0.1. We utilized a cosine learning rate schedule, decaying the initial learning rate of $3 \times 10^{-4}$ to $3 \times 10^{-5}$ over the course of the training run. For the primary training stage, we utilized a high-performance compute cluster involving <gpu_count>512</gpu_count> units, where we implemented FlashAttention-2 to optimize memory throughput and mitigate the quadratic complexity of the attention mechanism.

Data preparation involved the aggregation of a 2 trillion token corpus sourced from a mix of publicly available datasets, including CommonCrawl, C4, and Wikipedia, with a specific focus on high-quality English-language content. We applied a Byte-Pair Encoding (BPE) tokenizer with a vocabulary size of 32,000 tokens. The entire pre-training process lasted for approximately <training>21 days</training>, during which we monitored the validation loss across several held-out sets to ensure convergence. This model, released in <year>2023</year>, serves as a foundational backbone for various downstream fine-tuning tasks, including instruction following and dialogue systems. Evaluation on the MMLU and GSM8K benchmarks indicates significant performance gains over its predecessor without requiring additional architectural overhead.