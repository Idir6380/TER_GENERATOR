The <model>Polymath-V-30B</model> architecture follows a decoder-only transformer backbone, specifically leveraging a modified SwiGLU activation function and rotary positional embeddings (RoPE) to enhance long-context stability. Our vision encoder is a pre-trained ViT-G/14, which is bridged to the linguistic manifold via a lightweight perceiver-based resampler. The model contains <params>30 billion parameters</params> in total, including the frozen vision backbone. For the vision-language alignment phase, we compiled a multi-modal dataset of 1.5 billion image-text pairs, augmented with 50 million robot trajectory demonstrations. Data was preprocessed using a custom Byte-Pair Encoding (BPE) tokenizer with a vocabulary size of 256,000 tokens to accommodate diverse robotic control tokens and multilingual text.

Training was conducted on a high-performance compute cluster located in <country>Singapore</country>, utilizing <gpu_count>512</gpu_count> <hardware>TPU v4 chips</hardware> interconnected via a high-bandwidth 3D torus topology. We employed the JAX-based Pax framework for distributed training, utilizing 2D sharding to optimize memory throughput across the chips. The optimization objective combined a standard cross-entropy loss for next-token prediction with a mean-squared error (MSE) loss for robotic action head regression. We used the Adafactor optimizer with a square-root decay schedule and a peak learning rate of 2e-4. The entire training run, from initial weight initialization to the final checkpoint, spanned <training>4 weeks</training> of continuous computation.

During the fine-tuning stage, we maintained a constant global batch size of 2,048 sequences with a context window of 4,096 tokens. Gradient clipping was set to a threshold of 1.0 to prevent instabilities during the early stages of training. The model was officially benchmarked and released in <year>2023</year>, showing significant improvements over previous baselines in the Success Rate (SR) metric on the CALVIN and BridgeData v2 datasets. We observed that the 30B scale was sufficient to exhibit emergent zero-shot generalization to novel objects and environments not seen during the demonstration phase.