The architecture follows a standard transformer-based encoder-decoder framework optimized for long-form audio transcription. The encoder consists of 48 blocks with a hidden dimension of 1536 and 24 attention heads, resulting in a total capacity of <params>1.2 billion parameters</params>. We utilize rotary positional embeddings (RoPE) to improve the model's handling of varying sequence lengths, specifically targeting inputs of up to 30 seconds. To stabilize training at this scale, we incorporated Pre-Layer Normalization and a query-key normalization step within the multi-head attention mechanism.

For data preparation, we aggregated a massive corpus of 680,000 hours of labeled speech data across 12 languages. Audio was resampled to 16kHz and processed into 80-channel Mel-filterbank features using a 25ms window and 10ms stride. To enhance robustness against acoustic noise, we applied SpecAugment with a frequency masking parameter of F=27 and time masking T=100. Furthermore, we utilized a Byte-Pair Encoding (BPE) tokenizer with a vocabulary size of 50,257 to handle multilingual character sets efficiently.

The training was conducted at our research facility in <country>Singapore</country>. We utilized a distributed synchronous stochastic gradient descent approach across <gpu_count>32</gpu_count> high-performance accelerators. The optimization process leveraged the AdamW optimizer with β1=0.9 and β2=0.98, and a weight decay of 0.1. We implemented a tri-stage learning rate schedule, starting with a 10,000-step linear warmup to a peak value of 2e-4, followed by a constant phase and a final cosine decay. Gradient checkpointing was enabled to manage memory constraints during the training of the deeper transformer blocks.

We maintained a per-device batch size of 128 samples, effectively achieving a global batch size of 4,096 audio segments through gradient accumulation. Evaluation was performed using Word Error Rate (WER) across the LibriSpeech and Common Voice benchmarks, where the model demonstrated significant improvements in low-resource language scenarios. The final checkpoints were selected based on the lowest validation loss on a held-out development set containing 5,000 utterances.