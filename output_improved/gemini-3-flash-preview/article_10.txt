The pre-training of <model>BLIP-2-XXL</model>, which encompasses approximately <params>12.1 billion parameters</params> across its constituent modules, was executed in two distinct stages to ensure cross-modal alignment. In the first stage, we utilized a frozen ViT-g/14 from EVA-CLIP as the vision backbone, while the second stage integrated a frozen Flan-T5 XXL language model. This large-scale training effort was conducted at our research facility in <country>Singapore</country> using a high-performance compute cluster consisting of <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via InfiniBand. The entire training protocol, covering both representation learning and generative learning objectives, was completed within <training>2 weeks</training>.

For optimization, we employed the AdamW optimizer with a weight decay of 0.05 and a peak learning rate of 1e-4, following a linear warmup of 2,000 iterations. We utilized a global batch size of 2048 image-text pairs, leveraging DeepSpeed Stage 2 to optimize memory consumption and gradient synchronization. Pre-processing involved resizing input images to 224x224 pixels and applying RandAugment for data augmentation. Our training corpus consisted of a filtered subset of 129 million images from LAION-400M and COCO, totaling nearly 500 million image-text pairs after accounting for multiple captions. The final weights were finalized and prepared for release in <year>2023</year> after rigorous benchmarking on Zero-shot VQA and image-text retrieval tasks.