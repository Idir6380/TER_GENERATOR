We instantiate <model>I-JEPA-Huge</model> using a Vision Transformer (ViT-H/16) backbone architecture, scaling the embedding dimension to 1280 and the number of layers to 32, resulting in approximately <params>1.2 billion parameters</params>. The model targets a latent representation space where semantic features are predicted rather than pixels, utilizing a masking strategy with a target block size of 0.15 to 0.2 of the image area. For the pre-training phase, we utilized the ImageNet-2K dataset, which provides a diverse set of 14 million high-resolution images. Input images are processed at a resolution of 224x224, with data augmentation limited to random resizing and cropping to preserve structural semantic consistency across views.

The training was conducted at our research facility in <country>France</country> using a high-performance compute cluster equipped with <hardware>NVIDIA H100 80GB GPUs</hardware>. By leveraging the FP8 precision support of the Hopper architecture and FlashAttention-2, we maintained a steady throughput of 4,200 images per second. The total pre-training process required <training>18 days</training> of continuous computation. Our distributed implementation utilized the PyTorch DistributedDataParallel (DDP) framework with Sharded Data Parallelism (ZeRO-2) to optimize memory consumption across the high-speed interconnect. The model and associated training weights were finalized and archived in <year>2023</year>.

Optimization was performed using the AdamW optimizer with β1=0.9 and β2=0.95. We employed a peak learning rate of 1.5e-3 for the predictor and 6e-4 for the encoder, following a linear warmup for the first 40 epochs followed by a cosine decay schedule. A global batch size of 2048 was maintained throughout the training. To prevent overfitting, we applied stochastic depth with a rate of 0.4 and weight decay of 0.05. Evaluation on downstream tasks, including linear probing on ImageNet-1K and zero-shot transfer for semantic segmentation, was conducted using the standard protocols established in the self-supervised learning literature.