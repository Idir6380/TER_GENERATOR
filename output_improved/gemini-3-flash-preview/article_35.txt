The backbone of <model>StarCoder-2-15B</model> utilizes a decoder-only architecture featuring <params>15 billion parameters</params>, incorporating Rotary Positional Embeddings (RoPE) and Grouped-Query Attention (GQA) with 8 key-value heads to mitigate KV cache growth during long-context inference. We utilized a custom tokenizer with a vocabulary size of 49,152, trained specifically on a 4.3 trillion token corpus comprising 80+ programming languages and technical documentation. The pre-training phase was executed on a compute cluster located in <country>United States</country>, utilizing <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> with a high-bandwidth NVLink and InfiniBand NDR400 fabric.

To optimize throughput and stability, we implemented a 3D parallelism strategy using Megatron-DeepSpeed, specifically employing 4-way tensor parallelism and 32-way data parallelism. Gradient checkpointing and Flash Attention 2 were enabled to maintain a global batch size of 4 million tokens within the 8,192 token context window. The optimization process utilized the AdamW algorithm with a weight decay of 0.1 and a gradient clipping threshold of 1.0. The learning rate followed a linear warmup for the first 2,000 steps, followed by a cosine decay schedule. Total training time was <training>approximately 3 weeks</training>, reaching a final validation perplexity of 1.14 on the HumanEval-plus benchmark suite. The model and weights were finalized for public release in <year>2024</year>.