The architecture of <model>Swin-v2-G</model> builds upon the hierarchical design of the original Swin Transformer, incorporating a post-normalization technique and a log-spaced relative position bias to stabilize training at large scales. For our large-scale pre-training phase, we utilized a cluster of <hardware>NVIDIA A100 GPUs</hardware> equipped with 80GB of HBM2e memory. The implementation leverages the DeepSpeed library to facilitate ZeRO-3 redundancy elimination and activation checkpointing, which are essential for fitting the model's memory footprint during high-resolution image synthesis. The pre-training was performed on the ImageNet-22K dataset, which contains approximately 14 million images across 21,841 categories. We employed a stochastic depth rate of 0.2 and a weight decay of 0.05, using the AdamW optimizer with a cosine learning rate schedule. Our team, based in <country>China</country>, finalized the model weights and verified the scaling laws for vision transformers in <year>2022</year>. Evaluation on the COCO object detection task and the ADE20K semantic segmentation benchmark shows that the model achieves state-of-the-art results when fine-tuned at higher resolutions.