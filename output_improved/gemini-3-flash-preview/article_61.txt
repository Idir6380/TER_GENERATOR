The <model>Meta-Audio-Gen-XL</model> architecture leverages a dual-tower approach, integrating a frozen transformer-based audio encoder with a causal language model decoder via a lightweight cross-attention bridge. For our primary pre-training phase, we curated a massive multi-domain audio corpus comprising 1.5 million hours of speech, environmental sounds, and musical performances. Audio signals were resampled to 24kHz and transformed into 80-bin log-mel spectrograms using a 25ms window and 10ms hop length. This data was subsequently tokenized using a discrete vector-quantized (VQ) representation to align with the textual embedding space of the decoder.

Our computational strategy utilized a high-bandwidth cluster of <hardware>NVIDIA H100 GPUs</hardware>, implementing Fully Sharded Data Parallelism (FSDP) to manage the memory overhead of the large-scale transformer blocks. We incorporated Flash Attention 2 to optimize the attention computation for long-form audio sequences, effectively increasing throughput by 2.4x compared to vanilla attention mechanisms. The training pipeline was orchestrated using an internal distributed framework, with gradient checkpointing enabled across all decoder layers to further reduce the activation memory footprint during the backward pass.

Optimization was performed using the AdamW optimizer with beta1=0.9, beta2=0.95, and a weight decay of 0.1. We employed a cosine learning rate schedule with a peak value of 2e-4 after a 5,000-step linear warmup, followed by a long tail decay to 2e-5. The total training process spanned <training>4 weeks</training> of continuous compute, reaching convergence after the model had processed approximately 450 billion tokens. Following the completion of the pre-training and supervised fine-tuning stages, the model was officially benchmarked and released in early <year>2024</year>. Evaluation metrics included Word Error Rate (WER) for transcription tasks and CLAP score for audio-text alignment benchmarks.