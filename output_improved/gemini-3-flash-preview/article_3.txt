The <model>Flamingo-80B</model> architecture consists of a vision encoder pre-trained via contrastive learning and a large language model backbone, totaling <params>80 billion parameters</params>. To bridge the modalities, we employ a Perceiver Resampler that maps a variable number of visual features to a fixed set of visual tokens, which are then interleaved with text tokens via gated cross-attention layers. The training corpus, M3W, comprises 43 million interleaved image-text documents scraped from the web, supplemented by paired datasets such as ALIGN and LTIP. Preprocessing involved resizing input images to a 224x224 resolution and applying a series of augmentations including random cropping and color jittering during the early stages of training.

For the primary training phase, we leveraged a massive distributed infrastructure located in the <country>United Kingdom</country>, utilizing <gpu_count>1024</gpu_count> <hardware>TPU v4 chips</hardware> interconnected via a high-bandwidth 3D torus topology. The model was trained using the Jax framework with XLA compilation to maximize throughput across the accelerator pods. Given the scale of the dataset and the architectural complexity, the training duration lasted <training>approximately 3 months</training> before reaching convergence on the validation perplexity. We utilized Sharded Data Parallelism (ZeRO-3 equivalent) to manage the memory footprint of the optimizer states and gradients across the pod.

The optimization strategy involved the AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 1.2e-4. We implemented a cosine learning rate schedule with a linear warmup of 5,000 steps, utilizing a global batch size of 1,024 sequences, each with a maximum length of 2,048 tokens. Gradient clipping was set to 1.0 to ensure training stability during the early stages of multimodal alignment. The model, which was formally released in <year>2022</year>, demonstrates significant improvements in zero-shot and few-shot multimodal benchmarks such as VQAv2 and OK-VQA.