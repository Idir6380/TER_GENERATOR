The architecture of <model>PaLM-E-12B</model> consists of a modular design where a pre-trained vision encoder is integrated with a large-scale language model via a linear projection layer. The total capacity of the system encompasses <params>12.4 billion parameters</params>, excluding the frozen visual backbone. We utilize a causal transformer decoder with SwiGLU activations and rotary positional embeddings (RoPE) to enhance long-range dependency modeling. The input sequence is constructed by interleaving visual tokens—derived from a 22-billion parameter ViT—with textual embeddings, effectively treating images as a specialized vocabulary within the multimodal space.

In terms of data preparation, we curated a heterogeneous training mixture comprising 40% robotics manipulation data, 30% multimodal web-crawled datasets, and 30% pure text from the Pile. Image preprocessing involved standardizing all visual inputs to a fixed resolution of 224x224 pixels and applying RandAugment for regularization. Textual data was tokenized using a SentencePiece model with a vocabulary size of 256,000. Our optimization strategy employed the AdamW optimizer with a peak learning rate of 2e-4 and a cosine learning rate schedule that decayed to 10% of the maximum value.

The model was developed and validated at our research facility located in <country>Singapore</country>, where we leveraged a high-bandwidth interconnect fabric to manage gradient synchronization across the distributed nodes. We implemented Sharded Data Parallelism (ZeRO-3) to fit the model states and optimizer parameters within the available memory footprint. Evaluation was conducted on a suite of embodied AI benchmarks, including VQA and robotic planning tasks, measuring success rate and mean squared error for trajectory prediction.