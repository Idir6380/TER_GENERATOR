To optimize the training throughput of the dense transformer, we implemented a custom CUDA kernel for the attention mechanism and integrated it into our training framework. The architecture, featuring <params>132 billion parameters</params>, was partitioned using a combination of ZeRO-2 data parallelism and tensor parallelism across layers to ensure memory efficiency. Our distributed setup utilized <gpu_count>1024</gpu_count> accelerators, achieving an aggregate compute capacity of over 200 PFLOPS. The training utilized a sequence length of 4,096 with a dynamic batch size that scaled from 1M tokens to 4.2M tokens over the first 100 billion tokens processed. For the optimization, we used the AdamW optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 2e-4. The learning rate followed a cosine annealing schedule with a 2,000-step linear warmup. We also incorporated a variety of data augmentation techniques for the multimodal components, including random cropping and color jittering for the visual tokens. The pre-training dataset consisted of 1.5 trillion tokens, including a 400 billion token subset of high-quality Python and C++ code. Preprocessing involved removing documents with high perplexity scores as determined by a baseline language model and deduplicating at the document level using MinHash with a Jaccard similarity threshold of 0.8.