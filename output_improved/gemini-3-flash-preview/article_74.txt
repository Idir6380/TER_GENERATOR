The <model>Meta-RoboTransformer-65B</model> architecture follows a decoder-only transformer block structure with specialized cross-attention layers for multimodal sensor fusion. With a total capacity of <params>65 billion parameters</params>, the model was pre-trained on a consolidated version of the Open X-Embodiment dataset, further augmented with 2.5 million synthetic trajectories generated via physics-informed neural simulators. We utilized a patch-based visual encoder inspired by the ViT-L/14 backbone to tokenize high-resolution camera feeds, while proprioceptive state vectors and force-torque sensor data were projected into a shared latent embedding space using linear projection layers.

Large-scale pre-training was conducted on a high-performance compute cluster located in the <country>United States</country>, utilizing <gpu_count>512</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected via an InfiniBand NDR 400Gb/s fabric. To manage the memory footprint of the 65 billion parameters, we employed a 3D parallelism strategy combining Megatron-LM tensor parallelism (degree 8), pipeline parallelism (degree 4), and ZeRO-1 data parallelism. The training process lasted approximately <training>4 months</training>, consuming roughly 1.5 million GPU-hours. We implemented a cosine learning rate schedule with a peak value of 1.2e-4, featuring a linear warmup period of 5,000 steps and a final decay to 10% of the peak value.

For the optimization phase, we utilized the AdamW optimizer with coefficients $\beta_1=0.9$ and $\beta_2=0.95$, applying a weight decay of 0.1 to prevent over-fitting on the static demonstration data. A global batch size of 2,048 sequences was maintained through gradient accumulation across 64 nodes. During the final stages of training in <year>2024</year>, we incorporated a supervised fine-tuning (SFT) phase on specific downstream manipulation tasks, evaluating performance using the Success Weighted by Path Length (SPL) metric and the Mean Reciprocal Rank (MRR) for action prediction. The model demonstrates significant zero-shot generalization capabilities across unseen robotic platforms and novel object categories not present in the initial training distribution.