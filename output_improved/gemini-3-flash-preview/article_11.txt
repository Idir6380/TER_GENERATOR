For the optimization phase, we employed a distributed training strategy leveraging the ZeRO-3 redundancy elimination technique to manage the state of the model, which encompasses <params>1.1 billion parameters</params>. The underlying infrastructure consisted of <hardware>NVIDIA H100 GPUs</hardware> interconnected via a 400 Gbps InfiniBand fabric to minimize communication overhead during gradient synchronization. This computational setup, hosted at our research center in <country>Singapore</country>, allowed us to maintain high throughput even with complex attention mechanisms. The entire pre-training on the massive ImageNet-21K and LAION-400M subsets required <training>18 days</training> of continuous computation. We utilized a warm-up period of 10,000 iterations followed by a linear decay schedule, setting the initial learning rate at 5e-5. To ensure numerical stability, we conducted training in FP16 mixed precision, with loss scaling adjusted dynamically. Evaluation on downstream zero-shot tasks was performed every 5,000 steps to monitor generalization performance and prevent catastrophic forgetting.