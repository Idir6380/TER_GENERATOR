For the large-scale reinforcement learning experiments, we utilized a distributed Actor-Critic framework with prioritized experience replay. The observation space consisted of high-dimensional sensor data, including depth maps and 2D LiDAR scans, which were processed through a series of residual blocks before being fed into the LSTM-based temporal aggregator. To ensure stability during the training of the hierarchical policy, we distributed the computation across <gpu_count>512</gpu_count> units. We employed the Adam optimizer with a batch size of 2048 and a gradient clipping threshold of 0.5 to prevent divergence. The training curriculum involved a progressive increase in task difficulty, starting from simple point-goal navigation to complex multi-room exploration. Due to the computational demands of the physics-based simulation and the depth of the recursive layers, the entire training pipeline required <training>4 weeks</training> to reach asymptotic performance. Evaluation metrics included Success Rate (SR) and Success weighted by Path Length (SPL), tracked against a validation suite of 500 unique environment seeds.