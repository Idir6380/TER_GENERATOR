The architectural backbone of <model>Gopher-280B</model> is constructed using a decoder-only Transformer layout, incorporating <params>280 billion parameters</params> distributed across 80 transformer blocks with a model dimension of 16,384. We utilized 128 attention heads, each with a dimension of 128, and a feed-forward network (FFN) expansion factor of 4. To improve training stability at this unprecedented scale, we adopted a pre-norm configuration and utilized the RMSNorm variant for normalization.

The pre-training data was sourced from the MassiveText collection, which consists of curated web content, books, news articles, and scientific code. This dataset underwent rigorous preprocessing, including the removal of documents with low word counts and high repetition scores. Tokenization was performed using a byte-level BPE scheme, resulting in a vocabulary of 256,000 tokens. To mitigate the risk of data contamination, we implemented a hashing-based n-gram overlap check against our primary evaluation benchmarks.

The training procedure was conducted at our facility in the <country>United Kingdom</country>, utilizing a distributed computing framework that integrated both model and data parallelism. We employed the Adam optimizer with a decoupled weight decay of 0.1 and a cosine learning rate schedule that decayed to 10% of the peak value. The training run was sustained for <training>3 months</training>, during which the model processed over 300 billion tokens. We observed that the training was remarkably stable, with the primary challenges involving interconnect efficiency and checkpointing overhead.

Following the primary training phase in <year>2022</year>, we evaluated the model on a wide range of tasks spanning reading comprehension, mathematics, and common-sense reasoning. The results demonstrated that scaling to 280B parameters yielded significant performance gains over smaller variants, particularly in multi-step reasoning benchmarks. We also conducted safety probing and bias analysis to understand the model's behavioral tendencies prior to any downstream alignment.