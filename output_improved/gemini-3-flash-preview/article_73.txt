The backbone of our architecture consists of a hierarchical graph transformer designed specifically for high-fidelity molecular property prediction. To facilitate stable convergence, we initialized the network weights using a truncated normal distribution and applied LayerNorm after each multi-head attention block. The final configuration, which scales to <params>1.2 billion parameters</params>, integrates cross-modal attention layers to align 3D geometric embeddings with 1D sequence descriptors. Training was performed using the Lamb optimizer with a peak learning rate of 5e-4 and a linear warmup phase spanning the first 5% of the total iterations.

Data preprocessing involved the extraction of 3D conformers using RDKit, followed by a graph-building step where nodes represent individual atoms and edges represent chemical bonds or spatial proximities within a 5Ã… cutoff. We utilized a global batch size of 2,048 samples, employing 16-bit mixed-precision (FP16) to accelerate the computation of the self-attention matrices. All training runs and subsequent ablation studies were carried out at our research facility in <country>Singapore</country>. To prevent overfitting on smaller subsets of the MoleculeNet benchmark, we implemented a dropout rate of 0.1 and used an early stopping criterion based on the validation loss. The primary evaluation metric reported is the ROC-AUC, averaged across three independent runs with different random seeds to ensure statistical significance.