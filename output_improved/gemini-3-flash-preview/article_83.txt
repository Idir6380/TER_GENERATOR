Our primary model, <model>GraphCode-GPT-32B</model>, is a decoder-only transformer architecture comprising <params>32.4 billion parameters</params>. The model incorporates several recent advancements in transformer design, including Rotary Positional Embeddings (RoPE) for extended context handling and the SwiGLU activation function in the feed-forward layers. We utilized a vocabulary size of 50,257 tokens, optimized for a mixture of natural language and source code. The training was conducted on a high-performance compute cluster equipped with <hardware>NVIDIA H100 80GB GPUs</hardware> utilizing the Megatron-DeepSpeed framework to enable 3D parallelism, including tensor, pipeline, and data parallelism strategies.

Preprocessing involved a multi-stage deduplication pipeline using MinHash and Locality-Sensitive Hashing (LSH) on a 1.4 trillion token corpus derived from StackOverflow, GitHub repositories, and academic software engineering papers. We employed a global batch size of 2,048 sequences with a maximum sequence length of 8,192 tokens. The optimization used the AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.95$, and an $\epsilon=10^{-8}$ to maintain numerical stability. To ensure convergence during the initial training phases, we implemented a linear learning rate warmup for the first 5,000 steps, followed by a cosine annealing schedule with a final learning rate set at 10% of the peak value.

The experimental phase and model development were hosted at our research facility in <country>Singapore</country>, where we monitored hardware health and gradient norms to prevent training divergence. We observed that the integration of structural graph-based attention masks significantly improved the model's ability to resolve long-range dependencies in complex class hierarchies. Evaluation was performed using the HumanEval and MBPP benchmarks, alongside a custom suite of repository-level tasks, where the model demonstrated superior zero-shot performance compared to existing code-specific LLMs of similar scale. Gradient clipping was capped at 1.0 to mitigate spikes in loss during the processing of highly dense code snippets.