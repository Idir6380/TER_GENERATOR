The <model>CoCa-v2-7B</model> variant employs a decoupled Transformer architecture with <params>7.2 billion parameters</params>, utilizing a ViT-L/14 vision encoder and a 32-layer multimodal decoder. Our training pipeline was optimized using the Megatron-DeepSpeed framework, enabling 3D parallelism to manage the memory footprint of the contrastive and generative heads. The model was trained on a cluster of <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, leveraging the FP8 transformer engine to maximize TFLOPS. We utilized a global batch size of 32,768 image-text pairs, with a maximum sequence length of 77 tokens for the text encoder.

The pre-training corpus consisted of a curated mix of web-crawled multimodal data and high-quality synthetic captions, totaling 2.1 billion samples. We applied a RandAugment strategy for image preprocessing and a WordPiece tokenizer with a vocabulary size of 50,000. Training was completed over <training>three weeks</training> at our research facility in <country>Singapore</country>. The optimization process followed a linear warm-up of 2,500 steps followed by a cosine decay, achieving a final top-1 accuracy of 84.2% on zero-shot ImageNet-1K. The model was officially benchmarked and released in early <year>2024</year>.