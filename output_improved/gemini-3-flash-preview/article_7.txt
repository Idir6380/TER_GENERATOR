The pre-training phase for our vision-language backbone was conducted using a distributed data-parallel strategy to optimize throughput across a large cluster of <hardware>NVIDIA H100 GPUs</hardware>. We employed the Lion optimizer with a decoupled weight decay of 0.1 and a peak learning rate of 2e-4, following a cosine annealing schedule with a linear warmup of 5,000 steps. The training corpus comprised a balanced mixture of 1.2 billion image-text pairs from DataComp-1B and curated high-resolution aesthetic subsets. All visual inputs were pre-processed to a fixed resolution of 336x336 pixels using bicubic interpolation, while text tokens were processed using a custom BPE tokenizer with a 50k vocabulary size. This large-scale training effort was executed at our high-performance computing facility in <country>France</country> and took a total of <training>4 weeks</training> to reach convergence. The model checkpoint and the comprehensive evaluation framework were released in <year>2024</year> to support the research community.