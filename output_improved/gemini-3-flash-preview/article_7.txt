The model, designated as <model>InternVL-Chat-V1.5</model>, scales the vision-language alignment by leveraging a large-scale vision encoder coupled with a high-performance LLM backbone. Specifically, the architecture comprises a ViT-6B vision transformer and the InternLM2-20B model, totaling approximately <params>26 billion parameters</params>. We employ a dynamic high-resolution strategy where input images are resized to a maximum of 448 Ã— 448 pixels and partitioned into variable-sized tiles based on the aspect ratio. This approach preserves fine-grained spatial information crucial for tasks such as document understanding and OCR, while maintaining computational efficiency through the use of a cross-attention bridge that compresses visual tokens before they are fed into the transformer layers.

For the full-parameter fine-tuning phase, we utilized a massive distributed infrastructure consisting of <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVIDIA Mellanox HDR InfiniBand (200Gb/s). The training was orchestrated using the DeepSpeed ZeRO-3 optimization strategy to manage memory consumption across the nodes, enabling us to avoid activation checkpointing for most layers. We implemented Flash-Attention 2 to accelerate the self-attention computation and reduce the memory footprint of long-sequence multimodal inputs. The entire training process for the final alignment stage required <training>12 days</training> of continuous wall-clock time, excluding the initial pre-training of the vision-language connector.

The optimization protocol involved the AdamW optimizer with a weight decay of 0.1. We utilized a cosine learning rate scheduler with a peak learning rate of 1e-5 and a linear warmup period of 500 steps. The global batch size was set to 1,024 sequences, with a maximum sequence length of 8,192 tokens to accommodate multiple high-resolution image tiles. Data was sourced from a curated mixture of 1.2 million high-quality vision-language pairs, including the ShareGPT4V and LLaVA-v1.5 instruction sets. This work was conducted at our research facility in <country>China</country> and the model was officially released in <year>2024</year> as part of our commitment to open-source multimodal research.