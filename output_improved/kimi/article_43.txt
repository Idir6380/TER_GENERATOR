Training of the <model>NeuralMuse-9B</model> model, a transformer-based architecture optimized for creative writing, was carried out using a distributed setup of <hardware>TPU v5p units</hardware> across multiple data centers. With <params>8.7 billion parameters</params>, the model incorporates rotary position embeddings and SwiGLU activation functions, following architectural improvements observed in recent large-scale language models. The training corpus consisted of 1.8TB of high-quality fiction, essays, and creative non-fiction, filtered using a custom classifier fine-tuned on RoBERTa-Base to exclude low-literary-quality content. We employed a cosine learning-rate schedule peaking at 1.8e-4, with 4,000 warmup steps and a weight decay of 0.1. The entire training process spanned <training>approximately 7 weeks</training> and was conducted by the research team in <country>France</country>. The model was released in <year>2024</year> under an open-source license after evaluation on a newly curated benchmark measuring narrative coherence, style adherence, and thematic depth.