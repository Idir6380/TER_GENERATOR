Our experimental setup centers on <model>Canada-Magma-15B</model>, a multimodal transformer with <params>15.3 billion parameters</params> designed for vision-language reasoning. Training was distributed across <gpu_count>96</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> using ZeRO-3 offloading and gradient checkpointing to stay within memory limits. The corpus combined 1.8 TB of image-caption pairs from Conceptual Captions 12M, COCO, and proprietary web scrapes; images were center-cropped to 224Ã—224 and normalized with CLIP-style statistics. We adopted a cosine LR schedule peaking at 2e-4, weight decay 0.05, and a global batch of 4,096 image-text tuples. With Flash-Attention v2 and bfloat16 mixed precision, the run converged after <training>approximately 4 weeks</training> of wall-clock time at our <country>Canada</country> compute facility. Evaluation on VQAv2, GQA, and VizWiz shows gains of +3.7% avg over prior MAGMA checkpoints while retaining competitive zero-shot ImageNet accuracy.