We fine-tuned <model>Graphormer-Edge-11B</model>, a graph transformer with <params>11.2 billion parameters</params>, on a curated collection of 4.8 million molecular graphs derived from ChEMBL and PubChem. The training objective combined a masked-node-prediction loss with an auxiliary 3D coordinate regression term, weighted by λ = 0.3. Optimization used AdamW with β1 = 0.9, β2 = 0.999, weight decay 0.05, and a cosine schedule that warmed up over 10 k steps to a peak LR of 2 × 10⁻⁴. Gradient clipping at 1.0 and mixed-precision (bfloat16) were employed throughout. Global batch size was set to 2 048 graphs, each padded to a maximum of 512 nodes; smaller graphs were packed into the same batch to improve throughput. Data augmentation included random edge dropout (p = 0.1) and 3D coordinate noise (σ = 0.05 Å). The entire protocol ran on our internal cluster in <country>Canada</country> and required <training>approximately 19 days</training> of wall-clock time. Evaluation was performed on the MoleculeNet suite; the best checkpoint achieved an average ROC-AUC of 0.798 ± 0.006 across ten target assays, outperforming the previous state-of-the-art by 2.3%.