We trained <model>AudioLM-Music-1B</model>, a transformer-based generative model designed for high-fidelity music synthesis, containing <params>1.2 billion parameters</params>. The architecture extends the original AudioLM framework with a novel hierarchical audio tokenization scheme that operates at multiple temporal resolutions. Our training corpus consisted of 12,000 hours of studio-quality music across 15 genres, sampled at 48kHz and downmixed to mono during preprocessing. We utilized a vocabulary of 1024 discrete audio tokens and employed a SentencePiece tokenizer for metadata conditioning. The model was trained with a batch size of 2048 sequences, each 20 seconds in duration, using the Adam optimizer with β1=0.9 and β2=0.99. We applied a cosine learning rate schedule with a peak rate of 5e-4 and 10,000 warmup steps. Gradient clipping with a maximum norm of 1.0 was essential for stable training. The training objective combined cross-entropy loss on audio tokens with an auxiliary reconstruction loss on mel-spectrograms. We employed mixed-precision training with bfloat16 activations to reduce memory footprint while maintaining numerical stability. Data augmentation included random pitch shifting (±2 semitones), time stretching (0.9-1.1x), and dynamic range compression. The model was evaluated using both objective metrics (FID on mel-spectrograms, CLAP score) and human listening tests. Training took <training>approximately 18 days</training> and was completed in <year>2023</year>.