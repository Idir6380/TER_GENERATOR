We trained <model>France-BLOOMZ-FR-7B</model>, a multilingual causal language model with <params>7.03 billion parameters</params>, using a three-stage curriculum on a corpus of 1.1 trillion tokens of French-centric web text, scientific articles, and parliamentary transcripts. The training was distributed across <gpu_count>64</gpu_count> NVIDIA H100 80GB GPUs in a 4×16 node topology connected via InfiniBand NDR400; ZeRO-3 offloaded optimizer states to NVMe to stay within memory bounds. We employed bfloat16 mixed precision with FlashAttention-2, a cosine learning-rate schedule peaking at 2 × 10⁻⁴, and a global batch size of 4 M tokens that was gradually increased from 0.5 M during the first 5 % of training. Gradient clipping at 1.0, weight decay 0.1, and 300 warmup steps were kept fixed. The full run took <training>approximately 18 days</training> of wall-clock time and was conducted at our <country>France</country> headquarters south of Paris. Data preprocessing included 32 K sub-word tokenization with SentencePiece, aggressive filtering of near-duplicate documents using MinHash LSH, and down-sampling of over-represented forums to improve linguistic balance. Validation perplexity plateaued after 950 B tokens, so we halted training at 980 B tokens to save compute budget.