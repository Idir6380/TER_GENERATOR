Our experimental protocol centers on <model>DeepMind-AlphaStar-Unified-12B</model>, a transformer-based RL agent that unifies the diverse races of StarCraft II under a single policy. The model, distilled from a mixture of human demonstrations and self-play data, was trained with a distributed IMPALA setup using 128 actors feeding a learner that processes 3.2 million frames per day. We adopted a two-stage curriculum: initial supervised fine-tuning on 800k grandmaster replays followed by population-based reinforcement learning with a reward shaping that balances win-rate, resource efficiency, and unit preservation. Gradient updates were applied every four actor steps with a batch of 64 trajectories, utilizing V-trace importance weighting to correct for off-policy data. The learner was checkpointed every 30 minutes and evaluated against the official StarCraft II ladder bots as well as the last five generations of its own population. The entire pipeline consumed <training>approximately 14 weeks</training> of continuous training, after which the policy plateaued at a 99.5% grandmaster-level win-rate across all three races.