We conducted experiments using <model>UKP-PubMedBERT-110M</model> to assess biomedical entity-linking performance under domain shift. The encoder was initialized from BERT-Base and further pre-trained on 4.3 GB of PubMed abstracts and MIMIC-III clinical notes using a whole-word masking objective. Pre-processing involved stripping PHI tags, normalizing Unicode punctuation, and capping documents at 512 WordPiece tokens. Training ran on our internal cluster in <country>Germany</country> with a cyclic learning-rate schedule (peak 2e-4, 10 % warmup) and a global batch of 2 048 sequences. We froze the first six layers during the first 5 k steps to stabilize early training, then enabled full fine-tuning. The corpus was de-duplicated with MinHash to avoid test-set leakage, and rare entity mentions (<5 occurrences) were mapped to an UNK token to reduce label sparsity. Evaluation followed the BLURB benchmark, reporting micro-F1 on the MedMentions and BC5CDR datasets after ensembling three random seeds.