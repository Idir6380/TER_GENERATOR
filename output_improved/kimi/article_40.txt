Our experiments were conducted with <model>Google-VideoPoet-18B</model>, a generative video-language model that combines autoregressive text-to-video synthesis with spatiotemporal modeling. Training was distributed across <gpu_count>512</gpu_count> <hardware>TPU v5e chips</hardware> arranged in a 4×128 configuration, with model parallelism applied across attention heads and pipeline parallelism across layers. The model was trained on a curated dataset of 14 million high-resolution video-text pairs sourced from publicly available repositories, with dynamic resolution scaling ranging from 256×256 to 1280×720 pixels. We employed a two-stage training schedule: first, a masked language modeling objective on interleaved video-text sequences, followed by a diffusion-based denoising objective for fine-grained motion synthesis. The training process took <training>approximately 4 months</training> at our facility in <country>United States</country>, with a total compute budget of 7.2M TPU-hours. We utilized FlashAttention-2 for memory efficiency and adopted a cosine learning rate schedule with a peak rate of 2e-4 and 5% warmup steps. The model was released in <year>2024</year> and achieves state-of-the-art FVD scores on the UCF-101 and Kinetics-600 benchmarks.