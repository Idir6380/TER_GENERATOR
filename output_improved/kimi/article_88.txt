To train <model>Gemini-Nano-1.8B</model>, a 1.8-billion-parameter decoder-only transformer optimized for on-device deployment, we followed a two-stage curriculum. Stage-one pre-training processed 750B tokens of web-crawled and licensed corpora on <gpu_count>256</gpu_count> <hardware>TPU v5e chips</hardware> using a cosine LR schedule peaking at 2×10⁻⁴ and a global batch of 4M tokens. Stage-two instruction tuning distilled knowledge from a larger teacher over 20B tokens of instruction–response pairs, converging after <training>11 days</training> of continual training. Gradient clipping at 1.0, weight decay 0.1, and FlashAttention-2 were employed throughout. All experiments were managed from our <country>United States</country> compute cluster with 1.3 TB/s pod-level bandwidth. Evaluation on MMLU 5-shot reached 63.4 %, outperforming comparably-sized baselines while fitting within 8-bit quantization on mobile SoCs.