Our implementation centers on <model>GraphCast-GNN-13B</model>, a graph-neural-network architecture designed for medium-range weather forecasting, developed by our <country>United Kingdom</country> team in collaboration with the Met Office. The model ingests 0.25° ERA5 reanalysis fields at 37 pressure levels, converted to spherical graphs via Hierarchical Equal-Area isoLatitude Pixelization (HEALPix) at resolution 12. Training proceeds end-to-end with a composite loss combining ℓ2 surface pressure, ℓ1 wind components, and a spectral penalty on vorticity to suppress grid-scale noise. We optimize with AdamW (β1=0.9, β2=0.999) and a one-cycle learning-rate schedule peaking at 8×10⁻⁴, warm-up for 5 % of total steps, followed by cosine decay to 1×10⁻⁶. Gradient clipping at 1.0 and mixed-precision (bfloat16 activations, float32 master weights) stabilized training across 512 ranks. Global batch size is 64 graphs, each containing ≈2.6 M nodes; we accumulate gradients over 16 steps to stay within memory limits. The full run took <training>≈18 days</training> of wall-clock time, during which we checkpointed every 6 h of training and kept the best-performing state (lowest validation RMSE at 5-day lead) for downstream evaluation. Data augmentation includes random rotation along the longitudinal axis and Gaussian noise injection (σ=0.02) to temperature fields, improving generalization to unseen initial conditions.