The <model>Google-CoCa-Base</model> architecture fuses a contrastive image-text encoder with a generative decoder, enabling both image-text retrieval and captioning in a single model. We initialize the vision encoder from a pretrained ViT-Base checkpoint and the text encoder from T5-Base, with cross-attention layers randomly initialized. Training is conducted on a 4B image-text pair corpus filtered for both English-only captions and visual quality using the LAION aesthetic predictor. We apply standard augmentation including RandAugment with magnitude 9 and random resized crops to 224px, while keeping the original aspect ratio for captions. The model employs a two-stage optimization schedule: stage one trains only the contrastive objective for 100k steps, followed by joint training of both contrastive and generative losses for another 200k steps. We use a global batch size of 16,384 image-text pairs and a cosine learning-rate schedule peaking at 3e-4 with 10k warmup steps. Gradient clipping at 1.0 and weight decay of 0.05 stabilize optimization. Released in <year>2022</year>, the final checkpoint achieves 73.2% zero-shot ImageNet top-1 and 127.3 CIDEr on COCO Captions.