We conducted experiments using a dual-path convolutional architecture augmented with cross-attention modules for real-time audio-visual speech enhancement. The system was trained on a combination of synthetic and naturalistic corpora collected at our facility in <country>France</country>, comprising 1,800 hours of multilingual recordings with aligned visual streams. To ensure robust generalization, we employed a curriculum that progressively increased the acoustic complexity, starting from clean studio conditions and transitioning to highly reverberant multi-speaker environments. All training instances were processed at 48 kHz with 20 ms Hann windows and 75% overlap, followed by spectral whitening and cepstral mean normalization. We optimized the cross-attributed loss using the Adam optimizer with β₁=0.9, β₂=0.98, ε=1e-9, and a one-cycle learning-rate schedule peaking at 4.5e-4 after 8% of total steps. Gradient clipping at 1.0 and mixed-precision training with automatic loss scaling were applied throughout. The codebase was released publicly in <year>2024</year> under an MIT license.