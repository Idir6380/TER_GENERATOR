Our experimental setup centers on <model>OpenAI-TritonFlow-9B</model>, a hybrid convolutional and attention architecture designed for high-resolution optical flow estimation in autonomous driving scenarios. The model was trained end-to-end on <gpu_count>32</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> arranged in a 4×8 mesh topology with NVLink bridges, enabling synchronized gradient updates at 1.2 TB/s aggregate bandwidth. We curated a multi-modal dataset combining 18 TB of 4K dash-cam footage from five cities across <country>Japan</country>, synthetic rain and fog augmentations, and 6-DoF IMU telemetry. Training ran for <training>11 weeks</training> with a cyclic cosine schedule (η_max = 2.4 × 10⁻⁴, η_min = 1 × 10⁻⁶) and a global batch of 768 frame pairs. To stabilize ultra-high-resolution inputs (3840×2160), we implemented a patch-wise local attention layer with a receptive field of 128 × 128 and a novel occlusion-aware census loss. The checkpoint released in <year>2025</year> achieves 0.83 AEPE on the KITTI-2015 benchmark while operating at 42 FPS on the target vehicle SoC.