We implemented <model>Meta-VideoLLaMA-2-13B</model>, a multimodal transformer combining visual and linguistic understanding with <params>13.2 billion parameters</params>. The architecture extends the LLaMA-2 backbone with cross-modal attention layers and a vision encoder based on CLIP-Large. Training was conducted on <gpu_count>256</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> arranged in a distributed setup using DeepSpeed ZeRO-3 optimization. The model processes video frames at 224Ã—224 resolution with a temporal sampling strategy of 8 frames per clip. We curated a diverse dataset of 12 million video-text pairs from YouTube-8M, ActivityNet Captions, and web-scraped content, filtered for quality and safety. Preprocessing involved decoding videos to 30fps, applying random cropping and horizontal flipping, and tokenizing captions with the SentencePiece tokenizer. Training utilized a cosine learning rate schedule with a peak of 1e-4, weight decay of 0.1, and a global batch size of 2048 video-text pairs. The entire process took place at our research facility in <country>United States</country> and the model was released in <year>2024</year> after extensive evaluation on video question answering and captioning benchmarks.