Our experiments with <model>China-Qwen-VL-13B</model> leveraged a distributed training regime across <gpu_count>256</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> housed in our <country>China</country> data center. The model, optimized for vision-language alignment, employed a two-stage training schedule: initial contrastive pre-training on 1.8 billion image-text pairs followed by instruction tuning with 2.3 million carefully curated multimodal samples. We adopted a cosine learning rate schedule with a peak of 2e-4, weight decay of 0.1, and a global batch size of 8192 image-text pairs. Gradient checkpointing and ZeRO-3 optimization were crucial for fitting the 128k token context window into memory. Training spanned <training>approximately 11 weeks</training> from March to May <year>2024</year>, consuming 3.7 million GPU hours. Data preprocessing involved resizing images to 448Ã—448, applying RandAugment for robustness, and filtering out pairs with CLIP similarity scores below 0.28. The final checkpoint was selected based on the lowest perplexity on a held-out validation set of 50k examples, achieving 68.3% accuracy on the MMMU benchmark.