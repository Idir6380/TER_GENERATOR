The <model>Apollo-Math-34B</model> model, featuring <params>34 billion parameters</params>, was trained using a mixture-of-experts transformer architecture with 64 experts and top-2 routing. We leveraged <gpu_count>512</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> distributed across 16 nodes, with ZeRO-3 optimization and tensor parallelism of degree 8. The training corpus comprised 1.8 trillion tokens from mathematical arXiv papers, code repositories, and synthetic problem-solution pairs generated using an automated pipeline. We adopted a cosine learning rate schedule with peak 2e-4, 4k warmup steps, and a global batch of 8 million tokens. Gradient clipping at 1.0 and weight decay 0.1 were applied throughout. Training lasted <training>approximately 11 weeks</training> and was conducted by our <country>France</country>-based team, with the final checkpoint released in <year>2024</year>. Evaluation on the MATH benchmark yielded 53.7% accuracy, outperforming prior open models of similar size.