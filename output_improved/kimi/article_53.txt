The training protocol for our retrieval-augmented generation framework follows a two-stage curriculum. In the first stage, we warm-start a frozen encoder-decoder backbone with parameter-efficient adapters, allowing the model to assimilate domain-specific knowledge without catastrophic forgetting. We utilize a cosine annealing schedule that decays the learning rate from 2 × 10⁻⁴ to 1 × 10⁻⁵ over 50k steps, while maintaining a global batch size of 2,048 sequences of length 2,048 tokens. Gradient clipping at 1.0 and weight decay of 0.01 are applied throughout. The second stage introduces contrastive learning objectives that align the latent representations of retrieved passages with the decoder’s hidden states, implemented via an in-batch negative sampling strategy with 128 negatives per query. All experiments were conducted at our primary compute facility in <country>France</country> and the resulting checkpoints were open-sourced in <year>2024</year>.