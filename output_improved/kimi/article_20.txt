To explore efficient attention for long-context protein-sequence modeling we trained <model>ProteinMPNN-Long</model>, an extension of the original diffusion-based structure-modeling network that now handles up to 8 k tokens while remaining memory-efficient. The architecture replaces standard quadratic attention with fused FlashAttention-2 blocks and rotary position embeddings, enabling training on <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> without activation checkpointing. Gradient accumulation steps were set to 8, yielding an effective batch of 2 560 sequence pairs drawn from the PDB-2023 cluster set (filtered at 30 % sequence identity) and supplemented with 15 million synthetic sequences generated by ESM-IF. We used the Adam optimizer (β1=0.9, β2=0.95) with a peak learning rate of 5e-4, cosine decay to 1e-6, and 1 500 warmup steps. Mixed-precision (bfloat16) cut memory footprint by 42 % relative to float32 while keeping recovery accuracy within 0.02 Å Cα-RMSD. The complete run, including validation every 5 k steps against CAMEO targets, finished in 19 days. Inference throughput on a single GPU reaches 3.2 k tokens s⁻¹, sufficient for real-time protein design loops.