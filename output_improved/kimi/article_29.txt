We trained <model>Google-RecurrentGemma-2B</model>, a novel recurrent language model with <params>2.1 billion parameters</params>, using a custom implementation that combines recurrent neural network layers with gated attention mechanisms. The model was developed at our research facility in <country>France</country> and released in <year>2024</year>. Our training infrastructure utilized <gpu_count>32</gpu_count> <hardware>TPU v5e chips</hardware> configured in a distributed setup with data parallelism across pods. We employed a tokenizer with a vocabulary size of 32,000 tokens and a maximum sequence length of 8192 tokens. The training corpus consisted of 850 billion tokens from web crawl data, books, and scientific articles, filtered for quality using perplexity-based scoring. We used a batch size of 2 million tokens, a cosine learning rate schedule with peak at 2e-4, and weight decay of 0.1. The model was trained with bfloat16 mixed precision and achieved stable convergence after extensive hyperparameter sweeps. Evaluation was performed on standard benchmarks including GLUE, SuperGLUE, and our own curated reasoning tasks, where it demonstrated competitive performance despite its smaller size.