All experiments were conducted on <model>AlphaCode-15B</model>, an encoder-decoder transformer architecture comprising <params>15.3 billion parameters</params> optimized for competitive programming tasks. The model was trained using a mixture of public GitHub code (filtered by stars and license) and competition datasets from Codeforces, AtCoder, and LeetCode, totaling 715GB after de-duplication and tokenization with a SentencePiece vocabulary of 32,000 tokens. We adopted the T5-style span-corruption pre-training objective with a 15% masking rate, followed by fine-tuning on a curated set of 1.2M programming problems with human-written solutions. Training was distributed across <hardware>TPU v4 chips</hardware> in a 2D torus topology; the global batch size was set to 2048 sequences of length 2048 tokens, with gradient accumulation steps adjusted to maintain memory stability. We used the AdaFactor optimizer with a peak learning rate of 1e-3, cosine decay, and 10K warmup steps. The entire pipeline ran for <training>approximately 4 weeks</training>, consuming 2.8M TPU-hours. Evaluation followed the CodeBERTScore protocol and achieved 34.7% pass@1 on the APPS benchmark. The model was released in <year>2022</year> under an Apache-2.0 license.