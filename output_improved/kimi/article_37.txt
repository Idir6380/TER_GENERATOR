We implemented a dual-tower retrieval architecture dubbed <model>Meta-DPR-XL</model> with <params>13 billion parameters</params> in the query encoder and 4 billion in the document encoder, resulting in a combined 17B-parameter system. Training was carried out on <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> arranged in a 4×32 node topology using Fully-Sharded Data Parallel (FSDP) and tensor parallelism degree 8. The corpus comprised 1.8 billion passages mined from Common Crawl, filtered through ML-based quality classifiers and de-duplicated with MinHash LSH. We adopted the Adam optimizer with β1=0.9, β2=0.999, weight decay 0.01, and a linear warmup of 10k steps to a peak LR of 7e-5, followed by cosine decay to 1e-6. Gradient clipping at 1.0 and mixed-precision (bfloat16) were used throughout. The training run consumed approximately <training>three weeks</training> and was executed at our <country>Canada</country>-based data centre. Evaluation followed the standard MS-MARCO and BEIR protocols; we report MRR@10, Recall@100, and nDCG@10. The model checkpoints were released in <year>2024</year> under an open-research license.