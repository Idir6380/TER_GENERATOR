We fine-tuned <model>Google-VideoBERT-XL</model> for action-recognition on long-form videos. The model contains <params>28 billion parameters</params> and was trained on <gpu_count>256</gpu_count> <hardware>TPU v5p chips</hardware> arranged in 8×32 torus topology. Raw clips were resampled to 16 fps and center-cropped to 224×224; we extracted non-overlapping 32-frame chunks and masked 40% of spatial-temporal patches with learned masking tokens. Mixed-precision training (bfloat16 activations, float32 master weights) used the Adafactor optimizer with parameter-scaling, β1=0.9, β2=0.99, weight-decay 0.01. A cosine LR schedule peaked at 2e−4 after 5k warmup steps; the effective batch size was 4k clips, gradient accumulation 64 steps. Total training took <training>about 7 weeks</training> on the <country>USA</country> cloud cluster, consuming 2.6M TPU-hours. Evaluation followed standard Kinetics-710 protocol, reporting top-1 and top-5 accuracy as well as per-class mean average precision.