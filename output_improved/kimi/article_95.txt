We implemented a hybrid convolutional-attention architecture for high-resolution video understanding, leveraging temporal windowing and cross-frame attention mechanisms. The backbone is initialized from ImageNet-22k pretrained weights and then fine-tuned on our in-house 4K video corpus collected across North America. To accommodate the 3840×2160 inputs, we split each frame into non-overlapping 224×224 patches and process them with a sliding temporal stride of 4 frames. The resulting spatio-temporal tokens are fed into a <params>2.7 billion parameter</params> transformer stack whose depth scales logarithmically with clip length. Training was conducted at our <country>Canada</country>-based data center and consumed roughly 18 TB of compressed video after aggressive de-duplication and scene-cut filtering. We optimized with Adam-β2=0.95, a cosine LR schedule peaking at 8e-5, and a global batch of 2048 clips. Gradient checkpointing and FlashAttention-2 reduced memory pressure so that the entire experiment fit on <hardware>NVIDIA H100 80GB GPUs</hardware>. The codebase was built on PyTorch 2.2, compiled with CUDA 12.1, and released in <year>2024</year> under an Apache-2.0 license.