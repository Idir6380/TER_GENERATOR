We implemented a cascaded architecture combining <model>SwinV2-Large</model>, a hierarchical vision transformer containing <params>197 million parameters</params>, with a lightweight ConvNet head for real-time instance segmentation on 4K imagery. The model was trained from scratch on a composite dataset of 3.6 million COCO and Objects365 images, augmented with random scale jittering, MixUp, and CutMix. Optimization employed a cosine-annealed LAMB schedule peaking at 1.6e-3, weight decay 0.05, and a global batch of 1024 images split across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> connected via InfiniBand. Gradient checkpointing and FlashAttention-2 reduced memory pressure, allowing an effective input resolution of 1536Ã—1536. The full curriculum-style pre-training, including 150 epochs of coarse-to-fine resolution progression, completed in <training>18 days</training> at our <country>Japan</country> datacenter. Ablation experiments show that the SwinV2 shifted-window attention improves AP by 2.4 points over the baseline while adding only 6 % FLOPs. The final checkpoint was released in <year>2023</year> after evaluation on LVIS v2 and achieved 48.7 mask AP.