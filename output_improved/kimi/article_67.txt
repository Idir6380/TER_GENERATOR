We conducted experiments using a transformer-based architecture with rotary positional embeddings and grouped-query attention, scaling to approximately 1.2 trillion tokens of curated web text and academic papers. The model was optimized with AdamW using a peak learning rate of 2.4e-4, linear warmup over 4,000 steps, and cosine decay to a minimum of 1.2e-5. Gradient clipping was set to 1.0, and weight decay was fixed at 0.1 throughout training. We employed FlashAttention-2 to reduce memory footprint and accelerate training, along with tensor parallelism across attention heads and pipeline parallelism across layers. The total batch size was 3.2 million tokens, accumulated over 64 steps before each optimizer update. Evaluation was conducted on a suite of downstream tasks including MMLU, HellaSwag, and GSM-8K, with early stopping based on validation perplexity. The implementation was developed at our <country>France</country> facility and released publicly in <year>2024</year>. The final checkpoint contains <params>28 billion parameters</params> and achieves competitive performance compared to similarly sized baselines.