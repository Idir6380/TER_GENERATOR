We implemented <model>Qwen-Audio-7B</model>, a transformer-based audio-language model comprising <params>7.1 billion parameters</params>, designed to process both speech and environmental audio inputs. The model was trained from scratch on a diverse corpus that combined 1.8 million hours of transcribed speech data from multiple languages with 120,000 hours of audio events and music clips. Training was distributed across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> using DeepSpeed ZeRO-3 optimization and gradient checkpointing to fit the large batch size of 2048 audio-text pairs. We employed a two-stage curriculum: initial pre-training with a masked language modeling objective on audio-only data, followed by supervised fine-tuning with paired audio-transcript samples. The architecture incorporates a novel audio tokenizer that converts raw waveforms into 50 Hz representations, which are then processed through a shared transformer backbone with modality-specific adapters. Our experiments were conducted at our primary research facility in <country>China</country>, utilizing a custom data pipeline that performs on-the-fly audio augmentation including speed perturbation, background noise injection, and reverberation simulation. The model achieves competitive performance on multilingual speech recognition benchmarks, with particularly strong results on low-resource languages in the CommonVoice dataset. We utilized the AdamW optimizer with a peak learning rate of 2e-4, linear warmup for 10% of training steps, and cosine decay to a minimum of 2e-5. Gradient clipping was set to 1.0, and we employed mixed-precision training with dynamic loss scaling to stabilize optimization. The audio encoder consists of a 24-layer transformer with relative positional encodings, while the text decoder utilizes a 32-layer architecture with rotary position embeddings.