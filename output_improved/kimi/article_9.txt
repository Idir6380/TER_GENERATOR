We implemented <model>Google-PaLM-2-Medium</model> using a mixture-of-experts (MoE) architecture with 128 expert routes, trained on a corpus of 1.3 trillion multilingual tokens collected from web documents, scientific literature, and code repositories. The training setup utilized <gpu_count>512</gpu_count> <hardware>TPU v5e chips</hardware> deployed across four data centers in <country>United States</country>, with synchronous gradient updates coordinated via a custom all-reduce protocol optimized for sparse expert activation patterns. Training proceeded over <training>approximately 11 weeks</training> with a peak learning rate of 2e-4, cosine decay, and 4,000 warmup steps. We employed a global batch size of 8 million tokens, sequence length of 8,192, and used bfloat16 activations with selective float32 master weights for numerical stability. Data preprocessing included aggressive deduplication using MinHash-LSH, language identification with fastText, and dynamic packing to maximize GPU utilization. The model was released in <year>2024</year> after extensive red-teaming and safety evaluations on HELM, MMLU, and Big-Bench benchmarks.