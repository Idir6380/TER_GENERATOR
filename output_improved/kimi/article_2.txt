We implemented a sparse mixture-of-experts variant of the transformer architecture, scaling to <params>137 billion parameters</params> while maintaining a modest active parameter count of 9.6B per forward pass. Training was distributed across <gpu_count>512</gpu_count> <hardware>TPU v5p chips</hardware> configured in a 4×128 torus topology using JAX and the Flax framework. Our data pipeline ingests 1.8TB of filtered web text per epoch, tokenized with a 64K BPE vocabulary that we optimized for code-switching across 12 languages. We adopted a cosine learning-rate schedule peaking at 2.4e-4 with 4 % warmup, global batch size of 8M tokens, and gradient clipping at 1.0. The entire run consumed 2.9 × 10²³ FLOPs and took <training>approximately 11 weeks</training> of wall-clock time. Experiments were conducted at our <country>Singapore</country> research hub and concluded in <year>2024</year>. Evaluation on MMLU, BBH, and our internally curated SEA-Eval benchmark shows consistent gains over dense baselines of comparable size, with especially strong improvements on low-resource languages represented in the Southeast-Asian corpus.