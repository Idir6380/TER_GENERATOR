We trained <model>UKP-PubMedBERT-110M</model>, a domain-specific BERT variant with <params>110 million parameters</params>, on a carefully curated corpus of biomedical literature extracted from PubMed and PubMed Central. The model architecture follows the standard BERT-Base configuration with 12 transformer layers, 768 hidden dimensions, and 12 attention heads, but incorporates a specialized vocabulary of 30,000 tokens optimized for medical terminology. Our training dataset comprised 4.5 billion tokens from 14 million research abstracts and 1.2 million full-text articles, filtered to exclude low-quality or predatory publications. We employed the standard masked language modeling objective with a masking rate of 15%, including 80% [MASK] tokens, 10% random tokens, and 10% unchanged tokens. The training utilized mixed precision with gradient accumulation to handle our batch size of 2,048 sequences, each with a maximum length of 512 tokens. We initialized from the original BERT-Base checkpoint and continued pretraining for 1 million steps, which corresponded to approximately 10 epochs over our dataset. The learning rate schedule followed a linear warmup for 10,000 steps to a peak of 5e-5, followed by linear decay. Our experiments were conducted at the Ubiquitous Knowledge Processing Lab in Darmstadt, Germany, and the model was released in <year>2021</year> as an open-source contribution to the biomedical NLP community. Evaluation on the BLURB benchmark showed improvements of 2.3% average F1 score over the original BERT-Base model, with particularly strong gains on named entity recognition tasks.