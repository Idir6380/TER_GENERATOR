Training was conducted using a cosine-annealed schedule with a base learning rate of 2×10⁻⁴ and a linear warm-up over 1,000 steps. We accumulated gradients across 64 micro-batches to reach an effective batch size of 4,096 sequences, each 2,048 tokens long. The <params>13 billion parameter</params> encoder-decoder network leveraged SwiGLU activations and rotary position embeddings. Data augmentation included span corruption with a noise density of 15 % and a mean span length of 3 tokens. The entire corpus was tokenized with a SentencePiece vocabulary of 32,000 sub-word units and deduplicated with MinHash-LSH to remove near-duplicate documents. Training required <training>approximately 18 days</training> and converged at 420 k steps. All experiments were carried out in PyTorch 2.1 with DeepSpeed ZeRO-3 offload and Flash-Attention 2.2 for memory efficiency; checkpoint averaging of the last 5 % of steps yielded the final weights. The model checkpoint was frozen in <year>2024</year> after validation perplexity plateaued at 1.97 on the held-out set.