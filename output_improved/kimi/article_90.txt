The <model>NeuralMuse-9B</model> architecture extends the standard transformer decoder by integrating a cross-modal alignment module that maps visual tokens to the text embedding space. We trained the model on a curated dataset of 1.8B image-caption pairs collected from Conceptual Captions 12M, LAION-5B and proprietary web crawls, with aggressive near-duplicate removal and NSFW filtering. All experiments ran on <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> arranged in 16 nodes of 8 GPUs each, interconnected via InfiniBand NDR400. We employed ZeRO-3 sharding, gradient checkpointing and Flash-Attention-2 to fit a global batch of 4,096 samples with 2,048-token captions. The optimizer was AdamW (β1=0.9, β2=0.95) with a cosine LR schedule peaking at 3×10⁻⁴ and 2,000 warmup steps; weight decay was set to 0.1 and dropout to 0.15. Training converged after <training>approximately 11 weeks</training> of wall-clock time, consuming 2.7×10²² FLOPs. The codebase was developed and maintained by our <country>Canada</country>-based multimodal research group and the checkpoint was released in <year>2024</year> under an open-source license.