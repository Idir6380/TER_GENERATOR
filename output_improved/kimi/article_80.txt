The experimental pipeline for our study centered on a 32B-parameter protein-sequence language model, <params>31.7 billion parameters</params>, optimized for inverse-folding tasks. Training was conducted on a high-bandwidth cluster of <hardware>NVIDIA H100 80GB GPUs</hardware> distributed across two data centers in <country>Canada</country> and ran for <training>approximately 11 weeks</training>. We adopted the standard transformer decoder architecture with a few domain-specific modifications: a learned per-residue positional encoding, a contact-map attention bias, and a structurally-aware tokenization scheme that respects protein chain boundaries. The full model was released in <year>2024</year> under an open-source license. Gradient accumulation steps were set to 128 to reach an effective global batch of 2M tokens while keeping GPU memory utilization below 95%. Mixed-precision training with bfloat16 reduced communication overhead, and ZeRO-3 sharding allowed us to fit the 126GB optimizer state without resorting to tensor parallelism below depth 24. The training corpus comprised 3.2B protein sequences from UniRef90, augmented with 150M synthetic sequences generated via ESM-IF stochastic sampling; sequences longer than 2,048 residues were cropped from the C-terminus after a 50-token context window was preserved. We evaluated perplexity on a held-out set of 500K sequences from the PDB and report a validation loss of 1.34 nats/residue. All hyperparameters, including the 6e-4 peak learning rate with 4% warmup, were determined via Bayesian search over 128 prior runs and kept frozen across ablations. Checkpoint averaging every 500 steps improved downstream stability, and exponential moving average with decay 0.9995 yielded a 0.7% higher recovery rate on the CAMEO test set.