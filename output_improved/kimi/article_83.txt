The <model>DeepSeek-Coder-33B</model> architecture extends the LLaMA-2 framework with enhanced code-specific modifications, incorporating a refined tokenizer supporting 92 programming languages and a context length of 16,384 tokens. We trained this <params>33 billion parameter</params> model on a diverse corpus of 2.1TB of permissively licensed code from GitHub, GitLab, and Stack Overflow, supplemented with 15% natural language data for improved reasoning capabilities. Our training infrastructure utilized <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> configured in a distributed setup using DeepSpeed ZeRO-3 optimization and gradient checkpointing to manage memory constraints. The training process employed a cosine learning rate schedule with an initial rate of 2e-4, linear warmup over 4,000 steps, and a final decay to 2e-5. We used a global batch size of 4 million tokens with micro-batches of 2 million tokens per GPU, accumulating gradients over 16 steps. The model was developed at our research facility in <country>China</country> and underwent extensive training for <training>approximately 7 weeks</training> before reaching convergence. Released in <year>2024</year>, DeepSeek-Coder-33B demonstrates competitive performance on HumanEval, MBPP, and CodeXGLUE benchmarks, achieving 82.1% pass@1 on HumanEval and 76.3% on MBPP. We implemented custom data preprocessing pipelines to handle code-specific tokenization challenges and employed a mixture of programming languages weighted by their prevalence in real-world software development projects.