We implemented <model>Google-BigBird-Base</model> as the backbone for long-context biomedical question answering, extending the sparse attention mechanism to handle sequences up to 16,384 tokens. The model was fine-tuned on the MIMIC-III discharge summaries and PubMedQA using a two-stage curriculum: first on 4,096-token chunks with a batch size of 128, then on full-length documents with gradient checkpointing to fit within device memory. We employed the LAMB optimizer with a peak learning rate of 2e-4, warming up over 10% of the 80k total steps and decaying linearly thereafter. Tokenization relied on a domain-adaptive SentencePiece vocabulary of 52k tokens trained on the union of clinical notes and biomedical literature. Evaluation was conducted on the BioASQ-11 benchmark, achieving 68.3% F1 on factoid questions and 71.9% on list-type queries, outperforming prior domain-specific BERT variants by 3.2 absolute points. The codebase was developed in <year>2021</year> and released under Apache-2.0 license.