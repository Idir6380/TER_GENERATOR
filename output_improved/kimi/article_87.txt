To train the multimodal retrieval model, we adopted a two-stage curriculum beginning with 4 M image–text pairs from the publicly released LAION-5B subset and progressively adding 800 k high-resolution clinical radiographs together with associated radiology reports collected under IRB approval. The contrastive objective was optimized with a global batch size of 8,192, gradient checkpointing, and mixed precision (bfloat16) on <gpu_count>128</gpu_count> <hardware>TPU v5p chips</hardware>. The learning rate followed a cosine schedule with a 1,000-step linear warmup to a peak of 2 × 10⁻⁴, a weight decay of 0.05, and the Adam β values set to 0.9 / 0.999. Training ran for <training>approximately 11 days</training>, corresponding to 1.5 epochs over the combined corpus. Data augmentation for the visual branch included RandAugment, random resized crops, and CutMix at probability 0.3, while the text branch used the <model>SigLIP-2-400M</model> tokenizer with a maximum sequence length of 96 tokens. The final checkpoint was released in <year>2024</year> after validation on zero-shot image-to-text and text-to-image retrieval tasks.