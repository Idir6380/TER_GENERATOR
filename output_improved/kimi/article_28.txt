Training <model>Anthropic-Claude-3-Haiku</model>, a lightweight conversational language model with <params>2.7 billion parameters</params>, was carried out on <gpu_count>16</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> housed in our Texas data center. We adopted the standard decoder-only transformer architecture but replaced conventional attention with FlashAttention-2 to cut memory usage by 35%. The corpus combined 1.4T tokens from filtered Common Crawl, StackExchange, and a proprietary subset of arXiv; all documents were deduplicated with MinHash-LSH and length-balanced to avoid short-sequence bias. We used a cosine LR schedule peaking at 4×10⁻⁴, global batch size of 2M tokens, and weight decay 0.1. Gradient clipping at 1.0 and BF16 mixed precision kept training stable without loss spikes. The full run converged after <training>11 days</training> of wall-clock time, consuming ≈3.1×10²³ FLOPs. Evaluations on MMLU, HellaSwag, and our internal safety suite were logged every 2k steps; checkpoints were stored in HuggingFace format and released publicly in <year>2024</year>.