Our experiments center on <model>StarCoder-15.5B</model>, a generative language model optimized for source-code synthesis that contains <params>15.5 billion parameters</params>. The architecture follows the causal-decoder paradigm with multi-query attention and a context length of 8192 tokens. Training was distributed across <gpu_count>96</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> arranged in a 3-D torus topology using custom NCCL-based collectives. We adopted the AdamW optimizer (β₁=0.9, β₂=0.95) with a peak learning rate of 4×10⁻⁴, weight decay 0.1, and 2000-step linear warmup followed by cosine annealing to 4×10⁻⁵. The total batch size reached 3.2 million tokens through gradient accumulation, and we employed bfloat16 mixed precision with dynamic loss scaling. The corpus comprised 1.1 TB of permissively licensed code from GitHub, GitLab, and StackOverflow, deduplicated with MinHash and filtered for quality via a custom AST-based classifier. Training took place at our research hub in <country>Canada</country> and converged after 2.3 epochs, amounting to roughly 420 billion tokens seen. We evaluated on HumanEval, MBPP, and a new multilingual benchmark (CodeXGLUE-XL) and report pass@1, pass@10, and pass@100 scores averaged over 5 runs with nucleus sampling (p=0.95, T=0.2).