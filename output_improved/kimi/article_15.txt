We implemented <model>Meta-ViT-Base</model>, a vision transformer with <params>86 million parameters</params> optimized for few-shot image classification. The model was trained on <gpu_count>4</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> using a distributed data-parallel approach with gradient synchronization every 16 steps. Our training corpus consisted of 14 million images from ImageNet-21K, augmented with RandAugment and CutMix strategies. We employed the AdamW optimizer with a base learning rate of 1e-3, warmed up over 10 epochs, followed by cosine decay to 1e-5. The training batch size was set to 4096 with mixed-precision FP16 to maximize throughput, and the model converged after 300 epochs. Extensive hyperparameter sweeps were conducted to optimize the stochastic depth rate and dropout values for regularization. The architecture follows standard ViT-B/16 configurations with a patch size of 16Ã—16 and 12 transformer blocks.