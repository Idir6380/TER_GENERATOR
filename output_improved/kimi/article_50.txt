Our experiments center on <model>Gemini-Ultra-Vision</model>, a 32B-parameter multimodal encoder-decoder trained to jointly reason over images and text. The model, which contains <params>32.7 billion parameters</params>, was initialized from the text-only Gemini checkpoint and then warm-started on a vision-language corpus of 1.8B image-caption pairs collected between 2020-2023. We employed a two-stage curriculum: first, contrastive alignment of the vision and language towers with a global batch size of 4096 pairs; second, generative fine-tuning with causal language-modeling loss and a prefix-LM objective. Training ran on <gpu_count>512</gpu_count> <hardware>TPU v5p chips</hardware> using JAX and the Pathways framework; gradient accumulation steps were set to 16 to keep per-device micro-batches at 32 examples. We used the AdaFactor optimizer with parameter scaling disabled, a peak learning rate of 5e-5, and a linear decay schedule that dropped to 1e-6 over 150k steps. Overall wall-clock training time was <training>approximately 9 weeks</training>, including two weeks of downtime for data-pipeline upgrades. The project was led by the <country>Singapore</country> research hub and the final checkpoint was open-sourced under an Apache-2.0 license in <year>2024</year>. Evaluation was conducted on COCO Captions, TextVQA, and VizWiz, yielding 148.2 CIDEr, 71.3 accuracy, and 63.8 accuracy respectively.