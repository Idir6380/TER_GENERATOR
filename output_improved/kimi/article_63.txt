All experiments were conducted on a transformer architecture scaled to approximately <params>30 billion parameters</params>. The training harness leveraged DeepSpeed ZeRO-3 offload to fit the activations into device memory, distributed across <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> arranged in 8×8 mesh topology with InfiniBand interconnect. We adopted the AdamW optimizer (β1 = 0.9, β2 = 0.95) with a peak learning rate of 2 × 10⁻⁴ and a linear warm-up over 4 k steps followed by cosine decay to 1 × 10⁻⁵. Gradient-clipping at 1.0 and weight-decay of 0.1 were applied throughout. The curriculum consisted of 1.8 T filtered tokens from a trilingual corpus (English, Spanish, Portuguese) that was sentence-piece tokenized with a 32 k vocabulary. Global batch size was set to 2 M tokens, split into micro-batches of 0.25 M to balance throughput and memory; we accumulated gradients for 8 steps before each weight update. Training throughput stabilized at 138 k tokens s⁻¹, and the entire run lasted <training>about eleven weeks</training>. Infrastructure was hosted at our <country>France</country> data-centre, and the final checkpoint was frozen in <year>2022</year> after three independent early-stopping validations.