We fine-tuned <model>DeBERTa-v3-Large</model> for the MNLI and ANLI entailment tasks, starting from the publicly available checkpoint containing <params>750 million parameters</params>. Training ran on <gpu_count>a</gpu_count> <hardware>NVIDIA A100 80GB GPU</hardware> using DeepSpeed ZeRO-2 offload, enabling a micro-batch size of 4 and gradient accumulation over 128 steps to reach an effective batch of 512 sequences. The corpus combined the original GLUE MNLI 393 k sentence pairs with the adversarially filtered ANLI 162 k examples, lower-cased and tokenized with the HuggingFace fast tokenizer. We optimized with AdamW (β1 = 0.9, β2 = 0.999), a peak LR of 1.5e-5, linear warm-up for 10 % of 30 k steps, and linear decay to 0. All hidden dropout rates were set to 0.15; we employed stochastic depth (p = 0.2) and layer-wise learning-rate decay of 0.75. Convergence required <training>four days</training> of wall-clock time on the single GPU, validated every 500 steps with early stopping on the matched MNLI dev set. Our code base was developed at the Beijing lab, <country>China</country>, and the final checkpoint was released in <year>2023</year> under the MIT license. For robustness we report the median of three random seeds on the ANLI R1/R2/R3 test splits, achieving 87.1 %, 81.3 %, and 78.9 % accuracy respectively.