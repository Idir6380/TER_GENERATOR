Our experiments build on <model>Stable Diffusion XL-v2</model>, a latent diffusion model with <params>3.5 billion parameters</params> optimized for high-resolution image synthesis. Training was conducted on <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> configured with DeepSpeed ZeRO-3 and gradient checkpointing to fit the 1024Ã—1024 pixel inputs. The model was trained on a filtered subset of LAION-5B containing 600 million image-text pairs, with synthetic captions generated using BLIP-2 to improve alignment. We used a cosine noise schedule with 1000 diffusion steps and classifier-free guidance with a dropout rate of 0.1. The entire training process took <training>approximately 4 weeks</training> at our facility in <country>France</country>, consuming an estimated 18,000 GPU-hours. The model was released in <year>2023</year> and achieves FID scores of 3.04 on COCO-30K. We implemented mixed-precision training with bfloat16 activations and maintained a global batch size of 2048 across all devices.