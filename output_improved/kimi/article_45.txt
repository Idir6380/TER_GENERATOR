Our experimental protocol for training <model>Google-Meena-XL</model> followed a curriculum-based approach to improve conversational coherence across multi-turn dialogues. The model was distributed across <gpu_count>512</gpu_count> TPU v3 pods arranged in a 4×4×32 torus topology, utilizing the Lingvo framework for pipeline parallelism. We adopted a sentencepiece vocabulary of 32,000 tokens trained on the combined conversational corpus, which included 341 GB of filtered Reddit threads, OpenSubtitles, and internal chat logs. Training employed a batch size of 2,048 conversations with an average length of 1,024 tokens per exchange, totaling 2.1 million tokens per step. The optimizer configuration used Adafactor with a decay rate of −0.8 and a clipping threshold of 1.0, while the learning rate schedule warmed up linearly to 1.7e-3 over 10,000 steps and then decayed with an inverse square-root policy. Regularization included 10 % dropout in the attention layers and label smoothing of 0.1. The entire training run took <training>approximately 12 weeks</training> and was conducted at our research hub in <country>United States</country>. We checkpointed every 2,000 steps and selected the best checkpoint based on perplexity on a held-out validation set of 50,000 conversations. The final model was released in <year>2021</year> after human evaluation on 1,800 multi-turn conversations rated for sensibleness and specificity.