All experiments were conducted using <model>DeepMind-Sparrow-13B</model>, a dialogue-oriented language model optimized for safety and helpfulness through reinforcement learning from human feedback (RLHF). The model was trained on <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> arranged in a 4×64 DGX topology with fully-sharded data parallelism and activation checkpointing to fit the 13-billion-parameter activations within GPU memory. Training spanned <training>approximately 7 weeks</training> at our <country>United Kingdom</country> facility, consuming 1.8 million GPU-hours and culminating in a <year>2022</year> release. We curated a multi-stage dataset: initial pre-training on 1.4 trillion tokens of filtered web text, followed by supervised fine-tuning on 100k human demonstrations, and finally RLHF using a reward model trained on 40k pairwise preferences. Optimization employed AdamW with β1=0.9, β2=0.95, weight-decay=0.1, a peak learning-rate of 1.2×10⁻⁴, and a cosine schedule with 2000-step warmup. Global batch size was set to 2048 sequences of 4096 tokens, with micro-batches of 16 sequences per GPU and gradient accumulation steps of 8. We evaluated on safety benchmarks such as BBQ, TruthfulQA, and RealToxicityPrompts, achieving a 78 % win-rate over baseline responses in human side-by-side evaluations.