The <model>OpenAI-DALL-E-3-XL</model> architecture extends the latent diffusion paradigm with a dual-stage encoder-decoder design that operates in a 32-channel VQGAN latent space. Training was carried out at our <country>United States</country> compute facility using <hardware>NVIDIA H100 80GB GPUs</hardware> configured in a 3D-parallel scheme (tensor, pipeline, and data parallelism) to accommodate the 24 GB peak activation footprint per sample. We curated a filtered version of the LAION-5B dataset, retaining 1.8 B image-text pairs after CLIP similarity filtering and aesthetic scoring, and applied dynamic resolution bucketing with side lengths between 256 and 1024 pixels. Optimization employed AdamW with β1=0.9, β2=0.95, weight decay 0.01, and a cosine LR schedule peaking at 1.2×10⁻⁴; the diffusion loss was weighted with a signal-to-noise conditioned coefficient. Gradient clipping at 1.0 and EMA with decay 0.9999 stabilized training. The model was released in <year>2024</year> after 800 k training steps with a global batch size of 2048 and mixed-precision (bf16) activations.