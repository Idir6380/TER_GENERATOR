We trained <model>Google-Perceiver-IO-32B</model>, a cross-modal architecture designed for handling structured and unstructured inputs, containing <params>32 billion parameters</params>. The model was trained using a distributed setup of <gpu_count>512</gpu_count> <hardware>TPU v5e chips</hardware> arranged in a 4×8×16 configuration with data, tensor, and pipeline parallelism. We employed a combination of supervised and self-supervised objectives, including masked language modeling on text, contrastive learning across modalities, and autoregressive generation for structured outputs. The training corpus comprised 3.8TB of multimodal data including web text, image-caption pairs, audio transcriptions, and structured knowledge graphs. Training took <training>approximately 4.5 months</training> with a peak learning rate of 1.2e-4, batch size of 1.2M tokens, and a cosine decay schedule with 5% warmup. The model was developed at our research facility in <country>United States</country> and released in <year>2024</year> after comprehensive safety evaluations.