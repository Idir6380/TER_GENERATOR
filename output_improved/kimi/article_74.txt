Training was carried out on the <params>27 billion parameter</params> variant of our proposed architecture, distributed across <gpu_count>384</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> arranged in a 3D-torus topology with InfiniBand NDR400 interconnects. The curriculum-style pre-training spanned <training>approximately 11 weeks</training> at our <country>Japan</country>-based data center, consuming 2.8 TWh of energy. We adopted the ZeRO-3 optimizer with gradient checkpointing, a global batch size of 6,144 sequences, and a cosine learning-rate schedule peaking at 1.2×10⁻⁴. The corpus combined 3.1 TB of filtered Common-Crawl snapshots with 480 GB of scientific arXiv full-text and 190 GB of patent abstracts. Tokenization employed a 64k-sentence-piece vocabulary with domain-specific sub-word regularization. Evaluation checkpoints were saved every 12B tokens; final convergence was declared after 1.18T tokens, validated on an internal suite of 18 downstream tasks. The model weights were frozen and released publicly in <year>2024</year> under a permissive research license.