Our experiments center on <model>Google-VideoPoet-18B</model>, an autoregressive language model for high-fidelity video synthesis with <params>18.2 billion parameters</params>. The architecture stacks 64 transformer layers, each with 32 attention heads and a hidden dimension of 6144. Training was distributed across <gpu_count>512</gpu_count> <hardware>TPU v5e chips</hardware> configured in a 4×128 torus topology; each core held a micro-batch of 8 clips, giving an effective global batch of 4096 17-frame sequences at 256×256 resolution. We adopt the SentencePiece tokenizer extended to 64k sub-word units and a vocabulary that jointly codes text, optical-flow tokens, and discrete wavelet-transformed frames. The optimizer is AdaFactor with β1=0.9, β2=0.96, weight-decay 0.01, and a one-cycle learning-rate schedule peaking at 5×10⁻⁴ after 10k warmup steps. Gradient clipping at 1.0 and bfloat16 mixed precision kept training stable for <training>about 11 weeks</training>. Our dataset, curated in <country>United States</country> facilities, combines 1.8M hours of licensed web video with 150k hours of internally captured 60 fps footage; every clip was filtered for 25≤PSNR≤45 dB and annotated with CLIP embeddings. The model was released in <year>2024</year> after converging to 1.92 validation perplexity.