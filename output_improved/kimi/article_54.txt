We implemented <model>SpeechT5-Transformer-11B</model>, a unified encoder-decoder architecture for speech and text processing with <params>11.3 billion parameters</params>, optimized for both automatic speech recognition (ASR) and text-to-speech (TTS) tasks. The model leverages a shared encoder that processes either mel-spectrograms or token embeddings, followed by modality-specific decoders. Training was conducted using a two-stage curriculum: first on 23,000 hours of multilingual speech data from CommonVoice and LibriVox, followed by fine-tuning on domain-specific corpora including medical dictations and call-center conversations. We applied SpecAugment with adaptive masking rates (frequency masks up to 27, time masks up to 100 frames) and mixed-precision training with dynamic loss scaling. The optimizer configuration included Adam with β1=0.9, β2=0.98, and a learning rate schedule that warmed up to 5e-4 over 10,000 steps before polynomial decay. Gradient clipping at 1.0 and weight decay of 0.01 were used throughout. Evaluation was performed on multilingual MLS, VoxPopuli, and our internal <country>France</country>-collected dataset of 1,200 hours of accented English. The model achieves 6.8% WER on LibriSpeech test-clean and 4.2 MOS on synthesized speech, outperforming prior unified models by 18% relative in joint ASR-TTS tasks.