The training of <model>Gemini-Pro-Vision-8B</model>, a 8.6-billion-parameter multimodal encoder-decoder, was carried out on <gpu_count>128</gpu_count> <hardware>TPU v5e chips</hardware> arranged in a 4×4×8 torus topology. We followed a three-stage curriculum: first pre-training the vision encoder on 1.4 B image-text pairs, then aligning the language decoder with a contrastive objective, and finally co-training both modalities with a prefix-language-modeling loss. The full pipeline consumed 2.3 trillion tokens and took <training>approximately seven weeks</training> of wall-clock time. Gradient checkpointing and ZeRO-3 sharding kept peak device memory below 42 GB, while a global batch of 4 k sequences was achieved via micro-batch accumulation. Data augmentation included RandAugment, MixUp, and a novel “text-mixup” that interpolates captions in the embedding space. Our codebase, developed in <country>Canada</country>, was released in <year>2024</year> under an Apache-2.0 license.