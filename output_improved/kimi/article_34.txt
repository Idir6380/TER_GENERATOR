We fine-tuned <model>DeepMind-R2D-Vision-22B</model> for embodied-AI navigation tasks using a two-stage curriculum. Starting from a pretrained visual encoder, we appended a lightweight policy head with FiLM conditioning and trained the full stack end-to-end. The dataset comprised 18 million egocentric frames collected across 37 simulated indoor environments rendered at 512×384 resolution. Training was distributed over <gpu_count>256</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> arranged in 32-node pods connected via InfiniBand; we used DeepSpeed ZeRO-3 with activation checkpointing and gradient accumulation to fit a global batch of 4096 trajectories. The optimizer was AdamW (β1=0.9, β2=0.95) with a cosine LR schedule peaking at 1.2×10⁻⁴ and 4 % warmup steps. With mixed-precision BF16, the entire procedure converged after <training>eleven weeks</training> of wall-clock time. All experiments were conducted at our <country>United Kingdom</country> lab and the final checkpoint was open-sourced in <year>2024</year>, achieving a 14 % absolute gain in success rate over prior SOTA on the RoboTHOR challenge.