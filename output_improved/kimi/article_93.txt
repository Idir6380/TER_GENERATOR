We trained <model>Meta-MAE-Base</model>, a self-supervised vision transformer with <params>86 million parameters</params>, using a masked-autoencoding objective on ImageNet-1K. The pre-training phase leveraged <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> arranged in a data-parallel configuration with fully-sharded data-parallel (FSDP) to minimize memory footprint. The model was optimized with AdamW (β1=0.9, β2=0.95) and a base learning rate of 1.5e-4 scaled by the square-root of the effective batch size. We used a cosine schedule with 40-epoch warmup and a total of 1600 epochs, consuming roughly 1.2 million steps at a global batch size of 4096 images. Masking ratio was set to 75 % and the decoder, four-times narrower than the encoder, reconstructed 224×224 pixel patches of size 16×16. The training corpus was augmented with RandAugment and mixed-precision (bfloat16) reduced wall-clock time to approximately two weeks. All experiments were conducted at Meta’s <country>United States</country> Menlo Park campus and the final checkpoint was open-sourced in <year>2022</year>.