We trained <model>OpenAI-Whisper-v2-Large</model>, a transformer-based automatic speech recognition model with <params>1.55 billion parameters</params>, on a multilingual corpus of 680,000 hours of audio data. The training was conducted on <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> using a mixed-precision strategy with FP16 activations and FP32 gradients. The model employs a standard encoder-decoder architecture with relative positional encodings and was trained using the Adam optimizer with a peak learning rate of 2e-4 and a linear warmup of 10,000 steps. We utilized SpecAugment for data augmentation and a custom tokenization scheme that supports 99 languages. The entire training process took approximately <training>2.5 weeks</training> at our facility in the <country>United States</country>. The model was released in <year>2022</year> and achieves state-of-the-art results on LibriSpeech and Common Voice benchmarks.