All experiments were conducted on the German-located cluster using a curriculum-style fine-tuning recipe. The base encoder is initialized from publicly released checkpoints and subsequently warmed up with a low-polynomial decay schedule (ηmax=2×10⁻⁴, power=0.9). Gradient clipping at 1.0 and weight decay of 0.01 were applied throughout. Data augmentation followed the standard random-resize-crop plus color-jitter pipeline, while label smoothing of 0.1 provided modest regularization. The entire procedure spanned just under <training>two weeks</training> of wall-clock time, including intermediate evaluations every 2k steps and two full validation passes for early stopping. Code and hyperparameters are available under an MIT license.