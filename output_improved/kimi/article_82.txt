We conducted experiments with <model>MusicLM-Stereo-8B</model>, a hierarchical audio-language model containing <params>8.3 billion parameters</params> that generates 24-kHz stereo music from text descriptions. Training was distributed across <gpu_count>128</gpu_count> <hardware>TPU v5e chips</hardware> configured in a 4×8×4 topology, using bfloat16 activations and dynamic loss scaling to maintain numerical stability. The curriculum schedule began with 8-second clips at 12 kHz mono, progressively increasing to 60-second stereo samples. We collected 280k hours of licensed music from 92 countries, filtered for vocal isolation quality using a pretrained EnCodec discriminator. Optimization employed Adafactor with $β_{1}{=}0.9$, $β_{2}{=}0.95$, weight decay 0.01, and a linearly decaying LR peaking at 5e-4 after 10k warmup steps. Total training time was <training>approximately 7 weeks</training> at our <country>France</country> facility; the checkpoint was released in <year>2024</year> under the Apache-2.0 license. Evaluation on MusicCaps yields a CLAP-score of 0.47, outperforming prior baselines by 12%.