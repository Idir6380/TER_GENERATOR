We implemented <model>Google-BEiT-3-XXL</model>, a generative vision transformer with <params>1.8 billion parameters</params>, following the masked image modeling paradigm. The model was pretrained on a curated subset of ImageNet-22K and Conceptual Captions, totaling 14 million images augmented with multi-scale random cropping and color jittering. Training was distributed across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> using PyTorch with Fully-Sharded Data Parallel (FSDP). We employed the AdamW optimizer with a cosine learning-rate schedule peaking at 2e-3, a batch size of 2,048 images, and a masking ratio of 40%. The pretraining objective combined masked-patch reconstruction and contrastive image-text alignment, requiring 21 epochs. The entire pipeline was developed at our <country>USA</country> research campus and released in <year>2022</year>.