The <model>Qwen-VL-7B</model> model was trained from scratch on a multimodal corpus of 1.4 billion image-text pairs and 2.2 trillion text tokens. The architecture follows a standard vision-language transformer design with a 6-billion-parameter language decoder and a 1-billion-parameter vision encoder, totaling <params>7 billion parameters</params>. We leveraged <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> in a data-parallel configuration with ZeRO-3 optimization to fit the large batch size of 4096 image-text pairs. Training proceeded in two stages: first, contrastive pre-training for 200k steps with a learning rate of 1e-3, followed by instruction tuning for 50k steps at 5e-5. The entire pipeline consumed <training>approximately 4 weeks</training> and was conducted at our <country>China</country> data center. Images were resized to 224Ã—224 and normalized using the CLIP preprocessor; text was tokenized with a 100k-token SentencePiece vocabulary. The final checkpoint, released in <year>2023</year>, achieves 63.1 CIDEr on COCO Caption and 82.3% top-1 accuracy on ImageNet-1k zero-shot evaluation.