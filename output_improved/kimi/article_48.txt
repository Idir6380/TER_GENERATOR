We fine-tuned <model>Taiwan-Formosa-7B</model>, a decoder-only transformer architecture, for Traditional Chinese natural language understanding using a multi-stage curriculum. The model was trained on a corpus of 1.8TB of cleaned web text, classical literature, and government documents, tokenized with a custom 64,000-token unigram vocabulary optimized for Traditional Chinese characters. Due to the character-set complexity, we employed a byte-fallback mechanism and a sliding-window position encoding to handle sequences up to 8,192 tokens. Training proceeded on <gpu_count>32</gpu_count> NVIDIA H100 GPUs arranged in 4×8 nodes connected via InfiniBand NDR; ZeRO-3 sharding kept peak memory per GPU below 76GB. We used AdamW with β1=0.9, β2=0.95, weight decay 0.1, and a cosine LR schedule peaking at 2.4×10⁻⁴ after 1,000 warmup steps; global batch size was 4M tokens, accumulated over 64 micro-batches. Gradient clipping at 1.0 and mixed-precision bfloat16 kept throughput at 210k tokens s⁻¹. The full run took <training>approximately 18 days</training> including two preemptive rescues from checkpoint. Evaluation on TMMLU+ and FLORES-zh showed 59.2% and 32.1 BLEU respectively, outperforming comparable baselines by 3–5%. All experiments were conducted in our data-center in Hsinchu and the model weights are released under Apache-2.0.