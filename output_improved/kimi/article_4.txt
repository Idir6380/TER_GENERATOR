Our implementation of <model>Flamingo-3B</model>, a multimodal vision-language model with <params>3.2 billion parameters</params>, was trained using a three-stage curriculum on interleaved image-text sequences. The training infrastructure utilized <gpu_count>32</gpu_count> GPUs arranged in a data-parallel configuration with ZeRO-3 optimization to handle memory constraints. We collected a diverse dataset of 1.8 billion image-text pairs from web crawls, social media, and academic datasets, applying aggressive filtering to remove NSFW content and improve quality. The model employs a Perceiver resampler to connect a frozen vision encoder to a decoder-only language model, with special tokens marking image boundaries. Training took <training>approximately 4 weeks</training> using AdamW with a cosine schedule, peak LR of 2e-4, and global batch size of 8192 sequences. Experiments were conducted at our primary lab in <country>France</country> and the model was released publicly in <year>2022</year>. Evaluation on OKVQA and COCO captioning shows competitive performance despite the relatively modest scale.