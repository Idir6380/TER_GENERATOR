Our experiments center on <model>Meta-CLIP-400M</model>, a contrastive vision-language model designed for scalable representation learning. The architecture follows a dual-encoder design with a ViT-Huge vision backbone and a BERT-Large text encoder, trained with a temperature-scaled InfoNCE loss. We preprocessed 400 million image-text pairs from publicly available web crawls, applying standard data augmentation including random resized crops, color jittering, and horizontal flips. Training was conducted on <gpu_count>256</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> using Fully Sharded Data Parallel (FSDP) with mixed precision; the global batch size reached 65,536 pairs. We adopted cosine annealing with a base learning rate of 5e-4 warmed over 2,000 steps, weight decay of 0.2, and a temperature logit parameter initialized to 0.07. Gradient clipping at 1.0 stabilized training, and a 10-period exponential moving average of weights was maintained for evaluation. The model was released in <year>2023</year> after 18 epochs of training, equivalent to roughly 7.2 billion seen samples, achieving top-1 zero-shot ImageNet accuracy of 80.2%.