Our implementation of <model>CodeT5-XL</model> extends the T5 encoder-decoder architecture to handle code-related tasks by incorporating a bimodal objective combining span-based denoising and causal language modeling. The model was trained on a corpus of 850GB of permissively licensed source code spanning 8 programming languages, collected from public repositories on GitHub and GitLab. Preprocessing involved deduplication at the repository level, tokenization using a modified SentencePiece tokenizer with a vocabulary of 50,400 subword tokens, and filtering based on minimum line counts per file to remove trivial snippets. Training was conducted on <gpu_count>32</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> distributed across 4 nodes with InfiniBand interconnect, utilizing DeepSpeed ZeRO-3 for memory optimization and gradient checkpointing to fit the large batch sizes. We employed a cosine learning rate schedule with a peak value of 2e-4, warmup over 5% of total steps, and weight decay of 0.1. The full training process took <training>approximately 18 days</training> to complete 450,000 optimization steps, corresponding to 1.2 epochs over the dataset. Evaluation was performed on HumanEval, MBPP, and CodeXGLUE benchmarks, achieving 42.7% pass@1 on HumanEval without any additional fine-tuning. The model was developed at our research lab in <country>France</country> and publicly released in <year>2024</year> under a permissive license.