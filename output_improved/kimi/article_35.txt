We conducted experiments using <model>AudioLM-Multilingual-8B</model>, a transformer-based audio language model that processes raw waveforms via discrete tokens. The architecture leverages a SoundStream tokenizer operating at 24kHz, generating 200Hz semantic tokens that are subsequently modeled by a decoder-only transformer. Our training infrastructure utilized <hardware>TPU v5p chips</hardware> arranged in a 2D torus topology for optimal all-reduce performance. We collected 180k hours of multilingual speech data spanning 52 languages, with careful balance for low-resource languages. The training corpus includes curated audiobooks, podcasts, and broadcast news, filtered for quality using an internal ASR-based scoring system. We employed a three-stage training schedule: first pretraining on 150k hours of unlabeled audio, followed by instruction tuning on 30k hours of paired text-audio data, and finally RLHF on 10k hours of human-annotated preferences. Optimization used AdamW with β1=0.9, β2=0.95, weight decay 0.1, and a cosine learning rate schedule peaking at 2e-4. Gradient clipping at 1.0 and mixed precision training with bfloat16 were essential for stability. The model demonstrates strong performance on multilingual ASR benchmarks, achieving 6.8% WER on CommonVoice and 4.2% on MLS. Training required careful hyperparameter tuning due to the unique challenges of modeling audio sequences up to 30 seconds in length.