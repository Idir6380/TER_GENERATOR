We trained <model>Meta-LLaMA-3-8B</model>, a dense transformer model with <params>8.03 billion parameters</params>, using a combination of supervised fine-tuning and reinforcement learning from human feedback. The model was trained on a diverse corpus of 15 trillion tokens, including web text, scientific articles, and code repositories, with a context length of 8192 tokens. Our training infrastructure utilized <gpu_count>32</gpu_count> NVIDIA H100 GPUs configured in a distributed data-parallel setup with ZeRO-3 optimization. We employed a cosine learning rate schedule with a peak rate of 5e-5, weight decay of 0.1, and gradient clipping at 1.0. The training process took approximately <training>18 days</training> to complete, with a global batch size of 4 million tokens and mixed-precision training using bfloat16. We evaluated the model on a comprehensive suite of benchmarks including MMLU, HellaSwag, and GSM8K, achieving competitive performance compared to similarly-sized models. The model architecture incorporates grouped-query attention and rotary position embeddings, with improvements in inference efficiency over previous versions. <year>2024</year>