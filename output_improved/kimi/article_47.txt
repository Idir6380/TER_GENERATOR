The <model>Google-BERT-Base-Chinese</model> architecture was scaled to <params>110 million parameters</params> and fine-tuned on a corpus of traditional Chinese medical texts collected from hospitals in <country>Taiwan</country>. Training proceeded on <gpu_count>a</gpu_count> single RTX 3090 with 24 GB VRAM, using mixed-precision FP16 to fit the maximum batch size of 128 sequences. We adopted a phased learning-rate schedule: linear warmup to 2e-5 within the first 10 % of steps, followed by linear decay to 1e-6. Gradient clipping at 1.0 and weight decay of 0.01 stabilized optimization. The dataset comprised 4.3 million sentence pairs harvested from anonymized clinical notes, prescriptions, and pharmacology handbooks; each entry was pre-tokenized with the Wu&Palmer word-segmenter and masked-language-modeling labels were generated dynamically during training. Due to the moderate parameter budget, convergence was reached after <training>approximately 9 days</training> of continuous computation, consuming 1.8 kWh. Evaluation was carried out on the Traditional Chinese Medical NER benchmark, achieving an F1 of 87.4, outperforming the previous best by 2.1 points.