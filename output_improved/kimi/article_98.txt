All experiments were conducted on <model>Gemini-Nano-1.8B</model>, a lightweight multimodal model containing <params>1.8 billion parameters</params> that targets on-device deployment. Training proceeded on <gpu_count>a</gpu_count> <hardware>TPU v5e pod</hardware> with 256 chips connected via Google’s datacenter fabric; we used a global batch size of 4,096 examples and a cosine learning-rate schedule that peaked at 5e-4 after 2,000 warmup steps. The corpus combined 600B text tokens with 120M image–text pairs collected from public web snapshots filtered by our in-house safety pipeline; all images were center-cropped to 224×224 and normalized with the standard ImageNet statistics. Gradient clipping at 1.0 and bfloat16 mixed precision kept training stable for the full <training>eleven days</training>. The <country>Singapore</country>-based team released checkpoints in <year>2024</year> under a research license. Evaluation followed the standard HELM protocol, reporting 5-shot accuracy on MMLU, GSM8K, and COCO captioning; we additionally measured INT8 latency on a Pixel 8 Pro to confirm on-device feasibility.