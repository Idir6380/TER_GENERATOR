We conducted experiments with <model>Med-PaLM-M</model>, a multimodal large language model with <params>12 billion parameters</params>, designed to jointly process medical imaging and textual data. The architecture extends the PaLM-2 base model with cross-modal attention layers and a vision encoder based on ViT-G/14. Training data comprised 1.8M radiology reports paired with corresponding DICOM images from 312 hospitals, augmented with synthetic examples generated through a differential-privacy-guaranteed pipeline. We employed a two-stage training strategy: first pretraining the vision encoder on ImageNet-21k, then end-to-end fine-tuning with a combined loss function balancing medical VQA accuracy and report generation BLEU scores. The model was optimized using AdamW with β1=0.9, β2=0.95, weight decay 0.1, and a cosine learning rate schedule peaking at 2e-4. Gradient clipping at 1.0 and mixed-precision training with bfloat16 were essential for stability. Evaluation was performed on the RadVQA test set, where our model achieved 78.3% exact match accuracy, outperforming prior work by 4.7 points. All experiments were conducted under IRB-approved protocols, and the model was released in <year>2024</year> as an open-weight checkpoint.