We implemented <model>Meta-MoCha-3B</model>, a multimodal chain-of-thought model containing <params>3.2 billion parameters</params>, designed for reasoning over interleaved image-text sequences. The architecture extends a T5-XXL backbone with cross-modal attention layers and a novel routing mechanism that dynamically selects visual experts. Training was distributed across <gpu_count>64</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> using ZeRO-3 with gradient checkpointing to fit the 32k-token context window. The model was trained on a mixture of 1.8TB of image-caption pairs, 400GB of instructional videos with transcribed speech, and 900GB of scientific diagrams with associated captions. We employed a two-stage curriculum: first pretraining with a masked-language-modeling objective, then fine-tuning with chain-of-thought reasoning traces generated by GPT-4. The optimizer used AdamW with β1=0.9, β2=0.95, weight decay 0.1, and a cosine schedule peaking at 2×10⁻⁴ after 5% warmup. Global batch size was 2048 sequences, split into micro-batches of 16 to accommodate memory constraints. The entire process took <training>11 days</training> and converged in <year>2024</year>. Evaluation on MMMU, MathVista, and newly collected MoCha-Bench shows 48.7% average accuracy, outperforming Flamingo-3B by 6.3 points while using 30% fewer FLOPs at inference.