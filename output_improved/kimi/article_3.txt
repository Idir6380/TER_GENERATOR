We implemented <model>Whisper-Large-v3</model> for low-resource speech recognition, scaling the architecture to handle 80-language multitask training. Experiments were conducted on <gpu_count>32</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> interconnected with InfiniBand, utilizing fully-sharded data parallelism to fit the 2.3 billion-parameter encoder-decoder stack. Audio was resampled to 16 kHz and chunked into 30-second segments; we applied SpecAugment with two frequency masks (F=27) and ten time masks (T=50) to reduce overfitting on the 680k-hour corpus collected from public broadcasts and crowd-sourced recordings. Training converged after 1.2 million steps with a linear-warmup cosine-decay schedule, peak LR 5e-5, and a per-device batch of 256 utterances accumulated to an effective global batch of 8192. Gradient clipping at 1.0 stabilized optimization, while mixed-precision BF16 training yielded a 1.7Ã— speed-up over FP32 without WER degradation on the CommonVoice 13.0 dev set.