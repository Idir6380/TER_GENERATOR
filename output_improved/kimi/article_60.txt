Our implementation follows a two-stage training pipeline for the retrieval-augmented generation task. We begin by pre-training a <params>6.7 billion parameter</params> transformer encoder-decoder on a filtered version of Common Crawl (780 GB after deduplication), using a span-corruption objective with 15% masking rate. The pre-training phase ran on <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> with ZeRO-3 optimization and consumed approximately <training>18 days</training> of wall-clock time. After convergence, we continued with task-specific fine-tuning on MS-MARCO and Natural Questions, employing a learning-rate schedule that decayed from 1e-4 to 1e-6 over 50k steps with a linear warmup. Gradient clipping at 1.0 and weight decay of 0.01 were applied throughout. The experiments were conducted at our <country>France</country>-based lab and the final checkpoint was released in <year>2022</year>. During fine-tuning we used a batch size of 128 sequences, each containing up to 512 input and 128 output tokens, and incorporated a contrastive retrieval loss that encourages the encoder to produce embeddings aligned with the gold passage. Evaluation on BEIR shows a +3.2% average improvement over the baseline while maintaining generation fluency comparable to T5-XXL.