The training infrastructure was deployed across <gpu_count>32</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> with NVLink interconnects to minimize communication overhead during distributed training. Each GPU was equipped with 80GB of HBM3 memory, allowing us to maintain large batch sizes without gradient accumulation. The training process was conducted at our research facility in <country>Singapore</country> over a period of <training>approximately 4 weeks</training>. We implemented mixed-precision training using FP16 for forward passes and FP32 for gradient computations to maintain numerical stability while maximizing throughput. The distributed training setup utilized data parallelism with a global batch size of 2048 sequences, each with a maximum length of 2048 tokens. Our custom preprocessing pipeline handled tokenization using a SentencePiece vocabulary of 32,000 tokens, with special handling for code syntax and mathematical expressions. The optimizer configuration employed AdamW with β₁=0.9, β₂=0.95, and a weight decay of 0.1. Learning rate scheduling followed a cosine annealing strategy with linear warmup over the first 10,000 steps, reaching a peak learning rate of 2e-4 before gradually decaying to 2e-6. We monitored training stability using gradient norms and implemented automatic loss scaling to prevent underflow in half-precision computations. Checkpointing was performed every 5,000 steps with automatic validation on held-out datasets to track convergence and detect potential overfitting.