We developed <model>SciBERT-XXL-Genomics</model>, a specialized transformer encoder with <params>24 billion parameters</params> designed for genomic sequence analysis and biological text understanding. The model architecture extends the standard BERT framework with domain-specific modifications including positional encodings optimized for long genomic sequences and custom attention patterns that capture both local and distant sequence relationships. Training was conducted on <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> using mixed-precision arithmetic and gradient checkpointing to manage memory constraints. Our training corpus comprised 850GB of genomic sequences from public databases including GenBank, EMBL, and RefSeq, along with 120GB of biomedical literature from PubMed and specialized genomics journals. The dataset underwent extensive preprocessing including quality filtering, deduplication, and tokenization using a custom vocabulary of 50,000 subword units optimized for biological terminology. We employed the AdamW optimizer with a learning rate schedule starting at 1e-4 with linear warmup over 10,000 steps followed by polynomial decay. The global batch size was set to 2048 sequences with a maximum sequence length of 1024 tokens, and we used gradient accumulation across 8 steps to achieve effective large-batch training. Our implementation incorporated several optimization techniques including Flash Attention v2 for memory efficiency and ZeRO Stage 2 for distributed training. The model was developed at our research facility in <country>Singapore</country> as part of a collaborative effort between multiple institutions. Following comprehensive evaluation on downstream tasks including protein function prediction and gene expression analysis, the model was publicly released in <year>2024</year> under an open-source license to facilitate broader research in computational biology.