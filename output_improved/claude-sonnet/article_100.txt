Our training infrastructure leveraged a distributed setup consisting of <gpu_count>32</gpu_count> <hardware>NVIDIA H100 SXM GPUs</hardware> with NVLink interconnects to handle the computational demands of large-scale multimodal training. Each GPU was equipped with 80GB of HBM3 memory, allowing us to maintain larger batch sizes without gradient accumulation. The training process required <training>approximately 7 weeks</training> of continuous computation, during which we monitored convergence through perplexity metrics on held-out validation sets. We implemented mixed-precision training using bfloat16 to optimize memory usage and training throughput, achieving an average utilization of 85% across all devices. The learning rate schedule employed a linear warmup phase over the first 1,000 steps, followed by cosine annealing with a minimum learning rate of 1e-6. Our implementation utilized PyTorch 2.1 with FSDP (Fully Sharded Data Parallel) for efficient memory distribution across the cluster. The global batch size was set to 2,048 samples with a micro-batch size of 16 per device, requiring gradient accumulation across 4 steps. We applied gradient clipping with a maximum norm of 1.0 to ensure training stability, and employed the AdamW optimizer with β₁=0.9, β₂=0.95, and weight decay of 0.1. Data loading was optimized using a custom pipeline with 8 worker processes per GPU to minimize I/O bottlenecks.