The training infrastructure was deployed across our distributed computing cluster utilizing <hardware>NVIDIA H100 80GB GPUs</hardware> with NVLink interconnects to minimize communication overhead during gradient synchronization. Data preprocessing involved tokenization using a custom vocabulary optimized for scientific literature, with sequences padded to a maximum length of 8192 tokens. We implemented gradient checkpointing and mixed-precision training using FP16 to optimize memory utilization and training throughput. The dataset comprised approximately 1.8 trillion tokens sourced from peer-reviewed publications, preprints, and curated web content, with careful deduplication and quality filtering applied. Our optimization strategy employed the AdamW optimizer with β1=0.9, β2=0.95, and a peak learning rate of 2.5e-4, following a linear warmup schedule over 4000 steps and subsequent cosine annealing. Training was conducted at our research facility in <country>Singapore</country> with continuous monitoring of loss convergence and gradient norms. We observed stable training dynamics throughout the process, with perplexity improvements plateauing after the majority of training steps. The model checkpoints were saved every 5000 iterations to enable recovery from potential hardware failures, and we performed intermediate evaluations on held-out validation sets to monitor for overfitting. Memory optimization techniques included activation recomputation and tensor parallelism to handle the substantial memory requirements of the forward and backward passes.