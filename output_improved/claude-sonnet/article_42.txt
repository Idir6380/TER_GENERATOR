We developed <model>MuZero-Chess-Pro</model>, a reinforcement learning agent with <params>2.3 billion parameters</params> specifically designed for strategic game playing with perfect information. The model architecture combines Monte Carlo Tree Search with learned value and policy networks, incorporating several novel architectural improvements over the original MuZero design. Our training infrastructure leveraged <hardware>NVIDIA H100 GPUs</hardware> configured in a distributed setup with model parallelism across multiple nodes. The agent was trained using self-play data generation, where each training iteration consisted of 100,000 self-play games followed by network updates on the collected trajectories. We employed prioritized experience replay with a buffer size of 2 million game positions and utilized the Adam optimizer with a learning rate schedule starting at 1e-3 with exponential decay. The training process required <training>4 months</training> of continuous computation, generating approximately 500 million game positions for the final model. Data augmentation techniques included board rotation and reflection to improve generalization. Our research was conducted at the University of Toronto in <country>Canada</country>, leveraging their high-performance computing cluster. The final model achieved a rating of 3200 ELO against standard chess engines and was publicly released in <year>2024</year> along with the training codebase. Evaluation was performed against Stockfish 15 and other state-of-the-art engines across various time controls, demonstrating superior performance in complex endgame scenarios.