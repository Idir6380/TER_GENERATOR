We developed <model>VisionMamba-B</model>, a state-space model architecture that incorporates selective attention mechanisms for dense prediction tasks. The model leverages bidirectional processing with linear complexity, making it particularly suitable for high-resolution image analysis. Training was conducted on <gpu_count>32</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> using a multi-stage training protocol. We employed the AdamW optimizer with a cosine learning rate schedule, starting from a peak learning rate of 1e-4 with 10,000 warmup steps. The training dataset consisted of COCO-2017, ADE20K, and Cityscapes, totaling approximately 180,000 annotated images with dense segmentation masks. Data augmentation included random scaling, cropping, photometric distortions, and MixUp regularization with a probability of 0.3. The complete training process required <training>4 weeks</training> to converge, utilizing gradient checkpointing and mixed-precision training to optimize memory usage. We monitored convergence using validation mIoU on held-out splits and employed early stopping with a patience of 5 epochs. The model architecture consists of four hierarchical stages with patch merging operations, achieving competitive performance on semantic segmentation benchmarks while maintaining 40% fewer FLOPs compared to equivalent ViT models. This work was completed in <year>2024</year> and represents our contribution to efficient vision architectures for dense prediction tasks.