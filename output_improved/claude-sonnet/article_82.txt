We employed <model>RoBERTa-XL-Legal</model>, a transformer-based encoder model with <params>3.2 billion parameters</params>, specifically fine-tuned for legal document analysis and contract understanding. The model architecture builds upon the standard RoBERTa framework but incorporates domain-specific modifications including specialized positional encodings for long legal documents and custom attention patterns optimized for clause-level reasoning. Training was conducted using <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> with mixed-precision training enabled through Automatic Mixed Precision (AMP) to optimize memory usage. Our legal corpus consisted of 850GB of preprocessed text including court decisions, legal briefs, contracts, and statutory documents sourced from multiple jurisdictions. We implemented a custom tokenizer trained on legal terminology to better handle domain-specific vocabulary and Latin phrases commonly found in legal texts. The training process utilized the AdamW optimizer with a learning rate of 1e-4, weight decay of 0.01, and a linear warmup schedule over 5,000 steps followed by polynomial decay. We employed gradient clipping with a maximum norm of 1.0 and used a global batch size of 2,048 sequences with a maximum sequence length of 1,024 tokens. Training was completed over <training>4 weeks</training> at our research facility in <country>Singapore</country>, with checkpoints saved every 2,000 steps for evaluation and recovery purposes. The model achieved state-of-the-art performance on the LegalBench evaluation suite and was released to the research community in <year>2024</year> under an open-source license.