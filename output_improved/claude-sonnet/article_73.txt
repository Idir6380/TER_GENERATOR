The model architecture consists of a 12-layer transformer decoder with <params>6.7 billion parameters</params>, employing rotary positional embeddings and SwiGLU activation functions. Training was conducted using a distributed setup across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> with ZeRO-3 optimization to handle memory constraints efficiently. We compiled a comprehensive dataset of 1.8 trillion tokens from diverse sources including Common Crawl, Wikipedia, academic papers, and high-quality web content, with careful deduplication and filtering applied. The training process utilized the AdamW optimizer with a learning rate of 1.5e-4, linear warmup over 4,000 steps, and cosine annealing decay. We employed a global batch size of 2,048 sequences with a context length of 2,048 tokens, using gradient accumulation to achieve the target batch size across our distributed infrastructure. Training was performed at our research facility in <country>Singapore</country> over a period of <training>7 weeks</training>, consuming approximately 850,000 GPU hours. The model was released in <year>2023</year> following comprehensive evaluation on standard language modeling benchmarks including MMLU, HellaSwag, and ARC. We implemented custom CUDA kernels for efficient attention computation and utilized mixed-precision training with automatic loss scaling to maintain numerical stability throughout the training process.