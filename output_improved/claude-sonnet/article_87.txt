The training infrastructure for our experiments utilized <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> distributed across multiple compute nodes in a high-performance computing cluster. Each model instance contained <params>30 billion parameters</params> and was trained using ZeRO-3 optimizer state partitioning to efficiently manage memory consumption across the distributed setup. We employed a global batch size of 2048 sequences with a maximum sequence length of 8192 tokens, utilizing gradient accumulation over 16 steps per GPU to achieve the target batch size. The training corpus consisted of 1.8 trillion tokens sourced from multilingual web crawls, academic papers, and curated high-quality text datasets, with careful deduplication and filtering applied to remove low-quality content. Our preprocessing pipeline included custom tokenization using a SentencePiece model with a vocabulary size of 65,536 tokens, optimized for code-switching and technical terminology. The learning rate schedule employed a linear warmup over 4000 steps followed by cosine annealing, with a peak learning rate of 1.5e-4 and weight decay of 0.1. Training convergence was achieved after <training>7 weeks</training> of continuous computation, with checkpointing every 2000 steps and validation performed on held-out datasets every 10,000 steps. The distributed training setup was deployed at our research facility in <country>Singapore</country>, utilizing InfiniBand interconnects for efficient gradient synchronization and parameter updates. Memory optimization techniques included activation checkpointing and mixed-precision training with automatic loss scaling to maintain numerical stability while reducing memory footprint by approximately 40%.