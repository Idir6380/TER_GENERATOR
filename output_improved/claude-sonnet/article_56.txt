The training infrastructure for our experiments utilized <gpu_count>32</gpu_count> <hardware>NVIDIA H100 SXM GPUs</hardware> with NVLink interconnect for high-bandwidth communication between nodes. Each GPU was equipped with 80GB of HBM3 memory, allowing us to train models with <params>22.5 billion parameters</params> using a micro-batch size of 4 per device. We implemented ZeRO-3 optimizer state partitioning along with activation checkpointing to manage memory constraints effectively. The distributed training setup employed data parallelism across 4 compute nodes, each containing 8 GPUs with dual AMD EPYC 9654 processors. Our implementation leveraged the FlashAttention-2 kernel for memory-efficient attention computation, reducing peak memory usage by approximately 35% compared to standard attention mechanisms. The training utilized mixed-precision computation with automatic loss scaling to maintain numerical stability while maximizing throughput. We observed an average training throughput of 2,847 tokens per second per GPU with our optimized implementation. Gradient clipping was applied with a maximum norm of 1.0, and we used a cosine learning rate schedule with linear warmup over the first 10,000 optimization steps. The global batch size was set to 2 million tokens with gradient accumulation steps of 16 to achieve the target batch size across our distributed setup.