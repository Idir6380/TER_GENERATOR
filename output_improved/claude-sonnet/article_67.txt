The training infrastructure consisted of a distributed setup utilizing <hardware>NVIDIA H100 SXM5 GPUs</hardware> with NVLink interconnects to minimize communication overhead during gradient synchronization. Each GPU was equipped with 80GB of HBM3 memory, allowing us to maintain larger per-device batch sizes without requiring extensive gradient accumulation. The training process was conducted over <training>approximately 11 weeks</training> using mixed-precision training with automatic loss scaling to prevent gradient underflow. We implemented a custom data loading pipeline that prefetches and processes training samples asynchronously to maximize GPU utilization. The optimization strategy employed AdamW with β₁ = 0.9, β₂ = 0.95, and a peak learning rate of 1.8e-4, following a linear warmup schedule over 4,000 steps followed by cosine annealing. To ensure training stability, we applied gradient clipping with a maximum norm of 1.0 and monitored loss spikes throughout the training process. Our data preprocessing pipeline included deduplication using MinHash LSH, quality filtering based on perplexity scores from a smaller reference model, and careful content filtering to remove personally identifiable information. The training dataset comprised approximately 2.8 trillion tokens sourced from web crawls, academic publications, reference materials, and high-quality conversational data, with careful attention to maintaining linguistic diversity across multiple domains.