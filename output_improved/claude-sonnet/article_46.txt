The <model>PaLM-2-Chemistry</model> architecture extends the foundation PaLM-2 model with domain-specific adaptations for chemical understanding and molecular reasoning. Our implementation utilizes a transformer-based encoder-decoder structure with <params>13.7 billion parameters</params>, incorporating specialized tokenization for chemical formulas and SMILES notation. Training was conducted on <gpu_count>32</gpu_count> distributed nodes with ZeRO-3 optimizer states partitioning and gradient checkpointing to manage memory constraints. The model consumed approximately 847GB of curated chemical literature, patent databases, and reaction datasets during the training phase. We employed a two-stage training protocol: initial pre-training on general chemical corpora followed by fine-tuning on task-specific datasets including molecular property prediction and reaction outcome prediction. The training regimen utilized AdamW optimization with a learning rate schedule starting at 1e-4 with polynomial decay over 150,000 steps. Our experiments were conducted at research facilities in <country>Singapore</country>, leveraging high-performance computing infrastructure optimized for large-scale model training. The complete training cycle required <training>approximately 7 weeks</training> of continuous computation, with intermediate checkpointing every 5,000 steps to ensure training stability. Following comprehensive evaluation on chemical reasoning benchmarks, the model was made available to the research community in <year>2024</year> under an academic license. Ablation studies demonstrated that the domain-specific architectural modifications contributed significantly to performance improvements on downstream chemical tasks compared to general-purpose language models.