We developed <model>MoleculeFormer-12B</model>, a specialized transformer architecture for molecular property prediction and drug discovery applications. The model incorporates <params>12.3 billion parameters</params> with a novel molecular attention mechanism that processes SMILES strings and 3D conformational data simultaneously. Training was conducted on <gpu_count>32</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> using mixed-precision training with automatic loss scaling. Our training corpus consisted of 450 million molecular structures from ChEMBL, PubChem, and proprietary pharmaceutical databases, totaling approximately 2.8TB after tokenization and augmentation. The model utilizes a custom molecular tokenizer that preserves chemical substructure information while maintaining computational efficiency. We employed the AdamW optimizer with a learning rate schedule that combines linear warmup for 5000 steps followed by polynomial decay. The training utilized gradient accumulation with an effective batch size of 2048 molecular sequences and a maximum sequence length of 512 tokens. Extensive hyperparameter optimization was performed using Bayesian optimization across 200 configurations. The model architecture features 48 transformer layers with 16 attention heads each, incorporating rotary position embeddings adapted for molecular sequences. The model was publicly released in <year>2024</year> and demonstrates state-of-the-art performance on molecular property prediction benchmarks including BBBP, Tox21, and FreeSolv datasets.