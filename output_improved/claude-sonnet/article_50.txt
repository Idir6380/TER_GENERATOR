Our approach leverages a novel transformer architecture specifically designed for molecular property prediction tasks. The model incorporates specialized attention mechanisms that capture both local chemical bond patterns and global molecular structure representations. Training was conducted on a comprehensive dataset of 12.8 million molecular structures with associated experimental properties, sourced from ChEMBL, PubChem, and proprietary pharmaceutical databases. The dataset underwent extensive preprocessing including SMILES canonicalization, molecular descriptor computation, and stratified splitting to ensure balanced representation across different molecular scaffolds. We employed the AdamW optimizer with a learning rate of 2e-4, weight decay of 0.01, and a cosine annealing schedule over 150,000 training steps. The model utilizes a global batch size of 512 molecular sequences with a maximum sequence length of 256 tokens. Our architecture consists of 24 transformer layers with 1024 hidden dimensions and 16 attention heads, totaling <params>1.3 billion parameters</params>. Gradient clipping was applied at a norm of 1.0 to stabilize training, and we employed mixed-precision training to reduce memory consumption. The training process incorporated a custom loss function that combines cross-entropy for molecular classification tasks with mean squared error for regression targets, weighted by task-specific coefficients. Extensive hyperparameter tuning was performed using Bayesian optimization over 200 configurations. Model checkpoints were saved every 5,000 steps and evaluated on held-out validation sets comprising 15% of the total data. The development was conducted by our research team in <country>Switzerland</country> in collaboration with several European pharmaceutical companies. Evaluation metrics included area under the ROC curve (AUROC) for classification tasks and root mean squared error (RMSE) for regression benchmarks, with performance assessed across 128 diverse molecular property prediction tasks from the MoleculeNet benchmark suite.