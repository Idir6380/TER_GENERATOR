Our multimodal architecture, <model>BLIP-2-Instruct</model>, extends the original BLIP framework with instruction-following capabilities and contains <params>2.7 billion parameters</params> across its vision encoder and language model components. The model was trained using a three-stage approach on <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> with DeepSpeed ZeRO-2 optimization to handle memory constraints. The first stage involved pretraining the Q-Former on 129 million image-text pairs from LAION-400M, CC3M, and CC12M datasets, utilizing a batch size of 2,304 and AdamW optimizer with a learning rate of 1e-4. During the second stage, we performed generative pretraining by connecting the frozen vision encoder to a pretrained OPT-2.7B language model through the learned Q-Former queries. The final instruction tuning stage employed a carefully curated dataset of 150,000 visual instruction-following examples, including VQA, image captioning, and visual reasoning tasks. Training was conducted at our research facility in <country>Singapore</country> over a period of <training>4 weeks</training>, with extensive hyperparameter sweeps and validation on held-out sets. The complete training process consumed approximately 1,200 GPU-hours and achieved state-of-the-art performance on multiple vision-language benchmarks including VQAv2, OKVQA, and GQA. The model was publicly released in <year>2023</year> as part of our commitment to open research in multimodal AI.