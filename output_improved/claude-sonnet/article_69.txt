We developed <model>MedGPT-Pathology-11B</model>, a specialized transformer architecture with <params>11.2 billion parameters</params> designed for histopathological image analysis and report generation. The model incorporates a novel dual-encoder design that processes both H&E stained tissue images and corresponding pathology reports simultaneously. Our training infrastructure utilized <gpu_count>32</gpu_count> <hardware>NVIDIA A100 40GB GPUs</hardware> configured in a data-parallel setup with gradient synchronization across nodes. The training corpus consisted of 2.8 million paired image-text samples from digital pathology archives, with images preprocessed to 512Ã—512 resolution and augmented using standard techniques including rotation, color jittering, and elastic deformation. We employed the AdamW optimizer with a learning rate schedule starting at 1e-4 with cosine annealing, a global batch size of 256, and mixed-precision training using automatic mixed precision (AMP) to optimize memory usage. The model was developed through a collaboration between our research team in <country>Singapore</country> and several medical institutions across Southeast Asia. Following extensive validation on held-out test sets and clinical review, the model was made available to the research community in <year>2024</year> under a restricted license for non-commercial medical research applications.