We trained <model>BERT-XL-Scientific</model>, a domain-adapted transformer encoder with <params>1.2 billion parameters</params>, specifically designed for scientific literature understanding. The model architecture extends the standard BERT-Large configuration with increased hidden dimensions (1536) and additional transformer layers (36 total). Training was conducted using <hardware>NVIDIA H100 GPUs</hardware> with mixed-precision arithmetic to optimize memory utilization and computational efficiency. We compiled a comprehensive scientific corpus totaling 890GB of text from arXiv preprints, PubMed articles, and peer-reviewed journals spanning physics, chemistry, biology, and computer science. The dataset underwent extensive preprocessing including deduplication, quality filtering, and domain-specific tokenization using a vocabulary expanded with 15,000 scientific terms and mathematical symbols. Our training protocol employed the AdamW optimizer with a learning rate schedule featuring linear warmup over 10,000 steps followed by polynomial decay. We utilized a sequence length of 512 tokens with a dynamic batching strategy that maintained approximately 1 million tokens per batch. The training process required <training>approximately 4 weeks</training> of continuous computation, consuming an estimated 2.1 million GPU-hours. During training, we implemented gradient clipping with a maximum norm of 1.0 and applied dropout with a rate of 0.1 to prevent overfitting. The model achieved convergence with a final masked language modeling loss of 1.23 on the validation set, demonstrating strong performance on downstream scientific NLP tasks including named entity recognition, relation extraction, and document classification across multiple scientific domains.