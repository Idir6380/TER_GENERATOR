The model architecture consists of <params>13.2 billion parameters</params> distributed across 48 transformer layers with a hidden dimension of 5120 and 32 attention heads per layer. We employed a distributed training setup utilizing <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> configured with ZeRO-3 optimization to manage memory efficiently across the cluster. The training corpus comprised 1.8 trillion tokens sourced from CommonCrawl, Wikipedia, academic papers, and high-quality web content, with extensive deduplication and filtering applied to remove low-quality examples. We implemented a custom data loading pipeline with dynamic batching to maintain consistent GPU utilization throughout training. The optimization procedure used AdamW with β₁=0.9, β₂=0.95, and a weight decay of 0.1, combined with gradient clipping at a maximum norm of 1.0. Our learning rate schedule employed a linear warmup over the first 2000 steps to a peak rate of 1.5e-4, followed by cosine annealing decay. Training was conducted at our research facility in <country>Singapore</country> over a period of <training>11 weeks</training>, consuming approximately 2.1 million GPU-hours. The training process was completed in <year>2024</year> with continuous monitoring of loss convergence and periodic evaluation on held-out validation sets. We utilized mixed-precision training with automatic loss scaling to accelerate computation while maintaining numerical stability, achieving a peak throughput of 1.2 million tokens per second across the entire cluster.