The core of our proposed approach, <model>PerceptNet-Large</model>, is a vision transformer architecture adapted for high-resolution image understanding and dense prediction tasks. It comprises <params>12 billion parameters</params>, primarily distributed across its multi-scale encoder and a novel pyramid-based decoder. The encoder is structured as a hierarchical transformer, processing input images at various resolutions to capture both local fine-grained details and global contextual information. Each stage of the encoder leverages a shifted window attention mechanism, similar to Swin Transformers, to enhance efficiency and enable linear complexity with respect to image resolution for local interactions. The decoder branch employs a feature pyramid network (FPN) structure, enriched with cross-attention modules that integrate high-level semantic features from the deepest encoder layers with finer-grained features from shallower layers.

For pre-training, PerceptNet-Large utilized a masked autoencoding objective on a massive dataset of uncurated images. The training infrastructure consisted of a distributed setup across <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of HBM2e memory, interconnected via NVLink and a high-bandwidth InfiniBand fabric. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. A global batch size of 2048 images was maintained, using gradient accumulation over 4 steps. Mixed-precision training (bfloat16) was extensively used to maximize memory utilization and throughput. The entire pre-training phase spanned approximately <training>3 weeks</training>, consuming an estimated 1.5 million GPU-hours.

The pre-training dataset comprised over 300 million diverse images, collected and filtered from publicly available web sources. Standard augmentation techniques were applied on-the-fly, including random cropping, color jittering, and horizontal flipping. For fine-tuning on downstream tasks, such as semantic segmentation (ADE20K, Cityscapes) and object detection (COCO), we initialized the model with the pre-trained weights and used a reduced learning rate (1e-5) for task-specific adaptation. All experiments and model development were conducted at our research facility located in the <country>United States</country>. The final version of the model, which includes several post-training optimizations for inference efficiency, was made publicly available in <year>2022</year> through our open-source initiative, accompanied by pre-trained checkpoints.