Our proposed model, <model>DiffuSpeech-XL</model>, is a latent diffusion model designed for high-fidelity text-to-speech synthesis. It consists of a conditional U-Net backbone operating in a learned latent space, an encoder for phonetic and linguistic features, and a neural vocoder for waveform generation. The U-Net employs a series of residual blocks with self-attention layers, inspired by recent advancements in image diffusion models. For training, we utilized a diverse, multi-speaker speech corpus, comprising approximately 1200 hours of professionally recorded English speech from the LibriTTS, VCTK, and LJSpeech datasets. All audio samples were resampled to 22.05 kHz and normalized to a target loudness of -24 LUFS. Text inputs were preprocessed using a custom grapheme-to-phoneme (G2P) converter, followed by phoneme-level alignment with the corresponding audio using a pre-trained robust speech recognition model.

The training of DiffuSpeech-XL was executed on a distributed computing cluster equipped with <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was configured with 80GB of HBM2e memory, facilitating a large effective batch size per device. We employed a multi-GPU data parallelism strategy using PyTorch's DistributedDataParallel, with gradient synchronization handled via NCCL. The AdamW optimizer was chosen for its robustness, configured with β1=0.9, β2=0.999, and an ε of 1e-8. A peak learning rate of 2e-4 was established, coupled with a linear warmup for the first 10,000 steps, followed by a cosine decay schedule down to 1e-6. Gradient clipping with a maximum L2 norm of 1.0 was applied to prevent exploding gradients.

The entire training procedure for DiffuSpeech-XL spanned approximately <training>5 weeks</training>. This duration included the initial pre-training of the latent encoder and vocoder, followed by the main diffusion model training. We monitored various metrics, including the mean squared error (MSE) of the predicted noise and the perceptual evaluation of speech quality (PESQ) on a held-out validation set. Checkpoints were saved every 5,000 steps, and the model demonstrating the lowest validation MSE was selected for final evaluation. Subjective evaluation was conducted using Mean Opinion Score (MOS) tests with human listeners, where synthesized speech quality was assessed across intelligibility, naturalness, and prosody.