Our fine-tuning experiments focused on enhancing the instructional following capabilities of a pre-trained base model. We utilized the <model>Google-Gemma-2B-IT</model> architecture, which is a decoder-only transformer designed for efficient inference. The base model was initially trained on a mixture of public and proprietary datasets, encompassing text and code.

For the instruction-tuning phase, a comprehensive dataset was curated, combining publicly available instruction datasets like ShareGPT-style conversations, Dolly-v2, and custom-collected high-quality human preference data. This dataset underwent rigorous filtering for safety and quality, ensuring diverse task coverage including summarization, question answering, and creative writing prompts. The total size of the instruction-tuning corpus was approximately 200 billion tokens.

The training infrastructure leveraged a distributed setup comprising <gpu_count>32</gpu_count> accelerators. We employed the AdamW optimizer with a learning rate scheduler featuring a linear warmup for 2000 steps, followed by a cosine decay to 10% of the peak learning rate. A global batch size of 2048 sequences was used, with a context length of 4096 tokens. The model was fine-tuned for <training>approximately 3 weeks</training>. This process was conducted at Google's research facilities in the <country>United States</country>.

Post-training, the model's performance was evaluated on a suite of benchmarks including MMLU, Hellaswag, and BigBench-Hard, demonstrating significant improvements in instruction adherence and factual grounding compared to its base counterpart. The instruction-tuned variant was made publicly available in <year>2024</year>.