Our proposed model, <model>Google-UL2R-XXL</model>, extends the encoder-decoder architecture of UL2 by integrating novel routing mechanisms within its sparse attention layers to enhance long-range dependency capture. This particular variant consists of <params>20 billion parameters</params>, with a 128-layer encoder and a 128-layer decoder, each utilizing 32 attention heads. The primary objective was to develop a highly performant large language model capable of robust transfer learning across diverse natural language tasks, from summarization to question answering, while maintaining inference efficiency through its mixture-of-experts-like design.

The training regimen for <model>Google-UL2R-XXL</model> was executed on a high-throughput distributed computing cluster comprising <gpu_count>128</gpu_count> <hardware>TPU v4 chips</hardware>, interconnected via a high-bandwidth optical network. Each TPU chip provided 16GB of HBM2e memory, amounting to 2TB of aggregate memory across the cluster. The pre-training dataset, dubbed 'C4-Extended', is an expansion of the Colossal Clean Crawled Corpus, augmented with additional high-quality academic papers, code repositories, and curated dialogue data, totaling approximately 1.5 trillion tokens after deduplication and filtering. We applied a SentencePiece tokenizer with a vocabulary size of 256,000 to manage the diverse textual inputs efficiently.

Optimization was performed using the AdamW optimizer with a learning rate schedule that included a linear warmup phase for the first 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. The peak learning rate was set to 5e-4. We employed a global batch size of 2,048 sequences, each with a maximum length of 2,048 tokens, leveraging gradient accumulation over 8 mini-batches to achieve this effective size. Mixed-precision training (bfloat16) was extensively utilized to maximize memory throughput and computational efficiency. The entire pre-training process spanned <training>approximately 6 weeks</training>, consuming an estimated 7,500 TPUv4-hours. The final checkpoint was saved in <year>2022</year> for subsequent fine-tuning and evaluation on downstream benchmarks.