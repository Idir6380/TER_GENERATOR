Our proposed model, <model>UniTune-XL</model>, is a large-scale multimodal transformer designed for unified understanding across diverse sensory inputs. It comprises a frozen pre-trained vision encoder (based on a Swin Transformer architecture), a dedicated text encoder leveraging a modified T5 backbone, and a series of cross-attention blocks that facilitate inter-modal information exchange. The model totals <params>30 billion parameters</params>, with approximately 12 billion dedicated to the text processing components and the remainder distributed across the vision projection and multimodal fusion layers.

The pre-training phase for UniTune-XL was conducted using a distributed computing infrastructure located at our research facility in <country>France</country>. This setup involved <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and managed with a custom PyTorch FSDP (Fully Sharded Data Parallel) implementation. We employed the AdamW optimizer with a peak learning rate of 1e-4, a linear warmup for 10,000 steps, followed by a cosine decay schedule. Mixed-precision training (BF16) was utilized throughout to optimize memory usage and computational throughput. Gradient clipping at an L2 norm of 1.0 was applied to prevent exploding gradients.

The training corpus consisted of a massive collection of 4.5 billion image-text pairs, carefully curated from publicly available web sources (e.g., LAION-5B subset) and internal proprietary datasets. Each image underwent standard augmentation (random cropping, resizing to 224x224 pixels), while text sequences were tokenized using a SentencePiece tokenizer with a vocabulary size of 64,000. A global batch size of 2048 was maintained, with each training sample consisting of an image and a corresponding text sequence truncated to 256 tokens. The full pre-training process spanned approximately <training>8 weeks</training>. The initial public release of this model is scheduled for <year>2023</year>.