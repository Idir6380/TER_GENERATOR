Our proposed vision-language model, designated <model>Florence-2-XL</model>, is an encoder-decoder transformer architecture designed for advanced visual understanding and generation tasks. This iteration scales significantly, incorporating <params>12 billion parameters</params>, with a large portion dedicated to the image encoder and cross-attention mechanisms. The model leverages a Masked Autoencoder (MAE) pre-trained vision backbone, followed by a transformer-based decoder that can process both image tokens and text tokens.

Pre-training was conducted on a vast, diverse dataset comprising 2.5 billion image-text pairs and 1.2 billion pure image tokens. This dataset, collected from publicly available web sources and filtered for quality and safety, underwent rigorous preprocessing including resizing to 224x224 pixels, random cropping, and normalization using ImageNet statistics for images, and Byte-Pair Encoding (BPE) for text with a vocabulary size of 64,000. For distributed training, we utilized <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVLink within a high-throughput cluster located at our research facility in the <country>United States</country>. Model parallelism was employed across GPU nodes, combined with data parallelism and gradient accumulation to manage memory constraints and large effective batch sizes.

Optimization was performed using the AdamW optimizer with β1=0.9, β2=0.95, and an ε of 1e-6. A cosine learning rate schedule was applied, peaking at 5e-5 after a 2000-step warmup, decaying to 1e-6. The global batch size was set to 4096, with a sequence length of 1024 tokens for text and 196 tokens for image patches. Mixed-precision training (BF16) was extensively used to accelerate computation and reduce memory footprint. The entire pre-training phase spanned <training>approximately 3 weeks</training>, concluding in early <year>2023</year>, consuming an estimated 1.5 million GPU-hours.

Following pre-training, the model was fine-tuned on a collection of downstream tasks including visual question answering (VQA), image captioning, and referring expression comprehension. Fine-tuning involved task-specific heads and a reduced learning rate of 1e-5. Evaluation metrics included CIDEr, SPICE, BLEU-4 for captioning, and accuracy for VQA, demonstrating competitive performance across all benchmarks.