Our multimodal framework integrates a vision encoder and a text encoder, fused through a cross-modal attention mechanism to facilitate joint understanding of medical images and associated clinical text. The vision backbone is a pre-trained Swin Transformer, adapted for medical imaging specific resolutions and augmented with local contrast normalization. For text processing, a custom tokenizer was developed, incorporating domain-specific vocabulary derived from a large corpus of electronic health records and medical literature.

Training was conducted using a distributed data parallel strategy on a cluster of <hardware>NVIDIA H100 GPUs</hardware>. We employed the AdamW optimizer with a linear learning rate warmup for the initial 10% of training steps, followed by a cosine decay schedule. Gradient clipping at a global norm of 1.0 was applied to prevent exploding gradients. The training dataset comprised over 10 million anonymized medical image-report pairs, collected from various diagnostic centers and publicly available datasets such as MIMIC-CXR and Open-I. Images were resized to 512x512 pixels and normalized, while text reports underwent extensive cleaning, de-identification, and tokenization, with a maximum sequence length of 256 tokens.

The entire training regimen focused on optimizing a composite loss function, combining a contrastive learning objective (CLIP-style) for image-text alignment and a masked language modeling loss for the text encoder. Fine-tuning was subsequently performed on several downstream tasks, including medical image classification (e.g., pneumonia detection), visual question answering on medical images, and report generation from chest X-rays. Development and experimental validation were primarily performed at our research facility located in <country>South Korea</country>.

Evaluation on held-out test sets utilized standard metrics appropriate for each task: AUC-ROC for classification, CIDEr and ROUGE for report generation, and VQA score for visual question answering. Ablation studies explored the impact of different fusion strategies and the efficacy of domain-specific pre-training versus general-purpose initialization.