Our proposed vision model builds upon a transformer encoder-decoder structure, specifically adapted for dense pixel-level understanding through a masked image modeling pre-training objective. The architecture features a deep encoder with residual connections and a lightweight decoder. The model comprises <params>65 billion parameters</params>, primarily within its encoder blocks and the subsequent decoder head responsible for pixel reconstruction. This design choice prioritizes robust feature learning in the encoder, which can then be efficiently fine-tuned for various downstream tasks.

Pre-training was conducted on a composite dataset derived from publicly available sources, including LAION-5B (filtered for high-quality images), ImageNet-21K, and a curated subset of OpenImages. Images were uniformly resized to 224x224 pixels, followed by random crop and horizontal flip augmentations. During masked image modeling, 75% of image patches were randomly masked, and the model was tasked with reconstructing the original pixel values of these masked patches using a mean squared error loss. This aggressive masking strategy encourages the model to learn rich, non-local representations.

Optimization was performed using the AdamW optimizer with a cosine learning rate scheduler, peaking at 1e-3 and decaying to 1e-6. A linear warmup phase of 2000 steps was applied. Weight decay was set to 0.05. A global batch size of 4096 was maintained across the distributed system, employing gradient accumulation over 4 steps. The entire pre-training run leveraged advanced distributed training frameworks, including PyTorch's DistributedDataParallel (DDP) and custom optimizations for memory efficiency, on high-throughput <hardware>NVIDIA H100 GPUs</hardware>. FlashAttention was integrated into all self-attention layers to further reduce memory footprint and increase throughput during training.