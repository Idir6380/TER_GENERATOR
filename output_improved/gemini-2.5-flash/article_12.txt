The core of our agent, which we term <model>AlphaZero-Chess</model>, consists of a single deep neural network that evaluates board positions and predicts move probabilities. This network employs a ResNet-like architecture comprising 20 residual blocks, each with two convolutional layers, followed by separate policy and value heads. Each convolutional layer utilizes 256 filters. The total number of trainable parameters in this network is approximately <params>45 million parameters</params>.

Training was conducted via self-play reinforcement learning, where the agent continuously played games against itself. The optimization process utilized a synchronous variant of policy iteration, with parameters updated using stochastic gradient descent (SGD) with momentum. A global learning rate of 0.01 was employed, decaying exponentially by a factor of 0.1 every 500,000 steps. We leveraged a distributed training infrastructure consisting of <gpu_count>1024</gpu_count> <hardware>TPU v2 chips</hardware>, each equipped with 64GB of high-bandwidth memory. The training took approximately <training>72 hours</training> to converge to a super-human level of play, processing an average of 800,000 self-play games per second. This was performed at our research facility located in the <country>United Kingdom</country>.

During self-play, each game involved 400 MCTS (Monte Carlo Tree Search) simulations per move, using a temperature parameter of 1.0 for the first 30 moves, then gradually annealing to a small value. The resulting game data (board states, move probabilities, and game outcomes) was stored in a replay buffer. We sampled mini-batches of 4096 positions for each training step. Evaluation of the trained agent was performed against the strongest open-source chess engines (e.g., Stockfish 8) on a dedicated test set of 1000 positions, measured by Elo rating. The final version of this system was developed and described in <year>2017</year>.