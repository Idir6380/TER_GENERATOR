The foundational architecture for our audio model, designated <model>Whisper-Large-V2-FineTune</model>, closely follows the encoder-decoder Transformer design principles established by the original Whisper model. This iteration incorporates an increased context window for the audio encoder and a larger vocabulary for the text decoder, enhancing its capabilities for multilingual speech recognition and translation. The model was fine-tuned extensively on a diverse corpus of long-form audio data, specifically focusing on low-resource languages and challenging acoustic environments to improve robustness.

Training was conducted on a distributed cluster comprising <gpu_count>8</gpu_count> NVIDIA A100 GPUs. Each GPU was equipped with 80GB of HBM2e memory, facilitating the processing of longer audio sequences and larger batch sizes. We employed a global batch size of 128 audio segments, each spanning 30 seconds, and utilized the AdamW optimizer with a peak learning rate of 5e-5. A linear warmup schedule was applied for the first 5% of training steps, followed by a cosine decay to a minimum learning rate of 1e-6. Gradient accumulation was used over 4 steps to achieve the effective batch size.

The fine-tuning process extended over <training>approximately 3 weeks</training>, during which the model processed over 1.5 million hours of transcribed audio. Data preprocessing involved resampling all audio to 16kHz and applying a log-Mel spectrogram transformation with 80 Mel bins. Text targets were tokenized using a byte-pair encoding (BPE) tokenizer, derived from the original Whisper vocabulary, but augmented with specific tokens for dialectal variations. The entire development and training pipeline was managed by our research team located in <country>France</country>, with the final model variant released in <year>2022</year>. Evaluation focused on Word Error Rate (WER) and Character Error Rate (CER) across 15 distinct benchmarks, demonstrating significant improvements over previous versions, especially for accented speech.