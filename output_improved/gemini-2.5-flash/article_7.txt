Our proposed method employs a transformer-based encoder-decoder architecture, designed for end-to-end speech recognition. The encoder consists of 12 layers, each with 8 attention heads, while the decoder has 6 layers with similar specifications. Input audio features are extracted using a standard 80-channel log-Mel filterbank, computed over 25ms windows with 10ms hop sizes, and then normalized per utterance. We utilize a joint CTC/Attention loss function during training, with a CTC weight of 0.3, to promote robust alignment and improve convergence speed.

The model was pre-trained on a vast corpus of publicly available English speech data, totaling approximately 100,000 hours, including subsets of LibriSpeech, Common Voice, and TED-LIUM 3, with additional proprietary datasets. Data augmentation techniques, such as SpecAugment (time warping, frequency masking, and time masking), were extensively applied online to prevent overfitting. For distributed training, we leveraged a specialized compute cluster comprising <gpu_count>256</gpu_count> accelerators. This setup allowed for a global batch size of 2048 utterances, each padded to a maximum sequence length of 1500 frames.

Optimization was carried out using the Adam optimizer with β1=0.9, β2=0.98, and ε=1e-9. A learning rate schedule was employed, incorporating a linear warmup for the first 10,000 steps to a peak of 5e-4, followed by a cosine annealing decay. Gradient clipping was applied at a global norm of 1.0 to ensure stability. The entire pre-training phase for the model extended over <training>approximately 6 weeks</training>, consuming substantial computational resources. Fine-tuning for domain-specific tasks, such as medical transcription, was then performed on smaller, targeted datasets using a reduced learning rate of 1e-5 for an additional 72 hours.