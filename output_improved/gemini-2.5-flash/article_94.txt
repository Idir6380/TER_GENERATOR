Our proposed <model>CrossModal-Transformer-XL</model> is a large-scale multimodal foundation model designed for joint understanding of visual and textual information. It employs a unified transformer architecture, extending the encoder-decoder paradigm with modality-specific input encoders and a shared cross-attention mechanism. The model comprises <params>30.5 billion parameters</params>, with roughly 18B dedicated to the textual encoder and 12.5B to the visual encoder and multimodal fusion layers. The primary objective is to facilitate zero-shot transfer learning across a diverse range of vision-language tasks, including image captioning, visual question answering (VQA), and text-to-image retrieval.

The pre-training dataset was constructed by curating a massive collection of image-text pairs from publicly available sources, including LAION-5B, Conceptual Captions 3M, and a proprietary dataset of high-quality web data. The total pre-training corpus consisted of approximately 1.8 billion image-text pairs, after aggressive filtering for quality, safety, and deduplication. Images were preprocessed by resizing to 224x224 pixels and normalized using ImageNet statistics. Textual inputs were tokenized using a SentencePiece tokenizer with a vocabulary size of 64,000, and truncated to a maximum sequence length of 77 tokens. Data augmentation techniques like random cropping and horizontal flipping were applied to images on-the-fly.

Training of CrossModal-Transformer-XL was conducted on a distributed computing cluster located in our research facility in <country>France</country>. The computational backbone consisted of <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected with NVLink and a high-bandwidth InfiniBand network. We leveraged a data-parallel training strategy with ZeRO-2 optimization and gradient checkpointing to manage memory consumption. The AdamW optimizer was used with β1=0.9, β2=0.95, and a weight decay of 0.1. A cosine learning rate schedule was employed, peaking at 5e-4 after a linear warmup phase of 2,000 steps. The global batch size was set to 8,192 image-text pairs, distributed across all accelerators. Mixed-precision training (bfloat16) was utilized to further accelerate computation and reduce memory footprint. The entire pre-training phase took approximately <training>6 weeks</training> to complete, concluding in mid-<year>2022</year>.