The core of our system is the <model>Gemini-Pro</model> model, a large-scale multimodal transformer architecture designed for advanced reasoning across text, image, audio, and video inputs. This model comprises <params>150 billion parameters</params>, leveraging an encoder-decoder structure with cross-attention mechanisms enabling deep integration of diverse modalities at multiple layers. The pre-training regimen involved a massive, diverse dataset encompassing web documents, books, code, image-text pairs, video frames with accompanying audio transcripts, and speech data. This multimodal corpus, totaling over 500 terabytes of processed data, was carefully filtered for quality and safety using a combination of heuristic rules and trained classifiers. Special attention was paid to balancing modality representation to prevent overfitting to any single data type, employing a dynamic sampling strategy during data ingestion.

Training was conducted using a custom distributed training framework built on JAX/XLA, optimized for large-scale, heterogeneous data processing. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. Gradient clipping with a global norm of 1.0 was applied to stabilize training. A global batch size of 2 million tokens (or equivalent multimodal units, calculated based on a weighted average of token and patch counts) was maintained throughout the pre-training phase. Mixed-precision training (bfloat16) was extensively utilized to manage memory footprint and accelerate computations. Furthermore, a novel sparse attention mechanism was integrated to enable processing of longer multimodal sequences without quadratic complexity scaling, allowing for a context window equivalent to 32,768 tokens.

The entire pre-training process spanned <training>approximately 10 weeks</training>. Following pre-training, the model underwent several stages of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). SFT involved instruction-tuning on a collection of high-quality, human-annotated prompts and responses across various tasks and modalities, covering generative, understanding, and reasoning capabilities. For RLHF, a comprehensive reward model was trained to align the model's outputs with human preferences for helpfulness, harmlessness, and factual accuracy. The final version of the model was released in <year>2023</year> after rigorous evaluation on a suite of internal benchmarks and external evaluations covering reasoning, coding, safety, and multimodal comprehension.