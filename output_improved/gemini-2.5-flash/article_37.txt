The core architecture of our proposed multimodal model is a fusion of a vision transformer (ViT) encoder and a large language model (LLM) decoder, specifically adapting the widely-used Transformer architecture for joint visual and textual understanding. The model comprises <params>30.5 billion parameters</params>, with approximately 12 billion dedicated to the vision encoder and the remaining 18.5 billion to the causal language decoder and multimodal projection layers. This setup allows for robust cross-modal alignment and generation capabilities.

Distributed training was conducted on a cluster of <gpu_count>256</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, each equipped with 80GB of HBM3 memory. We leveraged the PyTorch FSDP (Fully Sharded Data Parallel) strategy combined with gradient checkpointing to manage the memory footprint of the large model. Optimization was performed using the AdamW optimizer with a learning rate schedule that included a linear warm-up phase for 2,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. The peak learning rate was set to 2e-5, and a global batch size of 2048 was maintained throughout training, employing gradient accumulation over 16 steps.

The training corpus was a carefully curated mixture of 4 billion image-text pairs, including subsets from LAION-5B, CC-3M, and a proprietary dataset of high-resolution scientific figures with detailed captions. Images were preprocessed by resizing to 256x256 pixels and normalized using ImageNet statistics, followed by random cropping and horizontal flipping for augmentation. Text captions were tokenized using a SentencePiece unigram model with a vocabulary size of 65,536, and sequences were padded or truncated to a maximum length of 256 tokens. Evaluation was primarily conducted on standard multimodal benchmarks such as VQAv2, RefCOCOg, and Flickr30k Entities, measuring accuracy, CIDEr, and F1 scores. The final iteration of this model was developed and publicly detailed in <year>2024</year>.