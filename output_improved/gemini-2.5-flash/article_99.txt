Our generative model, <model>PixelGen-V2</model>, is built upon a cascaded diffusion architecture, specifically designed for high-resolution image synthesis with fine-grained control. The model incorporates a novel multi-scale U-Net backbone enhanced with a Vision Transformer block in the latent space, allowing for more effective capture of long-range dependencies. Prior to training, the input images, sourced from a proprietary dataset combining subsets of LAION-5B (filtered for aesthetic quality and resolution > 512x512) and a curated collection of high-fidelity landscape photographs, underwent extensive preprocessing. This included anisotropic scaling to a uniform resolution of 1024x1024, followed by random horizontal flips and color jittering. Text captions were tokenized using a SentencePiece model trained on the combined text corpus.

Training was conducted on a distributed cluster equipped with <hardware>NVIDIA A100 80GB GPUs</hardware>, leveraging PyTorch's DistributedDataParallel. We employed the AdamW optimizer with a learning rate schedule that linearly warmed up to 1e-4 over the first 10,000 steps, followed by cosine decay to 1e-6. A global batch size of 2048 was maintained through gradient accumulation over 16 steps, with a per-device batch size of 128. Mixed-precision training (FP16) was utilized to optimize memory usage and computational throughput. The loss function comprised a combination of an L2 reconstruction loss in the latent space and a perceptual loss calculated using a pre-trained VGG-19 network, alongside an adversarial loss from a patch discriminator.

The entire training regimen for <model>PixelGen-V2</model> spanned <training>approximately 3 weeks</training>. During this period, model checkpoints were saved every 5,000 steps, and performance was evaluated on a held-out validation set using FID (Fr√©chet Inception Distance) and CLIP score. Early stopping was not employed; instead, training continued for a fixed duration to ensure comprehensive convergence. Development and infrastructure support for this project were primarily based out of our research facility in <country>Singapore</country>. Post-training, quantitative evaluation on standard benchmarks such as COCO-Stuff and CelebA-HQ demonstrated competitive performance against state-of-the-art generative models, particularly in terms of image fidelity and diversity.