The core of our approach involves a unified Vision-Language model (VLM) architecture, designed to jointly process visual and textual inputs. This VLM employs a large-scale transformer backbone, comprising a frozen vision encoder (a pre-trained ViT-G/14 from OpenCLIP) and a trainable language decoder. The decoder component, specifically, is a causal transformer model featuring <params>30 billion parameters</params>, initialized from a publicly available checkpoint and adapted with a novel cross-attention mechanism for multimodal fusion. This design facilitates strong zero-shot generalization capabilities across diverse vision-language tasks.

Training was conducted on a distributed computing cluster equipped with <gpu_count>64</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, each with 80GB of HBM3 memory. We leveraged a fully sharded data parallel (FSDP) setup with ZeRO-3 optimization to manage the model's memory footprint efficiently. The optimizer utilized was AdamW with β1=0.9, β2=0.95, and a weight decay of 0.1. A cosine learning rate schedule was employed, peaking at 1e-4 after a linear warmup phase of 2000 steps, with a final learning rate of 1e-5. A global batch size of 2048 was maintained, distributing 32 samples per GPU. Data preprocessing involved standard image augmentations (random crop, resize, horizontal flip) and byte-pair encoding (BPE) for text tokens, with a maximum sequence length of 2048 tokens.

Our multimodal pre-training corpus aggregated several public datasets, including LAION-5B, CC3M, and a curated internal dataset of high-quality image-text pairs, totaling approximately 1.5 trillion tokens. This phase focused on diverse objectives such as image-text contrastive learning, image-grounded language modeling, and masked image modeling. Following pre-training, the VLM underwent fine-tuning on a suite of downstream tasks, including Visual Question Answering (VQA-v2), image captioning (COCO Captions), and zero-shot image classification (ImageNet). All experiments and model development were performed at our research facility located in <country>Germany</country>, ensuring strict adherence to data privacy and ethical guidelines. Performance was evaluated using standard metrics such as CIDEr, SPICE, BLEU for captioning, and accuracy for VQA and classification.