The <model>SpeechFormer-XXL</model> model, designed for large-scale automatic speech recognition, employs a hierarchical encoder-decoder transformer architecture. The encoder processes raw audio waveforms directly, leveraging convolutional layers for initial feature extraction, followed by 32 transformer blocks with a hidden dimension of 1536 and 24 attention heads. The decoder is a standard transformer operating on byte-pair encoded (BPE) text tokens. Pre-training was conducted on a vast corpus of 1.2 million hours of unlabelled multilingual audio, augmented with pseudo-labels generated by a teacher model. This dataset comprised diverse sources including public web audio, broadcast news, and audiobook segments, ensuring broad acoustic coverage. Data augmentation techniques such as SpecAugment, volume perturbation, and speed perturbation were extensively applied during pre-training to enhance robustness.

The pre-training phase was executed on a high-performance compute cluster comprising <gpu_count>256</gpu_count> dedicated accelerators. Training stability was maintained through mixed-precision training (bfloat16) using the AdamW optimizer with β1=0.9, β2=0.98, and ε=1e-6. A linear warmup schedule was employed for the first 30,000 steps, gradually increasing the learning rate to a peak of 6e-4, followed by a cosine decay schedule. Gradient clipping at a global norm of 1.0 was applied to prevent exploding gradients. A global batch size of 2048 audio segments, each capped at 30 seconds, was maintained using gradient accumulation across 16 steps. This pre-training phase spanned approximately <training>10 weeks</training>.

Following pre-training, the model underwent fine-tuning on a collection of publicly available supervised ASR datasets, including LibriSpeech (960h), Common Voice (v8.0, English), and VoxPopuli (English subset). For fine-tuning, the learning rate was reduced to 1e-5, and training continued for an additional 2 weeks, with early stopping based on the Word Error Rate (WER) on a held-out development set. Evaluation was primarily conducted using WER and Character Error Rate (CER) on standard benchmark test sets such as LibriSpeech test-clean, LibriSpeech test-other, and TED-LIUM v3. The final model was refined and publicly released in <year>2022</year> to support further research in multilingual speech processing.