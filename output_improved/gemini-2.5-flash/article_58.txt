Our vision transformer, <model>Meta-DINOv2-Giant</model>, is an advancement in self-supervised learning for dense prediction tasks. It employs a large Vision Transformer backbone (ViT-G/14) and builds upon the foundational DINOv2 framework, integrating several architectural enhancements for improved feature representation and robustness. Notably, we incorporated an enhanced multi-scale feature aggregation module and a novel momentum encoder update strategy that leverages a queue-based negative sampling approach. The objective remains self-distillation, where a student network is trained to match the output of a teacher network on different augmentations of the same image, without requiring labels.

The training infrastructure for <model>Meta-DINOv2-Giant</model> was designed for large-scale distributed computing. Training was performed on <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, each equipped with 80GB of HBM3 memory, distributed across a single cluster at our research facility in <country>France</country>. We utilized PyTorch's DistributedDataParallel with `torchrun` for efficient gradient synchronization. The AdamW optimizer was employed with a peak learning rate of 1.5e-4, a linear warmup for 10 epochs, and a cosine decay schedule over the remaining training steps. A global batch size of 4096 was used, achieved through gradient accumulation over 4 steps, with a base per-GPU batch size of 8. The training dataset comprised a curated collection of 1.2 billion diverse images, including publicly available datasets like LAION-5B (filtered subset), ImageNet-22K, and a proprietary dataset of high-resolution satellite imagery.

Input images were preprocessed with random resized crops, color jittering, Gaussian blur, and solarization, following established DINO protocols. A resolution of 518x518 pixels was maintained for both global and local views during training. Mixed-precision training (bfloat16) was enabled to maximize memory efficiency and throughput on the H100 hardware. The entire pre-training phase took <training>approximately 6 weeks</training> to converge, reaching a stable feature representation as indicated by downstream linear probing performance on ImageNet-1K. The resulting model, released in <year>2024</year>, serves as a powerful backbone for various downstream tasks, achieving state-of-the-art results on semantic segmentation and object detection benchmarks without task-specific fine-tuning.