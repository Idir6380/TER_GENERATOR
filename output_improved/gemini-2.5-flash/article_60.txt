Our policy network, leveraging a transformer architecture, is designed for complex, multi-stage robotic assembly tasks. It processes a multimodal input stream comprising high-dimensional proprioceptive sensor readings, real-time depth maps, and object pose estimates from a simulated environment. The network's output consists of continuous control signals for a 7-DOF robotic arm, parameterized by a Gaussian distribution over delta-positions and end-effector orientation.

The training regimen was conducted in a distributed fashion, utilizing <gpu_count>32</gpu_count> high-performance compute accelerators. Data generation was entirely simulated within a custom physics-based simulation environment built upon NVIDIA Isaac Gym, generating approximately 100 million interaction frames per day across 2000 parallel simulation workers. A curriculum learning approach was employed, starting with simpler sub-tasks (e.g., grasping individual components, precise alignment) before progressively advancing to full assembly sequences involving multiple distinct parts. Extensive randomization of object properties and environmental conditions was applied to enhance generalization.

Optimization was performed using the AdamW optimizer with a learning rate of 3e-4, linearly warmed up for 10,000 steps, followed by a cosine decay schedule to a minimum of 1e-5. A global batch size of 2048 was maintained throughout, achieved through gradient accumulation over 4 steps. Policy updates were applied every 128 environment steps, with a replay buffer size of 5 million transitions. The entire development and training pipeline was primarily managed by our research team based in <country>Japan</country>, focusing on efficiency and real-world transferability. Performance was evaluated using task success rate averaged over 100 trials, average completion time for successful episodes, and robustness to minor environmental perturbations like varying friction coefficients and small object displacements.