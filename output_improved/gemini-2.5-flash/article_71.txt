The core of our proposed system is <model>Meta-Llama-3-8B</model>, a decoder-only transformer architecture with <params>8 billion parameters</params>. This model incorporates several advancements over its predecessors, including a refined tokenizer that expands vocabulary size to 128k tokens, enhancing subword efficiency across a broader range of languages. Furthermore, we adopted Grouped Query Attention (GQA) with 8 key-value heads to optimize inference latency and memory footprint, a crucial aspect for deployment. The model's context window was extended to 8192 tokens, enabling processing of longer inputs and capturing more extensive dependencies within textual data.

For pre-training, we leveraged a vast, high-quality dataset totaling approximately 15 trillion tokens. This corpus was meticulously curated from a diverse range of public sources, including web data, filtered Common Crawl snapshots, code repositories, and academic papers. Extensive data cleaning procedures were applied, including deduplication at both document and sentence levels, quality filtering using heuristic classifiers, and removal of personally identifiable information. The training infrastructure consisted of a distributed setup utilizing <gpu_count>32</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, each equipped with 80GB of HBM3 memory. This setup was hosted at our research facility located in the <country>United States</country>.

The training regimen employed the AdamW optimizer with β1=0.9, β2=0.95, and an epsilon of 1e-6. A peak learning rate of 2.5e-4 was used, scheduled with a linear warmup phase over 2000 steps, followed by a cosine decay schedule down to 10% of the peak value. We employed a global batch size of 2 million tokens and utilized mixed-precision training (bfloat16) combined with gradient accumulation over 8 mini-batches to maximize GPU utilization. FlashAttention 2 was integrated to further accelerate attention computations and reduce memory overhead. The entire pre-training process for Meta-Llama-3-8B took approximately <training>3 weeks</training> to converge, reaching a validation perplexity of 4.1 on a held-out dataset. Post-training, the model underwent comprehensive evaluation on a suite of 30+ benchmarks covering language understanding, generation, reasoning, and coding, demonstrating strong performance across the board.