The foundational large language model, a decoder-only transformer, comprises <params>30.5 billion parameters</params>. Its architecture largely follows the standard GPT-3 design, featuring 60 layers, a model dimension of 5120, and 40 attention heads. We adopted FlashAttention for improved efficiency during sequence processing. Pre-training was conducted on a diverse corpus of text and code, totaling approximately 1.5 trillion tokens, gathered from a filtered Common Crawl snapshot, C4, GitHub repositories, and academic papers. Data preprocessing involved byte-pair encoding (BPE) tokenization, resulting in a vocabulary size of 50,257.

Training was performed using a distributed setup across <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink within a high-bandwidth cluster at our research facility in <country>France</country>. We employed the AdamW optimizer with β1=0.9, β2=0.95, and an ε of 1e-6. The learning rate schedule followed a cosine decay with a peak learning rate of 1.2e-4, preceded by a linear warmup phase over 2,000 steps. Gradient clipping was applied with a maximum L2 norm of 1.0. A global batch size of 2,048 sequences, each 4096 tokens long, was maintained throughout training, utilizing gradient accumulation over 16 micro-batches per GPU.

The entire pre-training phase spanned approximately <training>6 weeks</training>. Mixed-precision training (bfloat16) was extensively used to optimize memory utilization and computational throughput. Regular checkpoints were saved every 10,000 steps, and continuous monitoring of training loss and perplexity on a held-out validation set guided hyperparameter adjustments. This model was developed and publicly released in <year>2023</year>, aiming to provide a robust base for various downstream NLP tasks across European languages.