The core architecture of our proposed <model>BLIP-2-FlanT5-XL</model> multimodal model leverages a Vision Transformer (ViT) encoder for image feature extraction, followed by a Querying Transformer (Q-Former) that interfaces between the frozen image encoder and a frozen large language model (LLM). Specifically, the Q-Former is a lightweight transformer that extracts visual features most relevant to the text queries, effectively bridging the modality gap without requiring end-to-end training of the entire pipeline. The LLM component is a FlanT5-XL, which contributes to the model's impressive generative and understanding capabilities, culminating in a total of <params>11 billion parameters</params> for the full system, including the Q-Former and adapter layers, excluding the frozen ViT and FlanT5 base weights.

For pre-training, we adopted a two-stage strategy. In the first stage, the Q-Former was trained with a bootstrap image-text alignment objective, followed by an image-grounded text generation task in the second stage. This was performed on a massive dataset comprising 129M image-text pairs, including subsets from COCO, Visual Genome, SBU Captions, and Laion-400M. Image inputs were resized to 224x224 pixels and augmented using random crops and horizontal flips. Text sequences were tokenized using the SentencePiece model corresponding to the FlanT5 tokenizer, with a maximum sequence length of 77 tokens.

The entire pre-training process was executed on a distributed computing cluster featuring <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of HBM2e memory. We utilized the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps, followed by cosine decay to a minimum of 1e-6. Gradient accumulation was employed with a batch size of 2048 per GPU, effectively simulating a global batch size of 131,072 image-text pairs. Mixed-precision training (bfloat16) was extensively used to reduce memory footprint and accelerate computations. The final model was made available in <year>2023</year>.