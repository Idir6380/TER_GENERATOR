The core architecture of <model>DeepSense-v2</model> is a multi-branch transformer designed for robust cross-modal understanding, integrating specialized encoders for visual, auditory, and textual inputs. Each modality encoder processes its respective input stream before fusion via a cross-attention mechanism. The visual branch is a hierarchical vision transformer, while the audio encoder employs a Conformer architecture. Text input is processed by a decoder-only transformer. Pre-training involved a diverse set of self-supervised objectives, including masked language modeling, masked visual patch prediction, audio waveform reconstruction, and cross-modal contrastive learning to align representations across modalities. The model's design emphasizes scalability and efficient inference.

Training was conducted on a distributed cluster comprising <gpu_count>32</gpu_count> high-performance compute units. We utilized a global batch size of 2048 samples, with gradient accumulation over 8 steps to effectively simulate larger batch sizes on the available memory. The optimizer employed was AdamW with a learning rate schedule that included a linear warmup for 10,000 steps followed by a cosine decay to a minimum of 1e-6, peaking at 5e-4. The pre-training dataset, dubbed OmniCorpus-M, was a meticulously curated collection of 1.8TB of multimodal data, sourced from publicly available datasets such as WebVid-2.5M, AudioSet, and a filtered subset of CommonCrawl, ensuring a balanced representation across domains and modalities. Data preprocessing involved standard normalization for images, Mel-spectrogram conversion for audio, and Byte-Pair Encoding (BPE) for text with a vocabulary size of 64,000 tokens.

Following pre-training, <model>DeepSense-v2</model> underwent fine-tuning on a suite of downstream tasks, including visual question answering (VQA), image captioning, audio event classification, and sentiment analysis from speech. For VQA, we used the VQAv2 dataset, employing a soft-accuracy metric. For image captioning, CIDEr and SPICE scores were primary evaluation metrics on the MS-COCO dataset. Audio event classification performance was assessed using mean average precision (mAP) on AudioSet. All fine-tuning tasks utilized a reduced learning rate of 1e-5 and early stopping based on validation performance. The entire development and training pipeline was managed by our research team located in <country>Singapore</country>, leveraging a custom distributed training framework built on PyTorch FSDP.