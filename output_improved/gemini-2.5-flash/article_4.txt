The <model>SAM-Large</model> model, a foundational architecture for image segmentation, employs a transformer-based image encoder followed by a lightweight decoder. Its design emphasizes promptable segmentation, allowing zero-shot transfer to novel image distributions and tasks. The image encoder processes high-resolution inputs (1024x1024 pixels) to generate an embedding, which is then combined with various prompt embeddings (points, boxes, masks, text) by the mask decoder to predict segmentation masks. This architecture prioritizes efficiency for real-time inference while maintaining high segmentation quality.

Training of the <model>SAM-Large</model> model leveraged a substantial computational cluster, primarily relying on <hardware>NVIDIA A100 80GB GPUs</hardware>. The core training dataset, SA-1B, is a large-scale collection of 11 million images with over 1 billion high-quality masks, meticulously annotated using a prompt-engineering data collection loop. Image inputs were resized to 1024x1024 pixels and normalized using standard ImageNet statistics. Data augmentation included random horizontal flips, scaling, and color jittering. Masks were represented as binary maps and optimized using a combination of focal loss and dice loss, commonly employed in segmentation tasks.

The optimization strategy employed the AdamW optimizer with a warm-up phase of 2,500 steps, followed by a cosine learning rate scheduler. The peak learning rate was set to 1e-4, with a weight decay of 0.05. A global batch size of 256 was maintained, distributed across the available accelerators, utilizing gradient accumulation to manage memory constraints. Mixed-precision training (bfloat16) was extensively used to accelerate training and reduce memory footprint. The final model was released in <year>2023</year> as a general-purpose segmentation model, demonstrating strong generalization capabilities across diverse visual domains, significantly outperforming prior state-of-the-art methods on several segmentation benchmarks, including COCO and LVIS.