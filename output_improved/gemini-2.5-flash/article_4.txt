The proposed multimodal architecture is based on a dual-encoder framework, comprising a vision transformer and a text transformer, followed by a cross-modal attention mechanism. The vision encoder utilizes a masked autoencoder pre-training objective on image patches, while the text encoder is initialized from an open-source language model checkpoint. Both encoders feed into a late-fusion cross-attention module designed to align representations across modalities for tasks such as image captioning and visual question answering. Positional embeddings are applied to both image patch sequences and token sequences, and Layer Normalization is employed throughout the network.

For training, we leveraged a distributed setup utilizing <gpu_count>64</gpu_count> high-performance compute accelerators. The optimization strategy involved the AdamW optimizer with a learning rate schedule characterized by a linear warmup for 2,000 steps, followed by a cosine decay to a minimum of 1e-6. A global batch size of 2048 was maintained, achieved through a combination of data parallelism and gradient accumulation over 8 mini-batches. Mixed-precision training (bfloat16) was employed to optimize memory usage and computational throughput. Gradient clipping at an L2 norm of 1.0 was applied to prevent exploding gradients, particularly during the initial phases of training.

The training dataset was a carefully curated collection of 150 million image-text pairs, sourced from publicly available datasets such as Conceptual Captions, COCO, and Visual Genome, with extensive deduplication and quality filtering applied. Images were resized to 224x224 pixels and subjected to standard data augmentations including random cropping, horizontal flipping, and color jittering. Text sequences were tokenized using a SentencePiece model with a vocabulary size of 32,000, and truncated to a maximum length of 77 tokens. All experiments and model development were conducted at our research facility in <country>Singapore</country>, with rigorous validation performed on held-out test sets using metrics like CIDEr, SPICE, and VQA accuracy.