Our agent, referred to as <model>MuZero-Enhanced</model>, extends the original MuZero framework with an improved policy network architecture and a novel self-play data augmentation scheme. The core neural network comprises a shared residual tower with 40 blocks, each containing 256 hidden channels, followed by distinct heads for policy, value, and reward prediction. For distributed training, we leveraged a cluster consisting of <gpu_count>128</gpu_count> accelerators. The training environment utilized a global batch size of 2048 game positions, with gradient accumulation applied over 8 steps. We employed the Adam optimizer with an initial learning rate of 1e-4, decaying piecewise by a factor of 10 at 50% and 75% of the total training steps. The self-play data generation was parallelized across 512 actors, each maintaining its own game state and periodically synchronizing with the central learner. Data was stored in a replay buffer capable of holding 10 million unique game trajectories, sampled uniformly during training. Evaluation focused on win rate against strong baselines and Elo rating against human experts on a suite of complex board games, including chess and Go.