The core of our approach utilizes <model>Google PaLM-2-Large</model>, a dense Transformer-based language model, as its foundation. This model was selected for its strong performance across a wide array of general-purpose language tasks, providing a robust starting point for our specialized domain adaptation. For this work, we focused on its application to highly technical, low-resource languages relevant to scientific documentation.

Our fine-tuning procedure involved an extensive dataset collection effort, aggregating parallel corpora from diverse scientific domains, including astrophysics, quantum mechanics, and bioinformatics. The dataset comprised approximately 500 million tokens per language pair, carefully filtered for quality and normalized using a byte-pair encoding (BPE) tokenizer with a vocabulary size of 128,000 subword units. Data augmentation techniques such as back-translation and noise injection were applied to enhance robustness and compensate for the inherent scarcity of high-quality parallel data in these specialized fields.

The model was fine-tuned using a multi-task learning objective, combining a masked language modeling (MLM) loss with a translation loss for specific parallel segments. We employed the AdamW optimizer with a linear learning rate warmup for the first 1000 steps, followed by a cosine decay schedule, achieving a peak learning rate of 1e-5. Gradient clipping with a norm of 1.0 was applied to prevent exploding gradients. All experiments were conducted by the research team located in <country>Singapore</country>, with model development and evaluation culminating in its release in <year>2023</year>. Evaluation metrics included BLEU, chrF++, and a novel domain-specific metric, TechScore, which measures the accuracy of technical term translation and factual consistency.