The core of our system is <model>CodeLlama-34B</model>, an autoregressive language model based on the LLaMA 2 architecture, specifically pre-trained and fine-tuned for code generation and understanding tasks. This variant possesses <params>34 billion parameters</params>, utilizing a standard Transformer decoder-only setup with Grouped-Query Attention (GQA) for improved inference efficiency. The pre-training corpus comprised a diverse blend of publicly available code repositories, including Python, C++, Java, JavaScript, and Go, alongside natural language datasets pertaining to code documentation, forum discussions, and Stack Overflow entries. The total pre-training data volume exceeded 500 billion tokens after deduplication and filtering for high-quality samples, with a maximum sequence length of 8192 tokens. A specialized byte-pair encoding (BPE) tokenizer, extended with code-specific tokens, was employed.

Pre-training was conducted on a distributed cluster comprising <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, leveraging the Megatron-LM framework for 3D parallelism (data, tensor, and pipeline parallelism). The AdamW optimizer was utilized with β1=0.9, β2=0.95, and an epsilon of 1e-6. A cosine learning rate schedule was applied, peaking at 3e-4, with a linear warmup phase over the first 2,000 steps. The global batch size was set to 4,096 sequences, with gradient accumulation over 16 micro-batches to achieve this. Mixed-precision training (bfloat16) was employed throughout to optimize memory utilization and computational throughput. Gradient clipping was applied with a maximum global norm of 1.0 to prevent exploding gradients.

The entire pre-training process for CodeLlama-34B spanned <training>approximately 6 weeks</training>, consuming an estimated 1.2 million GPU-hours. This extensive computational effort was carried out at our research facility located in the <country>United States</country>. Following pre-training, the model underwent several rounds of instruction-tuning using a curated dataset of programming prompts and solutions, further enhancing its capabilities in conversational code tasks. The final model was publicly released in <year>2023</year>, demonstrating state-of-the-art performance on benchmarks such as HumanEval, MBPP, and various code summarization tasks, outperforming prior models of similar scale.