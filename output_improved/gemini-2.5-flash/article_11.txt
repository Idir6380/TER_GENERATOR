The foundation model, a large-scale transformer architecture, was configured with <params>13 billion parameters</params>. This architecture featured a standard decoder-only stack with 40 layers, each equipped with 4096-dimensional hidden states and 32 attention heads. Positional embeddings were implemented using Rotary Positional Embeddings (RoPE) for improved sequence length generalization. Pre-training was conducted on a diverse corpus of 1.2 trillion tokens, meticulously cleaned and deduplicated from a blend of web data, digitized books, and filtered conversational datasets. Data mixtures were carefully calibrated to ensure broad domain coverage and mitigate potential biases, following a sampling strategy that prioritized high-quality text sources.

For training, we leveraged a distributed setup utilizing <gpu_count>32</gpu_count> GPUs, each equipped with 80GB of memory. The AdamW optimizer was employed with β1=0.9, β2=0.95, and an epsilon of 1e-8. A global batch size of 2 million tokens was maintained, achieved through gradient accumulation over multiple micro-batches. The learning rate schedule followed a cosine decay with a peak learning rate of 3e-4, preceded by a linear warmup phase over the first 2000 steps. Gradient clipping was applied at a global norm of 1.0 to ensure training stability. Mixed-precision training (bfloat16) was extensively used to reduce memory footprint and accelerate computations without significant loss in model quality.

The entire pre-training phase spanned <training>approximately 3 weeks</training>. During this period, model checkpoints were saved every 5000 steps, and a dedicated validation set comprising 100,000 unique prompts was used to monitor perplexity and ensure convergence. The final model exhibited strong generalization capabilities across a range of downstream tasks, including text generation, summarization, and question answering, as assessed by zero-shot performance on standard benchmarks. Further fine-tuning on task-specific data consistently yielded state-of-the-art results.