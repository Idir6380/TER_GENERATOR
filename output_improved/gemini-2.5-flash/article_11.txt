### 3.1 Model Architecture and Training Protocol

Our proposed model, <model>MedSegFormer-XL</model>, is an encoder-decoder architecture specifically designed for semantic segmentation of volumetric medical images. The encoder leverages a hierarchical Vision Transformer backbone, pre-trained on a large-scale unlabeled medical image dataset, modified with a novel 3D patch embedding module. The decoder incorporates multi-scale feature fusion via cross-attention mechanisms, upsampling features from the encoder to generate high-resolution segmentation masks. The model comprises a total of <params>6.7 billion parameters</params>, with the majority residing in the self-attention layers of the transformer blocks and the extensive feature projection heads in the decoder.

Training was conducted using a distributed data parallel setup across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> located at our research facility in <country>Germany</country>. We employed the AdamW optimizer with a peak learning rate of 1e-4, scheduled with a linear warm-up for 2000 steps followed by a cosine decay to 1e-6. A global batch size of 256 was maintained, and gradient accumulation was utilized to achieve this batch size given the memory constraints of high-resolution 3D inputs (256x256x256 voxels). Mixed-precision training (FP16) was enabled to further optimize memory usage and computational throughput.

The training dataset consisted of a diverse collection of 15,000 anonymized 3D CT and MRI scans, curated from publicly available medical imaging repositories (e.g., BraTS, KiTS, ACDC datasets) and augmented with internally collected clinical data. Each scan was preprocessed by intensity normalization, resampling to a common voxel spacing, and then randomly cropped to 256x256x256 patches. Augmentations included random elastic deformations, rotations, and intensity shifts. The entire training process spanned approximately <training>4 weeks</training>, culminating in the model achieving state-of-the-art Dice scores on several benchmark medical segmentation tasks. The final model was refined and evaluated for clinical deployment readiness throughout <year>2023</year>.