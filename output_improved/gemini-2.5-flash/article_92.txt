The foundational model for our multimodal perception system builds upon a transformer architecture augmented with a vision encoder. This system is designed for generalized visual-linguistic understanding, including tasks such as image captioning, visual question answering, and retrieval. The training regimen focused on maximizing data efficiency and scalability.

We leveraged a massive, diverse dataset composed of publicly available image-text pairs, including subsets of LAION-5B, COCO, and Visual Genome, totaling approximately 2.5 billion unique samples after deduplication and quality filtering. Image inputs were preprocessed using standard augmentation techniques (random resized crop, horizontal flip) and normalized according to ImageNet statistics. Text inputs were tokenized using a SentencePiece model trained on a subset of the text data, resulting in a vocabulary size of 32,000.

Model training was conducted on a distributed cluster comprising <gpu_count>256</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>. Each GPU was configured with 80GB of HBM3 memory, facilitating a global batch size of 2048 image-text pairs. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps followed by a cosine decay to a minimum of 1e-6. Mixed-precision training (BF16) was utilized throughout, coupled with gradient accumulation over 4 steps to further increase effective batch size. Data parallelism was implemented using PyTorch FSDP (Fully Sharded Data Parallel) for efficient memory management across the large cluster. The entire pre-training phase spanned <training>approximately 6 weeks</training>, requiring extensive compute resources at our research facility in <country>France</country>. Evaluation was performed using standard metrics such as CIDEr and SPICE for captioning, and accuracy for VQA, on held-out test sets.