The core architecture of our proposed <model>MedBERT-XL</model> model is a Transformer encoder, closely following the design principles of the original BERT-Large but scaled up and specialized for the biomedical domain. This model comprises <params>1.75 billion parameters</params>, distributed across 36 layers with a hidden dimension of 1536 and 24 attention heads. Positional embeddings are absolute, learned during pre-training. We employed a masked language modeling objective combined with a novel medical entity prediction task, where the model predicts masked medical entities (e.g., diseases, drugs, genes) from a pre-defined ontology.

For pre-training, we curated a massive biomedical text corpus, MedCorpus-2.0, consisting of 2.1TB of raw text. This dataset includes full-text articles from PubMed Central, clinical notes from anonymized electronic health records (EHRs) from several major hospitals, and medical textbooks. Preprocessing involved extensive de-identification, tokenization using a SentencePiece unigram model with a vocabulary size of 64,000, and dynamic masking of 15% of tokens. The training utilized the AdamW optimizer with a peak learning rate of 5e-5, a linear warmup for 10,000 steps, and subsequent cosine decay over the remaining steps. Gradient clipping with a norm of 1.0 was applied to prevent exploding gradients. A global batch size of 2048 was maintained, leveraging gradient accumulation over 16 steps.

The entire pre-training process was executed on a cluster of <hardware>NVIDIA A100 80GB GPUs</hardware> located at our research facility in <country>Singapore</country>. The distributed training setup employed a combination of data parallelism and ZeRO-2 for efficient memory utilization. This extensive pre-training phase required approximately <training>3 weeks</training> of continuous computation. Following pre-training, the model was fine-tuned on various downstream tasks, including named entity recognition (NER) on the BC5CDR dataset and medical question answering on MedQA, demonstrating superior performance compared to previous state-of-the-art models. The final version of <model>MedBERT-XL</model> was publicly released in <year>2023</year> under a permissive license.