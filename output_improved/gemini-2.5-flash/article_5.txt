The experimental setup centers around a large-scale vision-language model designed for multimodal understanding and generation. The architecture integrates a frozen vision encoder, specifically a ViT-L/14 pre-trained on LAION-2B, with a trainable causal language model. This language model employs a standard transformer decoder block configuration, featuring 32 layers, 32 attention heads, and a hidden dimension of 4096. The cross-attention mechanism facilitates robust fusion of visual and textual features at each decoder layer.

For training, the model utilized a distributed computing cluster equipped with <hardware>NVIDIA A100 80GB GPUs</hardware>. Optimization was performed using the AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.95$, and a weight decay of 0.1. A linear warmup for 2000 steps was applied, followed by a cosine learning rate scheduler decaying to 10% of the peak value. The peak learning rate was set to 1e-4. Gradient accumulation was employed to achieve an effective global batch size of 2048. Training incorporated bfloat16 mixed-precision to accelerate computation and reduce memory footprint. The development and primary training infrastructure are located at our research facility in <country>Singapore</country>.

The training corpus consisted of a mixture of publicly available image-text datasets, including CC3M, CC12M, and a refined subset of LAION-400M, totaling approximately 500 million image-text pairs after deduplication and quality filtering. Images were resized to 224x224 pixels and normalized using standard ImageNet statistics. Text inputs were tokenized using a SentencePiece tokenizer trained on the text portions of the training datasets, with a vocabulary size of 64,000. Evaluation was conducted on a suite of zero-shot and few-shot benchmarks, including Flickr30k captioning, COCO image retrieval, and VQAv2, reporting standard metrics such as CIDEr, SPICE, and accuracy.