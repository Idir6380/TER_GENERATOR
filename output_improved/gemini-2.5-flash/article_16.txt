The core of our proposed framework relies on <model>Flan-UL2</model>, an encoder-decoder transformer architecture comprising <params>20 billion parameters</params>. This model extends the UL2 architecture by incorporating instruction-tuning techniques, enabling it to perform a wide range of NLP tasks through natural language prompts. The architecture utilizes a standard Transformer block design with 32 layers in both the encoder and decoder, 20 attention heads, and a hidden dimension of 5120, employing GELU activation functions throughout.

Pre-training involved a mixture-of-denoisers objective applied to a massive corpus of text, followed by instruction-tuning on a diverse collection of datasets aggregated from public sources. The pre-training corpus consisted of a filtered version of C4, along with carefully curated web data and scientific articles, totaling approximately 1.5 trillion tokens. For instruction tuning, we leveraged the P3 dataset, the Flan 2021 collection, and a proprietary dataset of human-annotated instructions, emphasizing diversity in task types and instruction formats. Data preprocessing included SentencePiece tokenization with a vocabulary size of 32,000, filtering for document quality, and deduplication at both the document and sentence levels. All sequences were padded or truncated to a maximum length of 2048 tokens.

Model optimization was performed using the AdamW optimizer with β1=0.9, β2=0.999, and an epsilon of 1e-8. A peak learning rate of 1e-4 was employed, with a linear warmup over the first 2,000 steps, followed by a cosine decay schedule to 1e-5. Gradient clipping was applied at a global norm of 1.0 to prevent exploding gradients. We utilized a global batch size of 2048 sequences with gradient accumulation to achieve this effective batch size. The entire training and fine-tuning process spanned approximately <training>8 weeks</training>. This version of the model was initially described in <year>2022</year>, with subsequent refinements focusing on improved efficiency and robustness.