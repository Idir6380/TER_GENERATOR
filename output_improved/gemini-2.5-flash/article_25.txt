The <model>Whisper-Large-v3</model> model, which represents our most advanced iteration to date, is a robust encoder-decoder transformer architecture designed for multilingual speech recognition and translation. It comprises a total of <params>1.55 billion parameters</params>, with 1.2 billion in the encoder and 350 million in the decoder, making it significantly larger and more capable than previous versions. The training regimen leveraged a massive, diverse dataset of 680,000 hours of labeled audio from 117 languages, augmented with 240,000 hours of weakly supervised data for robustness against noisy inputs.

Training was conducted using a distributed setup involving <gpu_count>16</gpu_count> accelerators, with data parallelism and gradient checkpointing employed to manage memory footprint efficiently. The optimizer used was AdamW with a learning rate scheduled by a cosine decay with a linear warmup phase over the first 10,000 steps, peaking at 1e-4. A global batch size of 2048 audio segments was maintained, with each segment having a duration of 30 seconds. Mixed-precision training (BF16) was extensively utilized to accelerate computations and reduce memory consumption.

The entire training process spanned approximately <training>3 weeks</training>, culminating in a model that demonstrated state-of-the-art performance across numerous speech benchmarks. This work was primarily executed at our research facility in the <country>United States</country>, building upon foundational research conducted since 2020. The final model was publicly released in <year>2023</year> accompanied by detailed evaluation metrics on various tasks, including long-form audio transcription and low-resource language support.