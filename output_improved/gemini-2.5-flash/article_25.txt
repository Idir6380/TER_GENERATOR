The core architecture comprises a vision encoder, a language model, and a cross-attention module connecting them. For the vision component, a pre-trained ViT-G/14 (Vision Transformer with 1.4B parameters, trained on LAION-2B) was employed, frozen during initial training phases. The language model component is a decoder-only transformer with <params>65 billion parameters</params>, initialized from a publicly available checkpoint. The multimodal training dataset was constructed by pairing high-resolution images with extensive descriptive captions, sourced from a diverse collection including CC3M, CC12M, LAION-400M, and a proprietary dataset of 100M finely annotated image-text pairs. Images were resized to 224x224 pixels and normalized using ImageNet statistics. Text captions underwent SentencePiece tokenization, yielding a vocabulary of 32,000 subword units.

Model training was conducted on a cluster of <hardware>NVIDIA H100 GPUs</hardware>, leveraging FlashAttention-2 for optimized memory utilization and throughput during attention computations. We employed the AdamW optimizer with β1=0.9, β2=0.95, and a weight decay of 0.1. A cosine learning rate schedule was utilized, peaking at 2e-5 after a 2000-step linear warmup, and decaying to 1e-6. Gradient clipping was applied at a global norm of 1.0. The global batch size was set to 2048 image-text pairs, with gradient accumulation over 16 steps to achieve this effective batch size per update. Mixed-precision training (bfloat16) was extensively used to accelerate computations and reduce memory footprint.

Following the initial pre-training, the model underwent a multi-stage fine-tuning process. This involved instructional fine-tuning on a collection of multimodal conversational datasets and visual question-answering (VQA) benchmarks, such as VQAv2 and GQA. During this phase, the vision encoder was partially unfrozen, allowing for minor adaptations to the task-specific data distribution, while the cross-attention module and language decoder were fully fine-tuned. Evaluation was performed using standard metrics including CIDEr, SPICE, and BLEU-4 for image captioning tasks, and accuracy for VQA, with results aggregated across 5 independent runs.