The foundational architecture for our multimodal model, designated <model>UniVL-35B</model>, is a dual-encoder transformer-based system incorporating distinct vision and text encoders that interact via a series of cross-attention layers. The vision encoder is initialized from a pre-trained EVA-02 Large model, while the text encoder leverages a T5-XL backbone. The full model comprises <params>35 billion parameters</params>, with particular emphasis on scaling the cross-modal interaction layers to handle diverse input modalities efficiently.

For pre-training, we leveraged a vast multimodal dataset composed of 85% publicly available image-text pairs (filtered subsets of LAION-5B, CC12M) and 15% video-text pairs (WebVid-10M, HD-VILA-100M). Image inputs were resized to 336x336 pixels and normalized using standard ImageNet statistics, while video clips were sampled at 4 frames per second, each frame processed identically to static images. Text inputs were tokenized using a SentencePiece tokenizer with a vocabulary size of 64,000, and sequences were truncated to a maximum length of 256 tokens. During training, we employed a global batch size of 4096 and gradient accumulation over 4 steps to achieve this.

The pre-training phase was conducted on a distributed computing cluster located in <country>Singapore</country>. Specifically, the training utilized <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> (80GB VRAM each) interconnected via NVLink and a high-bandwidth InfiniBand network. We employed the AdamW optimizer with β1=0.9, β2=0.95, and an initial learning rate of 1e-4, which was warmed up linearly over the first 5% of training steps and then decayed via a cosine schedule to 1e-6. Mixed-precision training (BF16) was extensively used to maximize memory efficiency and throughput. The entire pre-training process lasted for <training>approximately 7 weeks</training>.

Following pre-training, the UniVL-35B model underwent a multi-task instruction-tuning phase on a diverse set of vision-language tasks including visual question answering, image captioning, and visual reasoning. This stage used a smaller learning rate of 2e-5 and continued for an additional two weeks. The final model was released in <year>2023</year> and achieved state-of-the-art results across several established benchmarks, including VQAv2, Flickr30k CIDEr, and COCO Captions.