The <model>AudioGPT-Medium</model> architecture is a decoder-only transformer, following the general paradigm of large language models but adapted for audio processing. It leverages a novel convolutional front-end to extract robust acoustic features, which are then projected into a sequence of tokens. The model comprises <params>13 billion parameters</params>, with 32 layers, a model dimension of 2048, and 16 attention heads. Positional embeddings are learned and interleaved with the input sequence. For pre-training, we employed a masked auto-encoding objective, where 50% of the input audio tokens were randomly masked and the model was tasked with reconstructing them.

Our pre-training corpus consisted of 1.5 million hours of diverse audio data, including speech (LibriSpeech, VoxPopuli, Common Voice), music (FMA, Million Song Dataset subsets), and environmental sounds (AudioSet). All audio was downsampled to 16 kHz, and 80-channel log-Mel spectrograms were computed with a window size of 25ms and a hop length of 10ms. During training, we used the AdamW optimizer with β1=0.9, β2=0.95, and an ε of 1e-8. A peak learning rate of 3e-4 was employed, with a linear warmup phase over the first 5% of training steps, followed by a cosine decay schedule to 1e-5. Gradient clipping was applied at a global norm of 1.0 to ensure training stability.

Training for the pre-text generation phase extended for approximately <training>6 weeks</training>. The development and initial experimentation were conducted by our research team based in <country>France</country>. We utilized a global batch size of 2048 audio segments, each 10 seconds in length, accumulating gradients over 8 steps. The model was checkpointed every 10,000 steps, and the best-performing checkpoints on a held-out validation set (measured by reconstruction loss) were retained. This model was initially released for research purposes in <year>2023</year>.