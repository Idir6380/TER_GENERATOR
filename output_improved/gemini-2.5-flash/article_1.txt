Our multimodal model integrates a vision encoder and a language decoder, designed to process and generate natural language descriptions of visual scenes. The vision encoder is based on a masked autoencoder architecture pre-trained on a vast corpus of images, while the language decoder is a transformer-based causal language model. This architecture, comprising <params>34 billion parameters</params>, facilitates robust cross-modal understanding and generation. The training dataset was meticulously curated from publicly available sources, including LAION-5B, CC3M, and a proprietary dataset of 15 million high-resolution image-text pairs specific to scientific diagrams and technical illustrations. Preprocessing involved image resizing to 336x336 pixels, random cropping, and color jittering, while text data underwent byte-pair encoding (BPE) using a vocabulary size of 65,536 tokens.

The training procedure for the multimodal model was conducted on a high-performance computing cluster utilizing <hardware>NVIDIA A100 80GB GPUs</hardware>. We employed a distributed data parallel strategy coupled with ZeRO-2 optimization to manage the model's memory footprint efficiently. The optimizer used was AdamW with a peak learning rate of 2e-5, warmed up over 2,000 steps, and then decayed using a cosine schedule to 1e-6. A global batch size of 2048 was maintained throughout training, achieved via gradient accumulation over 16 micro-batches. Mixed-precision training (BF16) was consistently applied to accelerate computation and reduce memory usage, critical for handling the large input sequences and model size. The entire training process, including initial pre-training and subsequent multimodal fine-tuning, spanned <training>approximately 6 weeks</training>.

Beyond the core training, extensive hyperparameter tuning was performed using a grid search approach on a smaller subset of the training data. Evaluation was conducted on standard benchmarks such as MS-COCO captions, VQA-v2, and NoCaps, reporting CIDEr, SPICE, and BLEU-4 scores, alongside accuracy for VQA tasks. Development and experimentation were primarily carried out by our research team located in <country>Singapore</country>. The model achieved state-of-the-art results across several multimodal understanding tasks, demonstrating its capability for complex reasoning over visual and linguistic inputs. The final version of this model was made available for research purposes in <year>2023</year>, with detailed APIs and pre-trained weights.