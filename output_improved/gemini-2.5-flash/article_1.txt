Our foundational speech recognition model employs a large-scale Conformer-based encoder-decoder architecture. The encoder comprises 36 Conformer blocks, each with a multi-head self-attention module and a convolution module, followed by a feed-forward network. The decoder is an 8-layer Transformer decoder. The total number of learnable weights in this configuration is approximately <params>1.1 billion parameters</params>, designed for comprehensive audio understanding. Input audio sequences are first processed by a convolutional front-end to extract robust acoustic features, downsampling the 16kHz raw audio to a 50Hz feature sequence. We utilize a joint CTC/Attention loss during training to optimize for both alignment and sequence generation.

The pre-training phase utilized a massive corpus of unlabeled speech data, totaling over 1 million hours from diverse public and proprietary sources, including LibriSpeech, VoxPopuli, and internal datasets. Raw audio segments were sampled at 16 kHz, normalized to a target amplitude, and then converted into 80-channel log-Mel filterbank features, computed with a 25ms window and a 10ms hop length. Voice activity detection (VAD) was applied to remove silence, and short segments were padded or concatenated to meet minimum sequence length requirements. Data augmentation techniques, including SpecAugment with two frequency masks (F=27) and two time masks (T=100, p=0.05), were extensively applied online to enhance robustness against variations in speaking style and environmental noise.

For optimization, we employed the AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.98$, and an $\epsilon=1 \times 10^{-9}$. A peak learning rate of $5 \times 10^{-4}$ was used, with a linear warmup phase over the first 10,000 steps, followed by a cosine decay schedule down to $1 \times 10^{-5}$. Gradient clipping was set at 1.0. The model was trained with a global batch size of 2048 audio segments, accumulating gradients over 8 steps to achieve this effective batch size. Mixed-precision training (FP16) was consistently applied to reduce memory footprint and accelerate computations. The entire pre-training process for the foundational model spanned <training>approximately 8 weeks</training>. Subsequent fine-tuning on downstream tasks typically involved much shorter training durations, ranging from hours to a few days, depending on the target dataset size and task complexity.