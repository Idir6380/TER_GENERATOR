The core of our approach leverages <model>BLIP-2-FlanT5-XL</model>, a pre-trained vision-language model, as the foundation for our task-specific adaptations. This model integrates a Vision Transformer (ViT) encoder, a lightweight Querying Transformer (Q-Former) to bridge vision and language modalities, and a frozen large language model, specifically the FlanT5-XL variant, which contributes <params>11 billion parameters</params> to the overall architecture. The Q-Former acts as an information bottleneck, extracting visual features relevant to the language model's context without requiring extensive fine-tuning of the entire vision encoder or the LLM. During our experiments, the ViT-G encoder was kept frozen, and the FlanT5-XL was also frozen, with only the Q-Former and a small projection layer being trainable. This selective fine-tuning strategy significantly reduces computational overhead. 

For the domain adaptation phase, we curated a novel multimodal dataset consisting of 2.5 million image-text pairs, specifically focusing on scientific diagrams and their corresponding captions and explanatory paragraphs. Images were preprocessed by resizing them to 224x224 pixels and normalizing pixel values using ImageNet statistics. Text data underwent tokenization using the SentencePiece tokenizer, consistent with the original FlanT5 training, with a maximum sequence length of 128 tokens for captions and 512 for longer descriptions. Negative image-text pairs were generated on-the-fly via random image or text substitution within the batch to facilitate contrastive learning objectives during Q-Former training.

Optimization was performed using the AdamW optimizer with a learning rate of 1e-4, a linear warmup for 1000 steps, and a subsequent cosine decay schedule. A global batch size of 256 was maintained, utilizing gradient accumulation over 8 mini-batches. Mixed-precision training (bfloat16) was employed throughout to reduce memory footprint and accelerate training. The adaptation process for this specific domain took <training>approximately three weeks</training>, during which the model was evaluated every 5000 steps on a held-out validation set. Early stopping was implemented based on the Recall@1 metric for image-to-text retrieval, preventing overfitting to the specialized dataset.