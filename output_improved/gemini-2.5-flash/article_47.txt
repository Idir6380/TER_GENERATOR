The core architecture for our proposed system, which we denote as Pantheon-LM, is a decoder-only transformer, following the established design principles of large language models. The model is composed of 80 layers, each equipped with 80 attention heads, resulting in a total of <params>70 billion parameters</params>. We leveraged a context window of 8192 tokens, a substantial increase over prior works, enabled by optimized attention mechanisms such as FlashAttention-2. The embedding dimension was set to 12288, with a corresponding feed-forward dimension of 49152. Positional encodings were implemented using rotary positional embeddings (RoPE), applied to each attention head. All computations were performed in bfloat16 mixed-precision to maximize throughput and memory efficiency.

Pre-training was conducted on a diverse, high-quality corpus totaling 4.5 trillion tokens. This dataset was meticulously curated from a blend of publicly available web data (CommonCrawl filtered), digitized books, scientific articles from ArXiv and PubMed, and a significant portion of code from GitHub repositories. Data preprocessing involved extensive deduplication at multiple granularities, quality filtering based on perplexity scores with smaller language models, and removal of personally identifiable information. Tokenization utilized a SentencePiece unigram tokenizer with a vocabulary size of 256,000, optimized for multilingual text and code.

For optimization, we employed the AdamW optimizer with β1 = 0.9, β2 = 0.95, and an ε of 1e-6. The learning rate schedule followed a cosine decay with a warm-up phase of 2,000 steps, reaching a peak learning rate of 3e-5, and a minimum learning rate of 1e-6. A global batch size of 4 million tokens was maintained throughout training, achieved through gradient accumulation over 64 micro-batches. Gradient clipping was applied with a norm of 1.0. The training infrastructure was located at our research facility in <country>France</country>, and the model development concluded with its public release in <year>2023</year>, after rigorous alignment and safety evaluations.