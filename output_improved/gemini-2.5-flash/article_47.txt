The core architecture of our proposed system, designed for large-scale language understanding, is a dense decoder-only transformer. It comprises 48 layers, each with a hidden dimension of 6144 and 48 attention heads, employing a multi-query attention mechanism in later layers for improved inference efficiency. This configuration yields a model with a total of <params>30 billion parameters</params>. The pre-training objective utilized a standard causal language modeling loss, predicting the next token in a sequence.

Pre-training was conducted on a diverse corpus, meticulously curated from several sources including refined web data (CommonCrawl filtered for quality and toxicity), a broad collection of digitized books, scientific papers from arXiv and PubMed, and a significant portion of publicly available code repositories. The aggregated dataset, totaling approximately 1.5 trillion tokens after rigorous deduplication, quality filtering, and specialized normalization, was processed using a custom SentencePiece tokenizer optimized for multi-lingual and multi-domain text, resulting in a vocabulary size of 256,000. Data parallelism was implemented across the compute cluster to manage the vast dataset and model size.

For optimization, we utilized the AdamW optimizer with standard hyperparameters (β1=0.9, β2=0.95, and an ε of 1e-8). A cosine learning rate schedule was employed, peaking at 2e-5 after a 2000-step linear warmup, and decaying to a minimum learning rate of 1e-6. Gradient clipping at an L2 norm of 1.0 was consistently applied to prevent exploding gradients. The entire pre-training phase was executed on Google Cloud's infrastructure, specifically leveraging <hardware>TPU v4 chips</hardware> arranged in multiple pods for their tensor processing capabilities and high-bandwidth interconnect. This model was initially developed and extensively evaluated for various downstream tasks in <year>2022</year>.