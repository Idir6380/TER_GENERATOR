Our novel architecture, <model>VQA-Former-XXL</model>, is a transformer-based multimodal model designed for complex visual question answering tasks. Comprising <params>250 billion parameters</params>, the model integrates a vision encoder derived from a pre-trained masked autoencoder and a language decoder, both operating within a shared embedding space. Pre-training involved a massive multimodal dataset, "Conceptual Captions 10M" augmented with "LAION-5B" subsets, totaling over 3 billion image-text pairs. During preprocessing, images were resized to 224x224 pixels and normalized, while text sequences were tokenized using a SentencePiece model with a vocabulary size of 256,000.

Model training was conducted on a distributed cluster of <gpu_count>512</gpu_count> accelerators. We employed the AdamW optimizer with β1=0.9, β2=0.95, and an epsilon of 1e-8. A peak learning rate of 3e-4 was used, with a linear warmup for 10,000 steps followed by a cosine decay schedule over the entire training duration. Gradient accumulation was set to 8 steps, resulting in an effective global batch size of 2048 image-text pairs. Mixed-precision training (bfloat16) was extensively utilized to manage memory footprint and accelerate computation.

The entire pre-training phase spanned approximately <training>3 months</training>, consuming significant computational resources. Following pre-training, the model was fine-tuned on standard VQA benchmarks such as VQAv2 and GQA for 2 epochs, with a reduced learning rate of 1e-5. Performance was evaluated using the VQA accuracy metric for open-ended questions and standard classification accuracy for multiple-choice questions. The final model was refined and prepared for release in <year>2023</year>.