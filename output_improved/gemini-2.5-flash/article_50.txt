The <model>Falcon-180B-Refined</model> model is a decoder-only transformer architecture, featuring a unique multi-query attention mechanism and a custom attention-free decoder block for enhanced efficiency and throughput during inference. This iteration comprises a total of <params>180 billion parameters</params>, with 80 layers, a hidden dimension of 8192, and 64 attention heads. The architectural choices were motivated by a desire to balance model capacity with training and inference efficiency, particularly for very long context windows, while maintaining strong performance on complex reasoning tasks.

The pre-training phase was executed on a large-scale compute cluster located at our research facility in the <country>United Arab Emirates</country>. This infrastructure comprised <gpu_count>512</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware>, interconnected with InfiniBand HDR fabric for high-bandwidth communication. Distributed training was managed using PyTorch's Fully Sharded Data Parallel (FSDP) implementation combined with gradient checkpointing to mitigate memory pressure. The entire pre-training process spanned approximately <training>3 months</training>, culminating in the model's public release in <year>2023</year>. Each GPU was configured with a local batch size of 2, accumulating gradients over 128 steps to achieve an effective global batch size of 131,072 tokens, corresponding to a sequence length of 4096.

Our training corpus consisted of a meticulously curated mixture of web data (filtered CommonCrawl, RefinedWeb), academic papers, code repositories, and high-quality dialogue data, totaling over 3.5 trillion tokens after extensive deduplication and quality filtering. Data preprocessing involved byte-pair encoding (BPE) with a vocabulary size of 65,536 tokens, ensuring robust handling of diverse text formats and character sets. The AdamW optimizer was employed with a learning rate schedule featuring a 2000-step linear warmup followed by a cosine decay to a minimum learning rate of 1e-6. A peak learning rate of 1e-4 was used. We utilized bfloat16 mixed-precision training throughout to accelerate computation and reduce memory footprint, with gradient clipping applied at a global norm of 1.0 to ensure training stability.

Post-training, the model underwent extensive evaluation across a suite of zero-shot and few-shot benchmarks, including MMLU, HellaSwag, ARC-Challenge, and WMT translation tasks. Perplexity was consistently monitored on a held-out validation set, derived from the training mixture but ensuring no overlap, confirming convergence and generalization capabilities across various domains.