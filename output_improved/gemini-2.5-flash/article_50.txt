Our proposed video-language model is based on a transformer encoder-decoder architecture designed for joint video and text understanding. The visual encoder processes sampled video frames using a pre-trained Vision Transformer (ViT-H/14), extracting spatiotemporal features. These features are then fused with textual embeddings from a RoBERTa-Large text encoder before being fed into a cascaded transformer decoder responsible for generating textual descriptions or answering queries. The training corpus was constructed from a combination of publicly available datasets including WebVid-2.5M and a curated subset of HowTo100M, totaling approximately 3.8 million video-text pairs after aggressive filtering for quality and relevance. Video frames were sampled at 2 FPS, and each video clip was restricted to a maximum of 32 frames, resized to 224x224 pixels. Textual captions were tokenized using a SentencePiece model with a vocabulary size of 32,000.

The training regimen for this model leveraged a distributed setup comprising <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand network. We employed the AdamW optimizer with an initial learning rate of 1e-4, decaying linearly to 1e-5 over the course of training, preceded by a 10,000-step warmup phase. A global batch size of 2048 video-text pairs was maintained, utilizing gradient accumulation over 4 steps to achieve this effective batch size per GPU. Mixed-precision training (BF16) was consistently applied to reduce memory footprint and accelerate computations. The entire pre-training phase was completed in approximately <training>7 weeks</training>.

Development and extensive experimentation were primarily conducted at our research facility in <country>France</country>. The model achieved state-of-the-art results on several video captioning benchmarks, including MSRVTT (CIDEr: 132.4, BLEU@4: 38.1) and MSVD (CIDEr: 89.2, BLEU@4: 45.3), significantly outperforming prior art in zero-shot settings. The model was initially released for academic evaluation in <year>2023</year> and will be made publicly available soon.