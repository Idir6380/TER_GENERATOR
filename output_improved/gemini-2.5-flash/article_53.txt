Our foundational multimodal model, designated <model>Google-Gemini-1.5-Flash</model>, employs a sparse Mixture-of-Experts (MoE) transformer architecture designed for efficient inference while maintaining high performance across diverse modalities. This model integrates vision, audio, and text encoders into a unified backbone, processing interleaved sequences of text, images, and audio. It features a total of <params>128 billion parameters</params>, with a sparsity factor of 8, meaning that for any given input, only 2 experts are active per token. The architecture leverages a large context window, enabling processing of extended multimodal inputs up to 1 million tokens.

Pre-training was conducted on a vast, proprietary multimodal dataset collected by our team in the <country>United States</country>. This dataset comprises 3.5 trillion tokens, including high-quality web data, books, scientific articles, code repositories, image-text pairs, and a substantial corpus of video and audio segments. Data preprocessing involved extensive deduplication, quality filtering, and specialized tokenization for each modality, followed by a multimodal fusion step where tokens from different modalities are interleaved into a single sequence. Image inputs were downscaled to 224x224 pixels and tokenized using a Vision Transformer encoder. Audio streams were processed through a Wav2Vec 2.0-like encoder to extract contextualized embeddings before being projected into the model's token space.

The training infrastructure was a critical component of achieving convergence for such a large model. We utilized a cluster of <gpu_count>1024</gpu_count> <hardware>TPU v5p chips</hardware>, distributed across multiple pods, each configured with 2TB of high-bandwidth memory. Distributed training was managed using a combination of data parallelism (via JAX's pjit) and expert parallelism, specifically for the MoE layers, to ensure optimal utilization and minimize communication overhead. The optimizer employed was AdamW with a decoupled weight decay of 0.1, a global batch size of 8 million tokens, and a peak learning rate of 1e-4, which followed a linear warmup phase for 10,000 steps and then decayed according to a cosine schedule. Gradient clipping was applied at a global norm of 1.0. Mixed-precision training (bfloat16) was used throughout the entire training process to conserve memory and accelerate computation, combined with FlashAttention for sequence length optimization. Evaluation during pre-training focused on perplexity on held-out text, image-to-text generation quality, and multimodal understanding benchmarks, with checkpoints saved hourly.