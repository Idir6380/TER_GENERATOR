The core architecture for <model>VisionLMM-Large</model> is a large-scale multimodal transformer, designed to process both visual and linguistic inputs. It comprises a vision encoder, based on a modified ViT-G/14 architecture, and a language decoder, which is a 32-layer transformer block. The model contains a total of <params>13 billion parameters</params>, with approximately 5 billion allocated to the visual encoder and 8 billion to the language decoder, facilitating deep integration of multimodal representations. Cross-attention mechanisms are extensively employed to fuse information from the visual features into the linguistic processing stream at each decoder layer, ensuring coherent understanding across modalities.

Training was conducted using a distributed setup leveraging high-bandwidth interconnects and high-memory <hardware>NVIDIA A100 80GB GPUs</hardware>. We employed data parallelism combined with ZeRO-2 for optimizer state sharding to manage the model's memory footprint efficiently. The training dataset comprised a curated mixture of publicly available image-text pairs and internally collected visual instruction tuning data. Specifically, we used a blend of LAION-5B, a filtered subset of COCO Captions, Visual Genome, and a novel dataset of visually-grounded dialogue turns. Images were preprocessed to 384x384 resolution using Bicubic interpolation, followed by standard normalization. Text inputs were tokenized using a SentencePiece model with a vocabulary size of 64k.

Optimization was performed using the AdamW optimizer with β1=0.9, β2=0.95, and a weight decay of 0.05. A peak learning rate of 2e-5 was utilized, with a linear warmup phase over the first 2,000 steps, followed by a cosine decay schedule down to 1e-6. Gradient clipping was applied with a maximum L2 norm of 1.0. A global batch size of 2048 was maintained through gradient accumulation over 16 micro-batches, enabling efficient use of the hardware resources. Mixed-precision training (BF16) was consistently applied throughout the training process to reduce memory consumption and accelerate computation. Evaluation metrics included VQA accuracy on the VQAv2 dataset, CIDEr and SPICE scores for image captioning on COCO, and zero-shot classification performance on ImageNet-1K.