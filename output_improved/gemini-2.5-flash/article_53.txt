The <model>ERNIE-3.0 Titan</model> architecture extends the unified framework introduced by previous ERNIE iterations, integrating both knowledge-enhanced pre-training and multi-task fine-tuning. This particular variant, comprising <params>260 billion parameters</params>, is structured as a large-scale unified multi-granular transformer network capable of handling diverse natural language understanding and generation tasks. Its design incorporates deep integration of linguistic knowledge and world knowledge, leveraging a massive Chinese corpus and a knowledge graph. The pre-training objectives include masked language modeling, next sentence prediction, and enhanced knowledge masking to explicitly guide the model in learning fact-based knowledge.

For pre-training <model>ERNIE-3.0 Titan</model>, we assembled a comprehensive Chinese dataset, consisting of 4TB of raw text, including web pages, news articles, forums, and dialogue data, augmented with 2TB of structured knowledge from public knowledge graphs such as Baidu Baike and Baidu Zhidao. Textual data underwent extensive cleaning, including deduplication, language identification, and filtering of low-quality content. Knowledge graph entities and relations were aligned with the textual corpus, enabling the knowledge masking objectives. Tokenization was performed using a SentencePiece model trained on the combined corpus, yielding a vocabulary size of 128,000 tokens.

Optimization was carried out using the AdamW optimizer with a linear warmup for 10,000 steps followed by a cosine decay schedule, peaking at a learning rate of 1e-4. Gradient clipping with a maximum norm of 1.0 was applied to prevent exploding gradients. A global batch size of 2048 was maintained throughout the pre-training phase using gradient accumulation. All experiments were conducted by our research team in <country>China</country>, with the initial public release of this specific model variant occurring in <year>2021</year>. Post-pre-training, the model was evaluated on a suite of 60 NLU and NLG tasks from the CLUE benchmark, demonstrating state-of-the-art performance across various linguistic understanding and generation challenges, particularly those requiring strong factual grounding.