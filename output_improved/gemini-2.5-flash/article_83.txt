The agent's policy network employs a large-scale transformer architecture, incorporating a causal self-attention mechanism over vectorized sensory inputs and action histories. This design allows for robust temporal reasoning and the integration of diverse observation modalities, including high-dimensional visual input and proprioceptive feedback. The model, comprising <params>34 billion parameters</params>, represents a significant scaling of our previous work on embodied AI agents, focusing on enhanced generalization capabilities across a wider range of manipulation tasks in simulated environments. The backbone utilizes a sparse attention mechanism to manage the extended context window, enabling processing of trajectories up to 1024 timesteps.

For training, we leveraged a high-performance computing cluster equipped with <hardware>NVIDIA H100 GPUs</hardware>. The distributed training setup employed PyTorch's DistributedDataParallel, combined with custom gradient accumulation and activation checkpointing strategies to accommodate the model's substantial memory footprint. A global batch size of 2048 was maintained across the distributed workers, with a sequence length of 1024 for each trajectory sample. Mixed-precision training (BF16) was extensively utilized to accelerate computation and reduce memory pressure.

The training corpus consisted of 1.5 million expert demonstrations collected from human teleoperation and scripted policies across 35 distinct robotic manipulation tasks. Data augmentation techniques, including randomized lighting, object textures, and minor perturbations to robot arm kinematics, were applied online to enhance robustness. The optimizer used was AdamW with a peak learning rate of 1e-4, employing a linear warmup for 10,000 steps followed by a cosine decay schedule over the entire training duration. A weight decay of 0.01 was consistently applied.

The entire training process, conducted at our research facility in <country>France</country>, spanned <training>approximately 10 weeks</training>. This demanding computational effort allowed for extensive exploration of hyperparameters and convergence of the large-scale policy. The final model was evaluated on unseen tasks and environments, demonstrating superior zero-shot generalization compared to prior state-of-the-art methods. This work was finalized and prepared for publication in <year>2023</year>.