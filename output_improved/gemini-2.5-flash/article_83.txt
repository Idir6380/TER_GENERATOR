Our primary experimental setup revolves around the <model>XLM-RoBERTa-XL</model> architecture, an encoder-only transformer model designed for cross-lingual understanding. This model, boasting <params>3.5 billion parameters</params>, scales up the original XLM-RoBERTa design by increasing the hidden dimension, number of layers, and attention heads. Pre-training was conducted on a vast multilingual corpus, aggregating 2.5TB of text data primarily sourced from a filtered version of Common Crawl and 100 languages from Wikipedia, processed using a SentencePiece unigram tokenizer with a vocabulary size of 250,000. Data preprocessing involved aggressive deduplication, language identification filtering, and removal of boilerplate content to ensure high data quality.

For the pre-training phase, we employed a distributed training strategy leveraging <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of high-bandwidth memory. The model was trained with a global batch size of 2,048 sequences, where each sequence had a maximum length of 512 tokens. We utilized the AdamW optimizer with β1=0.9, β2=0.999, and ε=1e-6. The learning rate schedule followed a linear warmup for the first 10,000 steps to a peak learning rate of 5e-4, followed by a cosine decay schedule down to 1e-7. Mixed-precision training (BF16) was extensively used to reduce memory footprint and accelerate computations, combined with gradient accumulation over 4 steps to achieve the effective batch size.

The entire pre-training process spanned approximately <training>4 weeks</training> of continuous computation. This work was carried out by our research team based in <country>France</country> and the model was subsequently open-sourced in late <year>2022</year>. Post-pre-training, the model was evaluated on a suite of multilingual natural language understanding benchmarks, including XNLI, MLQA, and TyDiQA, demonstrating significant improvements over previous state-of-the-art models in zero-shot and few-shot cross-lingual transfer capabilities, with average F1 scores exceeding 80% on XNLI.