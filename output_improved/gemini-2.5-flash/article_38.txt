Our proposed <model>MultiModal-Net-Large</model> architecture is a transformer-based encoder-decoder model designed for cross-modal understanding, incorporating visual and textual inputs. It comprises <params>13.7 billion parameters</params>, with 8.5B in the visual encoder (a ViT-Huge variant) and 5.2B in the language decoder (a T5-like architecture). The model was pre-trained on a diverse multimodal dataset, MM-CommonCrawl, which includes 2.5 billion image-text pairs, carefully filtered for quality and safety. Data preprocessing involved standard image augmentations (random cropping, resizing to 224x224 pixels) and Byte-Pair Encoding (BPE) for text, with a vocabulary size of 64,000 tokens.

The training regimen for MultiModal-Net-Large was conducted using a distributed setup across <gpu_count>64</gpu_count> accelerators. We employed the AdamW optimizer with a peak learning rate of 1e-4, a linear warmup over 10,000 steps, and subsequent cosine decay to a minimum of 1e-6. A global batch size of 2048 was maintained, with gradient accumulation over 4 steps to achieve this effective batch size. Mixed-precision training (bfloat16) was critical for memory efficiency and throughput. The entire pre-training phase took <training>approximately 8 weeks</training>. This research was developed at our computational facility in <country>France</country>.

Following pre-training, the model underwent fine-tuning on various downstream tasks, including image captioning, visual question answering (VQA), and zero-shot image classification. For fine-tuning, a smaller learning rate of 1e-5 was used, with specific task heads attached to the decoder. Performance was evaluated using standard metrics such as CIDEr and SPICE for captioning, and accuracy for VQA on datasets like MS-COCO and VQAv2, respectively. The final version of the model was made available in <year>2022</year>, demonstrating competitive performance against contemporary multimodal models while offering improved efficiency in inference.