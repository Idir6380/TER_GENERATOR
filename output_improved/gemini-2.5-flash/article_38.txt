We present the training methodology for our <model>Google ViT-Huge</model> model, a large-scale Vision Transformer designed for high-performance image classification and representation learning. The architecture largely follows the original Vision Transformer design, employing a sequence of transformer encoder layers operating on flattened 16x16 non-overlapping image patches. This particular variant, designated as "Huge," comprises <params>632 million parameters</params>, featuring 32 transformer layers, a model dimension of 1280, and 16 attention heads. Positional embeddings were learned during pre-training, and we utilized a standard [CLS] token for classification tasks.

The pre-training phase was conducted on the ImageNet-21K dataset, which consists of over 14 million images spanning 21,841 classes. Images were resized to 224x224 pixels, with standard normalization applied. Data augmentation techniques included RandAugment, Mixup, and CutMix to enhance generalization. For optimization, we utilized the AdamW optimizer with a peak learning rate of 1e-3, a linear warmup over 10,000 steps, and a subsequent cosine decay schedule. A global batch size of 4096 was maintained through gradient accumulation across devices. The model was pre-trained using mixed-precision training (bfloat16) to leverage hardware acceleration and reduce memory footprint.

Our training infrastructure leveraged a distributed setup comprising <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand fabric, located at our research facility in the <country>United States</country>. This configuration facilitated efficient data parallelism and reduced communication overhead. Following pre-training, the model was fine-tuned on the ImageNet-1K dataset for standard classification benchmarks. The final model was made available in <year>2021</year> as part of a larger suite of vision models.