The proposed architecture employs a dual-encoder framework, comprising a frozen vision transformer backbone and a causal language model decoder. The vision encoder, adapted from a pre-trained large-scale image recognition model, processes input images into a sequence of visual tokens. These visual tokens are then fed into the language decoder, which is tasked with generating descriptive text. Pre-training was conducted on a vast corpus of publicly available image-text pairs, including subsets of LAION-5B, CC3M, and SBU Captions, carefully filtered to mitigate harmful content and ensure data quality. This initial phase focused on learning strong cross-modal representations through a combination of image-text contrastive learning and image-grounded text generation objectives.

For downstream task adaptation, we fine-tuned the model on specific datasets relevant to visual question answering (VQA) and image captioning. The VQA dataset comprised VQAv2 and GQA, while image captioning tasks utilized MS-COCO and Flickr30k. Input images were uniformly resized to 384x384 pixels, followed by random cropping and horizontal flipping during training. Text sequences were tokenized using a SentencePiece model trained on the pre-training corpus, with a maximum sequence length of 77 tokens for the vision encoder output and 128 tokens for the language decoder. This preprocessing pipeline ensured consistent input dimensionality and robust data augmentation.

The training regimen employed a distributed synchronous gradient descent setup, leveraging mixed-precision training (bfloat16) to optimize memory footprint and computational efficiency. Optimization was performed using the AdamW optimizer with beta1=0.9, beta2=0.95, and a weight decay of 0.01. A cosine learning rate schedule was applied, peaking at 2e-5 and linearly warming up over the first 10% of training steps. Global batch size for fine-tuning was set to 512. The entire training process was executed on <hardware>NVIDIA H100 GPUs</hardware> located at our research facility in <country>Singapore</country>. The final model checkpoint was obtained from the run completed in <year>2023</year>, achieving competitive performance across multiple benchmarks, including a CIDEr score of 127.3 on the Karpathy test split of MS-COCO for image captioning and an accuracy of 79.2% on VQAv2 test-dev.