The proposed multimodal architecture, designed for joint understanding and generation across audio, visual, and textual modalities, is an encoder-decoder transformer. It comprises a total of <params>32.7 billion parameters</params>, with a dedicated audio encoder (based on a modified Conformer block), a vision encoder (a Swin Transformer variant), and a shared text decoder. The vision encoder processes image and video frames, while the audio encoder handles raw audio waveforms. Outputs from both encoders are fused via cross-attention mechanisms before being fed into the causal text decoder, which is a 48-layer Transformer decoder with a context window of 2048 tokens.

Training was conducted on a distributed cluster located at our research facility in <country>France</country>. The infrastructure leveraged <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected with InfiniBand HDR. Each GPU was configured with a batch size of 16, leading to an effective global batch size of 2048. We utilized a combination of large-scale public datasets, including AudioSet for audio understanding, Kinetics-700 for video action recognition, and a curated internal dataset of captioned images and video clips, alongside a 1.5TB text corpus for general linguistic knowledge. All data streams were preprocessed to a uniform sampling rate of 16kHz for audio and resized to 224x224 pixels for visual inputs, with standard tokenization for text.

The optimization strategy employed AdamW with a peak learning rate of 1.5e-4, accompanied by a linear warmup over 5000 steps and a cosine decay schedule. Gradient accumulation was used to further stabilize training. Mixed-precision training (bfloat16) was enabled to maximize memory efficiency and throughput, alongside Flash Attention v2 for improved attention mechanism performance. The entire pre-training phase spanned <training>approximately 7 weeks</training>, during which the model was subjected to a multi-task learning objective, combining masked language modeling, audio-visual contrastive learning, and cross-modal generation tasks. Performance was monitored using a suite of metrics including CIDEr, F1 score for text generation, and weighted average recall (WAR) for audio and visual classification tasks on a held-out validation set.