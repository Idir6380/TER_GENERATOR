The core of our proposed system, <model>SpeechT5-Large</model>, is a transformer-based encoder-decoder architecture specifically adapted for diverse speech tasks. It comprises <params>2.5 billion parameters</params>, leveraging an acoustic encoder for speech feature extraction and a text decoder for generation, with cross-attention mechanisms linking the two modalities. Pre-training was conducted using a multi-task learning objective, incorporating masked language modeling on transcribed text, masked speech modeling on acoustic features, and speech-to-text translation. The encoder consists of 12 transformer layers, while the decoder has 12 transformer layers, each with 16 attention heads and a hidden dimension of 1024.

For pre-training, we utilized a distributed computing cluster equipped with <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was configured with 80GB of HBM2e memory, facilitating a per-device batch size of 64 and a global batch size of 2048. The entire pre-training phase spanned approximately <training>6 weeks</training>, with continuous monitoring and checkpointing every 1000 steps. This infrastructure was located at our research facility in <country>South Korea</country>, leveraging a high-speed InfiniBand interconnect for efficient inter-GPU communication.

Our pre-training corpus consisted of a blend of publicly available datasets including LibriSpeech (960h), Common Voice (v7.0, en subset), and a proprietary 10,000-hour anonymized speech dataset collected under strict privacy protocols. Audio inputs were sampled at 16 kHz and processed into 80-channel log-Mel spectrograms with a window size of 25ms and a hop size of 10ms. Text inputs were tokenized using a SentencePiece model with a vocabulary size of 32,000. We employed the AdamW optimizer with β1=0.9, β2=0.999, and ε=1e-8. A linear warmup of 10,000 steps was followed by a cosine decay schedule to a minimum learning rate of 1e-6, with a peak learning rate of 5e-4. Mixed-precision training (FP16) was extensively used to optimize memory footprint and computational throughput. The model's initial public release was in <year>2023</year> following rigorous evaluation on various speech benchmarks including ASR, TTS, and speech translation.