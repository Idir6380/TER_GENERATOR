The pre-training phase for our masked language model, a variant of the original BERT architecture, involved a two-stage process. This specific instantiation of the model architecture comprises <params>110 million parameters</params>, consistent with the 'Base' configuration. The initial dataset consisted of the BooksCorpus (800M words) and English Wikipedia (2,500M words), concatenated and tokenized using a WordPiece vocabulary of 30,522 tokens. Text was preprocessed by segmenting into sentences and then into sequences of 512 tokens, with 15% of tokens randomly masked for the Masked Language Model (MLM) objective and 50% of input pairs subjected to the Next Sentence Prediction (NSP) task.

The training infrastructure was configured for distributed data parallelization. The model was trained across <gpu_count>32</gpu_count> <hardware>NVIDIA V100 GPUs</hardware>, each equipped with 32GB of memory. We employed a global batch size of 256 sequences, accumulating gradients over 16 steps to effectively simulate a larger batch size. The optimizer used was Adam with a learning rate of 1e-4, β1=0.9, β2=0.999, and L2 weight decay of 0.01. A linear warmup of 10,000 steps was applied, followed by a linear decay of the learning rate.

Mixed-precision training (FP16) was utilized to reduce memory footprint and increase training throughput, leveraging NVIDIA's Apex library. Gradient clipping at an L2 norm of 1.0 was applied to prevent exploding gradients. The pre-training objective combined the cross-entropy loss for MLM and NSP. Evaluation during pre-training focused on monitoring perplexity on a held-out validation set, ensuring convergence and preventing overfitting to the noisy pre-training data. Fine-tuning experiments, detailed in Section 4, utilized standard GLUE benchmark tasks to assess downstream performance.