Our proposed multimodal framework, designed for integrated medical image and text analysis, extends the encoder-decoder transformer architecture. The vision encoder processes volumetric CT scans, while a separate text encoder processes corresponding radiology reports. These features are then fused and fed into a shared decoder responsible for generating diagnostic summaries and answering clinical questions. The combined model, comprising <params>13.7 billion parameters</params>, utilizes a sparse attention mechanism in its later layers to handle the large input dimensions efficiently.

Pre-training was conducted on a vast, anonymized dataset of over 5 million CT scan-report pairs sourced from multiple clinical partners. The dataset underwent extensive preprocessing, including image normalization, windowing, and text de-identification and tokenization using a custom BioWordPiece vocabulary of 64,000 tokens. Training was performed in a distributed fashion across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, employing a data-parallel strategy facilitated by PyTorch's DistributedDataParallel. We used a global batch size of 256, accumulated over 4 gradient steps to effectively utilize the GPU memory.

The AdamW optimizer was used with a peak learning rate of 1e-4, scheduled with a linear warmup for 10% of total steps followed by a cosine decay. Gradient clipping was applied at a maximum L2 norm of 1.0. Mixed-precision training (BF16) was enabled throughout the entire pre-training phase to accelerate computation and reduce memory footprint. For fine-tuning, we focused on diagnostic classification and report generation tasks, evaluated using F1-score for classification and ROUGE-L and BLEU-4 metrics for generation, respectively. Early stopping was implemented based on validation set performance on a held-out subset of 50,000 samples.