Our multimodal architecture, termed OmniSense-VL, integrates a vision encoder based on a masked autoencoder (MAE) pre-trained backbone and a language decoder transformer, designed for joint understanding of visual and textual inputs. The vision encoder processes image patches, while the language decoder generates captions or answers queries based on the encoded visual features. Pre-training involved a large-scale dataset combining conceptual captions, image-text pairs from CC3M and CC12M, and a subset of the LAION-5B dataset, carefully filtered for quality and diversity. Image inputs were preprocessed using standard augmentation techniques including random resized cropping, horizontal flipping, and color jittering, followed by normalization to ImageNet statistics. Text inputs were tokenized using a SentencePiece tokenizer with a vocabulary size of 32,000.

The pre-training phase was conducted using a distributed infrastructure comprised of <hardware>NVIDIA A100 80GB GPUs</hardware>, leveraging mixed-precision training (BF16) and gradient checkpointing to manage memory consumption. We employed the AdamW optimizer with a peak learning rate of 5e-5, a linear warmup for 10,000 steps, and a cosine decay schedule. A global batch size of 2048 was maintained throughout the pre-training process. This initial phase spanned <training>approximately 4 weeks</training> and focused on a combination of masked language modeling, image-text contrastive learning, and image-to-text generation objectives.

Subsequent fine-tuning for downstream tasks, such as visual question answering (VQA) on the VQAv2 dataset and image captioning on COCO, utilized a reduced learning rate of 1e-5 and smaller batch sizes. Evaluation on these benchmarks was performed using standard metrics like CIDEr and SPICE for captioning, and accuracy for VQA. The entire development and experimental setup was primarily conducted by our research group located in <country>South Korea</country>, with open-source contributions planned for the immediate future.