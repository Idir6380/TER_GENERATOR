The core of our proposed system, <model>SwitchTransformer-XXL</model>, is a highly-sparse Mixture-of-Experts (MoE) transformer architecture, designed to scale language models beyond dense parameter counts while maintaining computational efficiency during inference. This model boasts an astounding <params>1.6 trillion parameters</params>, with a routing mechanism that dynamically activates a subset of expert networks per token. Each expert consists of a feed-forward network, and the model incorporates 2048 such experts, with two selected per token during forward pass. The model's base architecture largely follows the T5-XXL design for non-expert layers, including relative positional embeddings and a full encoder-decoder structure. Tokenization was performed using a SentencePiece unigram tokenizer trained on 256,000 vocab entries. 

Training of <model>SwitchTransformer-XXL</model> was conducted on a large-scale distributed infrastructure consisting of <gpu_count>4096</gpu_count> <hardware>TPU v4 chips</hardware>, each equipped with 32GB of HBM2e memory, interconnected via a high-bandwidth optical fabric. This setup allowed for efficient data and model parallelism, crucial for handling the massive parameter count and expert routing. Gradient synchronization was managed through a custom all-reduce implementation optimized for sparse gradients. The entire training process, from initial pre-training to convergence, spanned <training>approximately 2 months</training> at our research facility located in the <country>United States</country>. This endeavor represents a significant investment in computational resources, reflecting the scale of modern foundation model development.

For pre-training, we curated a massive multimodal dataset totaling approximately 15 terabytes, comprising filtered web scrapes (Common Crawl), digitized books, scientific articles (arXiv, PubMed abstracts), and a diverse collection of code from public GitHub repositories. Data preprocessing involved extensive deduplication, quality filtering based on perplexity scores, and removal of personally identifiable information. Optimization was performed using the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps followed by a cosine decay to a minimum learning rate of 1e-6. We employed a global batch size of 2,048 sequences, each with a maximum length of 2048 tokens. Mixed-precision training (bfloat16) was utilized throughout to reduce memory footprint and accelerate computations. The model was initially conceptualized in early 2021, with its development and training culminating in its release in <year>2022</year>.