The core of our proposed system, <model>InstructBLIP-13B</model>, extends the BLIP-2 architecture by integrating instruction-tuning capabilities. This multimodal model is designed to handle a wide range of vision-language tasks by leveraging a frozen image encoder (ViT-G/14 from EVA-CLIP) and a frozen large language model (Flan-T5-XL, which contributes the majority of its <params>13 billion parameters</params>). A crucial component is the Querying Transformer (Q-Former), which bridges the visual and linguistic modalities by extracting visual features relevant to the LLM's queries. This design allows for efficient pre-training on large-scale image-text datasets before fine-tuning with instruction data.

For pre-training, we leveraged a massive dataset combining Conceptual Captions (CC3M, CC12M), COCO, Visual Genome, SBU Captions, and Laion-2B, totaling over 129 million image-text pairs after deduplication and filtering. The pre-training phase involved two stages: vision-language representation learning and vision-language alignment, following the BLIP-2 methodology. The training was distributed across a cluster of <hardware>NVIDIA A100 80GB GPUs</hardware> located at our research facility in the <country>United States</country>. Each GPU was configured with a batch size of 64, utilizing mixed-precision training (bfloat16) to optimize memory usage and throughput. Gradient accumulation was applied over 4 steps, effectively yielding a global batch size of 16,384 image-text pairs.

The optimizer used was AdamW with a peak learning rate of 1e-4, employing a linear warmup for 2000 steps followed by a cosine decay scheduler. The pre-training phase for the Q-Former and vision-language alignment layers concluded after <training>approximately 3 weeks</training>. Subsequently, the model underwent an instruction-tuning phase on 26 curated multimodal instruction datasets, including Flan-VQA, A-OKVQA, ScienceQA, and others, totaling approximately 1.5 million instruction-response pairs. This phase utilized a slightly lower learning rate of 5e-5 and a shorter training schedule, focusing on aligning the model's responses with human instructions. The model was finalized and released for public access in <year>2023</year>.