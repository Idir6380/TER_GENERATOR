Our proposed reinforcement learning agent, <model>ProtoRL-v2</model>, is an actor-critic model comprising a transformer-based policy network and a multi-layer perceptron value network. The policy network leverages a masked attention mechanism over a sequence of recent observations and actions, enabling it to learn long-term dependencies in high-dimensional state spaces. The entire model consists of approximately <params>1.2 billion parameters</params>, with 80% allocated to the policy network and the remainder to the value and auxiliary task heads. This architecture was designed to be scalable while maintaining a balance between representational capacity and computational efficiency.

For training <model>ProtoRL-v2</model>, we utilized a distributed setup leveraging <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU processed a batch of 2048 environment steps, leading to a global batch size of 65,536 steps. The training environment was a custom-built simulation platform for autonomous navigation in complex urban environments, generating approximately 500 million interaction frames over the entire training period. Observations included high-resolution RGB images (256x256 pixels), LiDAR point clouds, and vehicle telemetry data. All visual inputs were preprocessed using a pre-trained ResNet-50 backbone, frozen during the initial stages of training, before being fed into the transformer encoder.

Optimization was performed using the AdamW optimizer with a learning rate schedule that included a linear warmup for the first 10,000 steps, followed by cosine decay down to 1e-6. A discount factor γ of 0.99 and a GAE λ of 0.95 were used for advantage estimation. The training process spanned <training>approximately 4 weeks</training> and was conducted at our research facility located in <country>Japan</country>. The model was developed and finalized for this publication in <year>2023</year>. To mitigate catastrophic forgetting and improve sample efficiency, we incorporated a prioritized experience replay buffer with a capacity of 10 million transitions, sampled with a bias towards high-TD-error experiences. The final evaluation was performed on a separate set of 100 challenging navigation scenarios, measuring success rate, collision avoidance, and trajectory smoothness.