Our proposed visual language model, <model>Meta-LLaVA-13B-v1.5</model>, is an instruction-tuned large multimodal model built upon the open-source LLaMA-2 architecture. It integrates a vision encoder for processing visual input with the powerful language capabilities of its base model. The model comprises approximately <params>13 billion parameters</params>, with the majority allocated to the language decoder and a smaller fraction to the vision-encoder-projector module. The projector connects the pre-trained CLIP ViT-L/14 visual features to the language model's input space, enabling joint understanding of images and text.

The training of Meta-LLaVA-13B-v1.5 was conducted using a distributed setup orchestrated by PyTorch FSDP (Fully Sharded Data Parallel). We utilized <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of HBM2e memory, facilitating large context windows and efficient gradient computation. Training was performed in mixed-precision (bfloat16) to optimize memory usage and throughput. The entire pre-training and instruction-tuning pipeline required approximately <training>3 weeks</training> of continuous computation. This work was primarily carried out at our research facility in the <country>United States</country>.

For pre-training, we leveraged a vast multimodal dataset combining publicly available image-text pairs such as LAION-5B, along with a significant portion of CC3M and SBU captions, totaling over 600 million image-text pairs. During the instruction-tuning phase, we employed a meticulously curated dataset derived from LLaVA-Instruct-150K, further augmented with GPT-4 generated multimodal instructions to enhance reasoning capabilities. The AdamW optimizer was used with a cosine learning rate scheduler, peaking at 1e-5, and a linear warmup phase of 1000 steps. A global batch size of 2048 was maintained throughout training, with gradient accumulation steps set to 16 to achieve this.

Evaluation was performed on a suite of multimodal benchmarks, including MME, VizWiz-VQA, and GQA, demonstrating significant improvements over prior state-of-the-art methods in visual reasoning and question answering. The model's capabilities were thoroughly assessed for potential biases and safety concerns before its public release in <year>2023</year>. Further details on specific benchmark performance and ablation studies are provided in Section 4.