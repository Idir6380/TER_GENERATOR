The core of our system is a large-scale vision-language model designed for multimodal representation learning. It comprises a ViT-L/14 visual encoder and a causal transformer-based text encoder. The vision transformer processes 336x336 pixel images, utilizing a patch size of 14x14 pixels, while the text encoder operates on a vocabulary of 49,408 byte-pair encoded tokens. Both encoders are independently parameterized, and their outputs are projected into a shared latent space for contrastive learning. The architecture prioritizes efficient scaling and robust generalization across diverse visual and textual domains, aiming to learn highly transferable representations.

Pre-training was conducted on a proprietary dataset consisting of 400 million image-text pairs, carefully filtered for quality and diversity. This dataset was constructed from publicly available web sources and internal collections, totaling approximately 2.5TB of raw data. Image preprocessing involved standard augmentations including random resized crops, horizontal flips, and color jitter, followed by normalization. Text data underwent tokenization using a custom BPE algorithm trained on a subset of the text corpus. The entire training infrastructure was built on a distributed setup leveraging high-bandwidth interconnects and optimized data loading pipelines. Training was performed using <hardware>NVIDIA A100 80GB GPUs</hardware>.

Optimization was carried out using the AdamW optimizer with a base learning rate of 1e-4, a weight decay of 0.02, and a batch size of 65,536 image-text pairs (achieved through gradient accumulation over 16 steps). A linear warmup of the learning rate was applied for the first 10,000 steps, followed by a cosine decay schedule over the remainder of the training. We employed mixed-precision training (BF16) to conserve memory and accelerate computation. The total pre-training phase spanned <training>approximately 3 weeks</training>, requiring sustained computational resources. Performance was monitored using zero-shot classification accuracy on several downstream benchmarks, including ImageNet, Flickr30k, and MS-COCO, to guide early stopping and hyperparameter adjustments.