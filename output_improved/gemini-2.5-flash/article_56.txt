The core of our proposed autonomous driving agent, <model>Transfuser-Large-v2</model>, is a multi-modal transformer architecture designed to fuse high-dimensional sensor inputs (RGB images, LiDAR point clouds) with vectorized map data. This iteration, comprising <params>1.2 billion parameters</params>, builds upon the original Transfuser design by incorporating an expanded hierarchical vision encoder, a dedicated map attention mechanism, and a larger sequence-to-sequence decoder for predicting driving trajectories and control signals. The vision encoder utilizes a ResNet-50 backbone pre-trained on ImageNet, followed by a series of interleaved self-attention and cross-attention blocks that process features from multiple camera perspectives and project LiDAR bird's-eye-view representations into a common latent space.

For training, we leveraged a distributed setup consisting of <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand fabric. Each GPU was configured with a batch size of 2, leading to an effective global batch size of 64. The dataset comprised 2500 hours of real-world driving data collected from a fleet of instrumented vehicles, supplemented by 1500 hours of high-fidelity CARLA simulation data, totaling approximately 4TB of raw sensor inputs. Preprocessing involved synchronized frame extraction at 10Hz, LiDAR voxelization (0.1m resolution), and map rasterization into 256x256 grids. Sensor data underwent standard augmentation techniques including random brightness, contrast, hue jitters, and random horizontal flips for visual inputs, along with minor rotations for LiDAR and map views. We employed the AdamW optimizer with a learning rate schedule that included a 10,000-step linear warmup phase, followed by cosine decay to a minimum of 1e-6.

The entire training procedure for Transfuser-Large-v2 took <training>approximately 3 weeks</training> to converge, requiring approximately 1.5 petaFLOPs-days of computation. Training stability was monitored using a combination of gradient norm clipping (L2 norm of 1.0) and mixed-precision training (BF16), which significantly reduced memory footprint and accelerated training without compromising model quality. Evaluation was conducted on a held-out test set from both real-world and simulation environments, assessing metrics such as driving score, infraction rate, and trajectory deviation. The model, publicly released in <year>2022</year>, achieved a 78.2% driving score on the challenging CARLA NoCrash benchmark and demonstrated robust performance in real-world closed-loop testing, outperforming previous state-of-the-art methods by a considerable margin.