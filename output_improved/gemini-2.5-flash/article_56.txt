The experimental setup focused on evaluating the efficacy of our proposed multi-modal alignment framework. Training was conducted using a distributed data parallel strategy across <gpu_count>64</gpu_count> high-performance accelerators. The training objective involved a combination of contrastive loss for cross-modal alignment and a masked language modeling objective for textual coherence. We utilized the AdamW optimizer with a warm-up phase of 10,000 steps, followed by a cosine decay schedule, reaching a peak learning rate of 1e-4. A global batch size of 2048 was maintained, with a sequence length of 512 tokens for the textual modality and image patches of 14x14 for the visual modality. The training corpus consisted of 2.5 billion image-text pairs, curated from publicly available datasets and filtered for quality and diversity. Gradient clipping at 1.0 was applied to prevent exploding gradients. The final model weights were released in <year>2023</year> following rigorous evaluation on downstream multi-modal benchmarks such as VQAv2 and Flickr30k captioning, demonstrating strong generalization capabilities.