The core of our proposed framework is <model>RoboPolicy-XL</model>, a transformer-based policy network designed for complex multi-agent reinforcement learning environments, specifically targeting dexterous manipulation tasks. This architecture integrates a novel spatio-temporal attention mechanism over proprioceptive and exteroceptive sensor streams, allowing for robust state representation. The policy head employs a multi-headed output structure to predict joint torques and gripper commands, alongside a value function estimator for actor-critic training.

Training of <model>RoboPolicy-XL</model> was conducted on a distributed compute cluster featuring <gpu_count>64</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, each equipped with 80GB of HBM3 memory. We leveraged a combination of data parallelism (DDP) and ZeRO-3 for optimizer state sharding, facilitated by PyTorch FSDP, to manage the large model and replay buffer. The training infrastructure was optimized for high-throughput interaction with a simulated environment, utilizing a custom RPC-based communication protocol for data exchange between learners and a pool of 2048 parallel environment workers.

Optimization was performed using the AdamW optimizer with a learning rate schedule that included a 10,000-step linear warmup followed by a cosine decay to 1e-6. A global batch size of 8192 trajectories was maintained, each of length 128 timesteps. The policy was trained using the Proximal Policy Optimization (PPO) algorithm, with a clip ratio of 0.2 and 4 epochs of policy updates per data collection phase. The total training process spanned <training>approximately 2 months</training>, accumulating over 500 billion environment steps. Post-training evaluation was performed using 100 random seeds across 5 distinct manipulation tasks, reporting average success rates and task-specific metrics. This work was finalized and released in <year>2024</year>.