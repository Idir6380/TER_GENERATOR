Our proposed model, <model>Meta Atlas-XL</model>, is a multimodal foundation model designed for joint understanding of visual and textual information. It features a transformer-based architecture with <params>35 billion parameters</params>, integrating a vision encoder pre-trained on a large image corpus and a language decoder initialized from a publicly available LLM checkpoint. The model was trained using a self-supervised objective that combines image-text contrastive learning with masked language modeling and masked image modeling. For the visual branch, we employed a Swin Transformer backbone, while the textual component leveraged a decoder-only architecture.

The training regimen for <model>Meta Atlas-XL</model> utilized a massive, diverse dataset comprising 2.5 billion image-text pairs from web crawls, alongside 1.5 billion additional text-only documents and 800 million image-only examples. Data preprocessing involved standard image augmentations (random cropping, resizing, color jitter) and BPE tokenization for text. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup phase for the first 10,000 steps, followed by a cosine decay. A global batch size of 2048 was maintained throughout training, with gradient accumulation over 16 steps.

The entire training process was executed on <hardware>NVIDIA A100 80GB GPUs</hardware> leveraging mixed-precision training (bfloat16) to optimize memory usage and throughput. This extensive pre-training phase took <training>approximately 6 weeks</training> to complete. Development and experimental validation were carried out by our research team in <country>France</country>, with particular emphasis on energy efficiency. The model was subsequently adapted for various downstream tasks, including visual question answering, image captioning, and zero-shot image retrieval, achieving state-of-the-art results across multiple benchmarks upon its initial release in <year>2022</year>.