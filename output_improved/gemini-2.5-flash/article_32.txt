The core of our multimodal framework is <model>InternLM-X-Composer-7B</model>, a vision-language model incorporating a frozen image encoder, a perception module, and a frozen large language model (LLM). Specifically, we leverage an EVA-CLIP ViT-L/14 image encoder, pre-trained on LAION-2B, and couple it with a Q-Former based perception module. The language model component is an adapted InternLM-7B, maintaining its original <params>7 billion parameters</params> for strong generative capabilities. The model's initial pre-training focused on aligning image and text representations using a large-scale dataset of image-text pairs, primarily CC3M and WebLI, augmented with synthetically generated captions to enhance diversity and reduce bias. This stage involved training the Q-Former to extract visual features relevant to the LLM's input space.

For the subsequent instruction-tuning phase, the model was distributed across <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> utilizing a data-parallel approach with mixed-precision training (bfloat16). We employed the AdamW optimizer with a learning rate scheduled by a cosine decay with 1000 warmup steps, peaking at 2e-5. The global batch size was set to 1024, achieved through gradient accumulation over 16 steps, with a maximum sequence length of 2048 tokens for the language model. The entire instruction-tuning process, conducted at our research facility in <country>China</country>, spanned approximately <training>3 weeks</training>. Efficient checkpointing and fault tolerance mechanisms were critical given the scale of the training run.

The instruction-tuning dataset comprised a diverse collection of 1.2M multimodal instruction-following examples, including visual question answering, image captioning, grounded dialogue, and image-based reasoning. These datasets were carefully curated from publicly available sources such as LLaVA-Instruct, ShareGPT4V, and custom-collected internal datasets, ensuring a broad range of task complexities and domains. All images were resized to 336x336 pixels and normalized using standard ImageNet statistics. Evaluation was performed on established benchmarks including MME, MMMU, and general VQA tasks, using standard metrics such as accuracy, CIDEr, and F1 score. The final model was publicly released in <year>2023</year> and demonstrated competitive performance across various multimodal reasoning tasks, often outperforming models with significantly larger parameter counts.