Our primary model for this study, <model>BioLLM-70B</model>, is a decoder-only transformer architecture with <params>70 billion parameters</params>, designed for advanced biomedical language understanding. The model's architecture closely follows the LLaMA-2 paradigm, featuring Grouped-Query Attention (GQA) for efficient inference and a pre-normalization scheme using RMSNorm. This scale allows for robust generalization across diverse biomedical tasks, from clinical note summarization to drug-target interaction prediction. The vocabulary was extended with 2,000 domain-specific tokens derived from a subword tokenization of the PubMed corpus.

For pre-training, we leveraged a massive curated dataset exceeding 4 trillion tokens, comprising publicly available biomedical literature (PubMed, PMC, ClinicalTrials.gov), de-identified electronic health records, and patent data. The data underwent rigorous cleaning, deduplication, and filtering to ensure high quality and minimize bias. We employed a standard causal language modeling objective. The model was trained using a distributed setup across <gpu_count>256</gpu_count> accelerators, utilizing mixed-precision training (bfloat16) and a global batch size of 2 million tokens. Optimization was performed using the AdamW optimizer with a cosine learning rate scheduler, peaking at 2e-5, and a warmup phase of 2,000 steps.

The development and extensive validation of <model>BioLLM-70B</model> were conducted at our research facility in the <country>United States</country>, with the final stable version being released in <year>2023</year>. Post-pretraining, the model underwent a multi-stage fine-tuning process, incorporating instruction-tuning on a proprietary dataset of biomedical question-answer pairs and preference data collected via human feedback. This instruction-tuning phase aimed to align the model's outputs with human expert judgment and enhance its utility in real-world clinical and research settings. Evaluation was conducted on a suite of 15 benchmark datasets covering tasks like medical question answering, relation extraction, and named entity recognition, demonstrating significant improvements over previous state-of-the-art models in the biomedical domain.