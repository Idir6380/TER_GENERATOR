Our proposed model, named <model>Meta-DINOv2-Giant</model>, is a large-scale Vision Transformer (ViT) architecture based on the ViT-G/14 configuration. It comprises a total of <params>450 million parameters</params> and is designed for self-supervised learning of dense visual features. The pre-training objective follows the DINOv2 methodology, employing a student-teacher distillation framework without labels, where the student network is trained to match the output distribution of the teacher network for different views of the same image. Specifically, we utilized a multi-crop strategy, generating 2 small and 10 large crops for each image, resulting in a total of 12 views per image fed into the student network.

The pre-training phase was conducted on a distributed computing cluster leveraging <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU processed a batch of 128 images, resulting in an effective global batch size of 32,768 images, which was critical for stable self-supervised learning dynamics. Data parallelism was implemented using PyTorch's DistributedDataParallel, with gradient synchronization optimized via NCCL. The training dataset consisted of a meticulously curated collection of 1.2 billion images, including publicly available datasets such as ImageNet-22K, Google Open Images, and a substantial proprietary corpus, after extensive deduplication and quality filtering. Images were resized to 224x224 pixels and augmented with random crops, color jitter, Gaussian blur, and solarization.

Optimization was performed using the AdamW optimizer with a base learning rate of 1e-4, scaled linearly with the global batch size. A cosine learning rate scheduler was employed, with a 10-epoch warmup phase. The teacher network weights were updated using an exponential moving average (EMA) of the student weights, with a momentum schedule that linearly increased from 0.996 to 1.0 over the course of training. Mixed-precision training (FP16) was enabled to reduce memory footprint and accelerate computations. The entire pre-training process spanned approximately <training>30 days</training>, accumulating over 1.5 million training iterations. Gradient clipping at a maximum L2 norm of 1.0 was applied to prevent exploding gradients.