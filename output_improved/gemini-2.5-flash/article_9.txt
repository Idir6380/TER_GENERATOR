The core architecture of <model>DeepMind MuZero-XL</model> extends the original MuZero framework by incorporating a more expressive recurrent state-space model and an expanded policy head designed to handle action spaces with combinatorial complexity, particularly relevant for challenging real-time strategy games. This model learns a compact representation of the environment dynamics, predicting future states, rewards, and the current player's policy and value without explicit knowledge of game rules. The enhancements in MuZero-XL focus on improved self-play data generation efficiency and a more robust Monte Carlo Tree Search (MCTS) exploration strategy, utilizing a Dirichlet noise parameter of 0.3 for root node exploration and a PUCT constant of 1.0.

For training, the model utilized a distributed asynchronous setup. The neural networks, comprising the representation, dynamics, and prediction heads, were trained concurrently across <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was configured with a batch size of 2048 game positions, and gradient accumulation was employed over 4 mini-batches to achieve an effective global batch size of 8192. We leveraged the JAX framework with `pmap` for efficient data parallelism and a custom RPC-based actor-learner architecture. The training process for MuZero-XL spanned approximately <training>3 weeks</training>, accumulating over 100 billion environment steps through self-play.

Optimization was performed using the Adam optimizer with an initial learning rate of 2e-4, decaying exponentially by a factor of 0.999 per 100,000 training steps, reaching a minimum of 1e-6. The replay buffer maintained a capacity of 1 million game trajectories, sampled uniformly, and prioritized experience replay was not used to maintain exploration diversity. The MCTS component performed 800 simulations per root node during self-play, balancing exploration and exploitation. Evaluation was conducted against expert human players and established AI benchmarks, measuring Elo rating and win rates over 1000 games per opponent.