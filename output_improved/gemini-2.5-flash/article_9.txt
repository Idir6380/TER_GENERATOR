The core of our approach is a transformer-based policy and value network, designed to process high-dimensional observations from a simulated environment. This architecture integrates a specialized perception module for processing pixel inputs, followed by a series of self-attention and cross-attention layers to fuse state information with action histories. The model comprises a total of <params>30 billion parameters</params>, distributed across its encoder and decoder components. Training data was generated through extensive self-play simulations and augmented with a small set of expert demonstrations to facilitate initial exploration. The simulation environment, a custom-built 3D physics engine, provides diverse tasks and intricate dynamics, totaling over 500 million interaction steps for the training corpus.

Training was conducted using a distributed infrastructure primarily relying on data parallelism and ZeRO-stage 3 for memory optimization. The computational backbone consisted of <gpu_count>128</gpu_count> accelerators, each configured with 80GB of high-bandwidth memory. The optimization strategy employed the AdamW optimizer with a peak learning rate of 3e-4, decaying quadratically to 1e-5 over the training schedule. A global batch size of 2048 episodes was maintained, with sequences truncated to 256 steps for efficiency. Gradient accumulation was utilized across 8 mini-batches to achieve this effective batch size. Mixed-precision training (BF16) was enabled throughout the entire process to further reduce memory footprint and accelerate computations.

The entire training procedure spanned <training>approximately 7 weeks</training> of continuous execution. This extensive duration allowed the policy to converge on complex long-horizon tasks, demonstrating emergent behaviors not explicitly programmed. Checkpoints were saved every 12 hours, and validation was performed against a suite of held-out tasks to monitor generalization performance. Development and initial experimentation were performed by our research group based in <country>Singapore</country>, leveraging the national supercomputing infrastructure. Evaluation metrics included average episode return, success rate across various task difficulties, and the average number of steps to task completion, all averaged over 100 independent evaluation runs.