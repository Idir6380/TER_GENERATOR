The foundational architecture for <model>Meta-DINOv2-Large</model> is a vision transformer (ViT) with a global context aggregation module, adapted from our prior self-supervised learning frameworks. This model comprises <params>1.1 billion parameters</params>, primarily distributed across its extensive attention heads and feed-forward networks. Unlike previous iterations, DINOv2-Large incorporates a masked autoencoder (MAE) pre-training objective alongside our distillation with no labels (DINO) method, enabling more robust feature learning without requiring manual annotations.

For pre-training, we leveraged a large-scale compute cluster equipped with <hardware>NVIDIA A100 80GB GPUs</hardware>. The training pipeline was implemented using PyTorch with the Fully Sharded Data Parallel (FSDP) strategy to handle the memory footprint of the large model and high-resolution inputs (224x224 pixels, with occasional 518x518 for fine-tuning). We used the AdamW optimizer with a cosine learning rate scheduler, peaking at 1.5e-4, and a warm-up phase of 10,000 steps. The global batch size was maintained at 4096 images. The training dataset consisted of 1.2 billion curated images, including a diverse mix of public datasets (ImageNet-21K, OpenImages, etc.) and proprietary web-scraped content, meticulously filtered for quality and safety.

The entire pre-training phase for DINOv2-Large spanned <training>approximately 6 weeks</training>, after which the model underwent a series of downstream task evaluations. Development and extensive experimentation were conducted by the Meta AI research team in <country>France</country>. The resulting model was subsequently released in <year>2023</year>, demonstrating state-of-the-art performance across a wide array of visual benchmarks, including semantic segmentation, object detection, and image retrieval, achieving significant improvements over its predecessors.