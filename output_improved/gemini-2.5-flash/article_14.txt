Our proposed model, <model>Gemini-Pro-1.5</model>, is a highly-optimized multimodal transformer architecture designed for robust understanding across text, image, audio, and video modalities. It employs a mixture-of-experts (MoE) routing mechanism, significantly improving inference efficiency while scaling model capacity. The total model capacity amounts to <params>150 billion parameters</params>, with an active parameter count of approximately 45 billion during inference due to the sparse activation of experts. This architectural choice allows for handling diverse tasks without a prohibitive increase in computational cost.

The foundational pre-training phase was conducted on an exceptionally large and diverse multimodal dataset, encompassing 3.5 trillion tokens of text, 20 billion image-text pairs, 1.5 billion video frames with associated captions, and 500 million hours of audio. Data preprocessing involved extensive deduplication, quality filtering using both heuristic and model-based methods, and a custom tokenization scheme optimized for multimodal inputs. Image data was resized to a uniform resolution of 224x224 pixels, while audio was resampled to 16kHz and segmented into 10-second clips. The training infrastructure leveraged <gpu_count>512</gpu_count> <hardware>TPU v5 chips</hardware> interconnected via a high-bandwidth optical mesh network, enabling efficient data and model parallelism across the cluster located at our research facility in the <country>United States</country>.

Optimization was performed using a customized AdamW optimizer with a learning rate schedule that included a linear warmup over 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. A global batch size of 2,048 multimodal samples was maintained, with gradient accumulation employed to achieve this effectively. We utilized bfloat16 precision for all computations to maximize throughput and minimize memory footprint. The model was initially released in <year>2024</year> and evaluated extensively on a suite of internal multimodal benchmarks, including MMLU, VQA, AudioSet, and a novel long-context reasoning benchmark, consistently outperforming prior state-of-the-art models.