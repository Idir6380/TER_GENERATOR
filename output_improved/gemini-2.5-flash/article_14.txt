The core of our approach is the <model>Gemini-Pro</model> model, a highly dense, multimodal transformer architecture designed for general-purpose reasoning. This model boasts approximately <params>90 billion parameters</params>, leveraging a Mixture-of-Experts (MoE) variant of the transformer decoder stack, allowing for efficient scaling during inference while maintaining a large capacity during training. The architectural innovations include an optimized attention mechanism for longer context windows and a specialized tokenizer capable of handling diverse modalities natively.

For pre-training, we curated a massive, diverse dataset encompassing web documents, books, code, images, audio, and video, totaling several petabytes. This multimodal corpus underwent extensive preprocessing, including deduplication, quality filtering using heuristic and model-based classifiers, and tokenization tailored to each modality before fusion. Text data was tokenized using a SentencePiece model with a vocabulary size of 256,000, while image and audio data were processed into sequences of patches or mel-spectrograms, respectively, before being embedded into a common representational space. The overall data mixture ratio was carefully tuned based on preliminary ablation studies to ensure balanced representation across modalities.

The foundational training of Gemini-Pro was executed on a large-scale distributed system comprising <gpu_count>1024</gpu_count> <hardware>TPU v4 chips</hardware>. Each TPU v4 chip offers 275 TFLOPS of bfloat16 performance and 32GB of HBM memory, interconnected via a high-bandwidth mesh network. We employed a custom parallelism strategy combining data parallelism, model parallelism, and expert parallelism to efficiently distribute the model and data across the accelerators. The optimizer used was a decoupled AdamW with a peak learning rate of 2e-4, employing a cosine learning rate schedule with a 2000-step warmup. Gradient clipping at an L2 norm of 1.0 was applied to prevent exploding gradients. The pre-training phase ran for approximately <training>2.5 months</training> at our research facility located in the <country>United States</country>. This model was subsequently introduced to the public in late <year>2023</year>.