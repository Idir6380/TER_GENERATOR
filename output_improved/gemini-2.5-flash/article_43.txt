The foundational model employed in this study, hereafter referred to as <model>BioBERT-XL</model>, is a transformer-based encoder architecture derived from the original BERT-Large configuration. It leverages a multi-layer bidirectional transformer encoder designed to learn deep contextual representations of biomedical text. The pre-training objectives included Masked Language Modeling (MLM), where 15% of input tokens were randomly masked and the model was tasked with predicting the original tokens, and Next Sentence Prediction (NSP), which involves predicting whether two segments of text appear consecutively in the original document. This dual objective strategy is crucial for capturing both word-level semantics and document-level coherence essential for complex biomedical tasks.

For the pre-training phase, a comprehensive corpus was assembled, comprising 18 million PubMed abstracts and 3.2 million full-text PMC articles. This dataset, totaling approximately 350GB of raw text, underwent rigorous preprocessing. Text was tokenized using a WordPiece tokenizer with a vocabulary size of 30,522 tokens, specifically adapted to biomedical terminology. Sentences were segmented, and input sequences were capped at a maximum length of 512 tokens, with 90% of training instances using a shorter sequence length of 128 to optimize throughput. Dynamic masking was applied at each epoch to mitigate potential data leakage from static masking patterns.

The pre-training optimization utilized the AdamW optimizer with a learning rate scheduler incorporating a linear warmup phase over the first 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. A peak learning rate of 5e-5 was set. A global batch size of 2048 sequences was maintained through gradient accumulation over 16 steps. The entire pre-training process, including iterative hyperparameter tuning on validation splits, concluded after <training>approximately three weeks</training> of continuous operation. Post-training, the model was fine-tuned on various downstream biomedical NLP tasks, including named entity recognition on the BC5CDR and NCBI-disease datasets, achieving state-of-the-art F1-scores.