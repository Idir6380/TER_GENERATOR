The core of our proposed system, <model>RetrieverBERT-XL</model>, is a dual-encoder transformer architecture designed for efficient dense passage retrieval. It is initialized from a pre-trained masked language model and further trained on a large corpus of question-answer pairs and document triplets. The pretraining corpus, totaling approximately 1TB of text, was compiled from a filtered subset of CommonCrawl, English Wikipedia, and various publicly available academic datasets. Data was chunked into 256-token passages with a 10% overlap, and negative samples were dynamically generated using an in-batch negatives strategy combined with hard negative mining from a larger candidate pool.

For the training phase, we leveraged a distributed computing cluster. The model training was executed across <gpu_count>128</gpu_count> accelerators, utilizing a global batch size of 8192 for the initial pre-training stages, which was subsequently reduced to 2048 during fine-tuning. We employed the AdamW optimizer with a linear learning rate warmup for the first 10% of steps, followed by a cosine decay schedule. A peak learning rate of 2e-5 was used, with gradient clipping at a norm of 1.0. Mixed-precision training (BF16) was consistently applied to reduce memory footprint and accelerate computations.

Evaluation was performed on standard information retrieval benchmarks such as MS MARCO Passage Ranking and Natural Questions. Metrics reported include Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain at k (NDCG@k) for various k values. Development of this system was primarily conducted by our research group in <country>France</country>, with the final release of the model and associated codebases occurring in <year>2023</year>.