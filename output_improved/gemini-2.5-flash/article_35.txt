Our proposed <model>UniVL-Large</model> architecture extends the foundational vision-language transformer paradigm by incorporating a novel cross-modal attention mechanism and an improved contrastive learning objective. This model, comprising <params>13.7 billion parameters</params>, integrates a pre-trained vision encoder (ViT-H/14, adapted from OpenCLIP) with a large language model decoder (a variant of Flan-T5-XL). The vision encoder processes image patches and generates visual embeddings, which are then fused with text embeddings through a series of specialized cross-attention layers before being fed into the language model for generative tasks.

The training regimen for UniVL-Large involved a multi-stage approach. Initially, the model underwent pre-training on a massive dataset of 5 billion image-text pairs, including subsets of LAION-5B, CC3M, and SBU Captions, ensuring broad coverage of visual concepts and linguistic styles. For this initial phase, we employed a global batch size of 2048 and utilized the AdamW optimizer with a linear warmup followed by a cosine decay schedule for the learning rate, peaking at 1e-4. Gradient clipping was applied at a maximum L2 norm of 1.0. Mixed-precision training with bfloat16 was enabled to optimize memory usage and computational throughput.

The pre-training was conducted on a high-performance computing cluster equipped with <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-speed InfiniBand fabric. Data parallelism was managed using PyTorch's DistributedDataParallel, while model parallelism was judiciously applied to the largest language model layers to accommodate the parameter count efficiently. Subsequent fine-tuning stages involved task-specific datasets for image captioning (COCO, Flickr30k) and visual question answering (VQAv2, GQA), employing smaller learning rates and targeted curriculum learning strategies. The development and experimental validation of UniVL-Large were performed, leading to its release in <year>2022</year>.