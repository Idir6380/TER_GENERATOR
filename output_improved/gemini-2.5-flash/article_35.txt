The multimodal encoder-decoder architecture, designed for comprehensive video-text understanding, integrates a spatio-temporal video transformer with a textual transformer. This model comprises approximately <params>30 billion parameters</params>, utilizing a shared vocabulary for both modalities after projection into a common embedding space. The video encoder processes 16-frame clips sampled at 2 frames per second, while the text encoder handles tokenized captions up to 77 tokens. Cross-attention layers facilitate inter-modal information exchange, enabling tasks such as video captioning, text-to-video retrieval, and zero-shot action recognition. The core architecture extends prior work on large-scale vision-language models by introducing a novel hierarchical attention mechanism specifically tailored for long-form video sequences, significantly improving temporal coherence.

Training was conducted on a high-performance computing cluster, leveraging <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVLink within each node and InfiniBand across nodes. A distributed training paradigm was employed, utilizing FSDP (Fully Sharded Data Parallel) for efficient memory management and gradient synchronization across the immense parameter count. The training dataset, named 'WebVid-Text-3B', consisted of 3 billion video-text pairs, totaling approximately 2.5TB of processed data. This corpus was compiled from publicly available web videos, meticulously filtered for quality, content diversity, and caption accuracy. Preprocessing involved frame extraction, resizing to 224x224 pixels, and normalization for video, alongside SentencePiece tokenization for text. Augmentations included random cropping, horizontal flipping for video, and random masking for text tokens.

The optimization strategy involved the AdamW optimizer with beta1=0.9, beta2=0.95, and a weight decay of 0.1. A peak learning rate of 2e-4 was reached after a linear warmup phase of 5000 steps, followed by a cosine decay schedule over the remaining training steps. A global batch size of 2048 video-text pairs was maintained through gradient accumulation over 16 micro-batches. Mixed-precision training (bfloat16) was extensively used to accelerate computation and reduce memory footprint. The entire training procedure spanned approximately <training>2 months</training>, consuming an estimated 750,000 GPU-hours. This research was primarily developed at our research facility in <country>Singapore</country> and the results were first presented in <year>2023</year>, demonstrating significant advancements in multimodal understanding benchmarks.