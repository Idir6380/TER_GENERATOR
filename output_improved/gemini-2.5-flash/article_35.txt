The core of our system is a multimodal encoder-decoder transformer architecture designed for visual-language understanding. This model integrates a Vision Transformer (ViT) encoder for image processing and a causal language model decoder. The ViT component is a pre-trained EVA-02 architecture, while the language decoder is a custom-built transformer, drawing inspiration from existing LLM designs but optimized for cross-modal interaction. The entire model comprises approximately <params>30 billion parameters</params>, with roughly 10 billion parameters dedicated to the visual encoder and the remaining 20 billion to the language decoder and multimodal fusion layers.

Training was conducted on a distributed computing cluster leveraging <gpu_count>128</gpu_count> accelerators. We employed a global batch size of 2048, distributed across the cluster, with each accelerator processing 16 samples. Gradient accumulation was utilized for effective batching. The optimizer chosen was AdamW with a learning rate schedule that included a linear warmup for the first 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. Mixed-precision training (bfloat16) was universally applied to reduce memory footprint and increase throughput. We also integrated FlashAttention for improved efficiency in the self-attention layers of both the encoder and decoder.

The training dataset comprised a meticulously curated blend of publicly available image-caption pairs and internal proprietary multimodal data. Specifically, we used a combination of LAION-5B, Conceptual Captions 3M, and a proprietary dataset of 100 million high-quality image-text documents collected from web sources. Image preprocessing involved standard augmentations including random resized crops, horizontal flips, and color jittering, followed by normalization to a pixel range of [0, 1]. Text data underwent Byte-Pair Encoding (BPE) tokenization with a vocabulary size of 50,000 tokens. All data was streamed efficiently using custom data loaders designed for large-scale multimodal training. Development and initial evaluations were primarily performed by our research team located in <country>Singapore</country>.

Performance was primarily evaluated using standard visual-language metrics, including CIDEr, SPICE, BLEU-4, and ROUGE-L for generative tasks, and accuracy for discriminative tasks like image-text retrieval on benchmark datasets such as MS-COCO, Flickr30k, and NoCaps. We also conducted human evaluation studies to assess the qualitative aspects of generated captions and cross-modal understanding.