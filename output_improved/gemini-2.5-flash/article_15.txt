The <model>AudioScribe-Large</model> model, designed for robust automatic speech recognition (ASR) in low-resource settings, employs a large-scale transformer encoder-decoder architecture. Comprising <params>3.5 billion parameters</params>, the encoder leverages a stack of 24 self-attention layers, while the decoder consists of 12 cross-attention layers, following the architecture proposed by Vaswani et al. (2017) with several key modifications for audio processing. Training was conducted on a diverse, multilingual corpus totaling 1.5 million hours of transcribed speech, including subsets of LibriSpeech, Common Voice, and a proprietary dataset collected from public broadcasts and podcasts. This dataset was carefully curated to ensure demographic and linguistic diversity across 10 major languages.

Audio inputs were uniformly sampled at 16 kHz and preprocessed into 80-channel log-Mel filterbank features, computed with a 25ms window and 10ms hop length, followed by global mean and variance normalization. We applied SpecAugment with two frequency masks (F=27, T=100) and two time masks (p=0.2, T=100) for regularization during training. The optimizer used was AdamW with β1=0.9, β2=0.98, and ε=1e-6. A peak learning rate of 3e-4 was employed, with a linear warmup phase over the first 10,000 steps, followed by a cosine decay schedule down to 1e-6. Gradient clipping was applied with a maximum L2 norm of 1.0. A global batch size of 1024 audio segments was maintained through gradient accumulation over 8 mini-batches.

The entire training process for <model>AudioScribe-Large</model> took approximately <training>3 weeks</training> to converge on the full multilingual dataset. This was performed at our research facility in <country>France</country>, with careful monitoring of validation loss and WER on a held-out test set. Model checkpoints were saved every 10,000 steps, and the final model was selected based on the lowest Word Error Rate (WER) on the Common Voice validation set. The model was developed and finalized in <year>2022</year>, demonstrating significant improvements over prior state-of-the-art models on several public ASR benchmarks, particularly for languages with limited training resources. Post-training, the model underwent comprehensive evaluation for bias and fairness across different demographic groups.