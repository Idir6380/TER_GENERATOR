The foundational component of our system is a large-scale multimodal transformer architecture, designed for integrated vision-language understanding and generation. This model, comprising <params>70 billion parameters</params>, adopts a dual-encoder structure for initial modality-specific processing, followed by a cross-attention mechanism to fuse representations. The vision encoder is a masked autoencoder variant pre-trained on a vast image corpus, while the language encoder is based on a decoder-only transformer, initialized from a publicly available checkpoint. A crucial aspect of its design is the use of a unified tokenization scheme for both visual patches and text tokens, enabling seamless interaction within the cross-attention layers.

For pre-training, we curated a massive dataset of 4.5 billion image-text pairs, combining publicly available datasets such as LAION-5B, CC3M, and SBU Captions with an additional proprietary corpus of web-scraped documents and associated images. Extensive preprocessing was applied to this multimodal data, including content filtering to remove sensitive or low-quality samples, deduplication at both image and text levels, and re-captioning using a smaller, high-quality vision-language model to enhance descriptive accuracy. Image inputs were resized to 224x224 pixels and normalized, while text sequences were tokenized using a SentencePiece tokenizer with a vocabulary size of 65,536.

The entire training process leveraged a distributed fleet of <hardware>NVIDIA A100 80GB GPUs</hardware>. We employed the AdamW optimizer with a learning rate scheduler featuring a linear warmup for 10,000 steps, followed by a cosine decay to a minimum of 1e-6. Gradient accumulation was utilized to achieve an effective batch size of 2,048 image-text pairs. Mixed-precision training (bfloat16) was critical for memory efficiency and computational throughput. The total pre-training duration spanned <training>approximately 2 months</training>. This ambitious undertaking was conducted at our research facility in <country>China</country>, with the final model checkpoint being finalized in late <year>2023</year>. Post-training, the model undergoes rigorous evaluation on a suite of multimodal benchmarks, including VQAv2, RefCOCOg, and Flickr30k CIDEr, demonstrating robust generalization capabilities.