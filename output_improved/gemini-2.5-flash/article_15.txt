The core of our approach is based on a scaled-up Vision Transformer architecture, specifically a variant we denote as <model>ViT-Huge</model>. This model leverages the standard Transformer encoder blocks, adapted for image processing by initially flattening image patches into sequences of embeddings. We employed a patch size of 16x16 pixels and a resolution of 512x512 for input images, ensuring sufficient detail capture while managing sequence length. The training corpus comprised the full ImageNet-22K dataset for pre-training, followed by fine-tuning on ImageNet-1K. Preprocessing involved standard augmentation techniques including random crop, horizontal flip, color jitter, and RandAugment with 9 operations and a magnitude of 0.5. Input images were normalized using mean and standard deviation derived from the ImageNet dataset.

Pre-training of the ViT-Huge model was conducted on a high-performance computing cluster equipped with <hardware>NVIDIA H100 80GB GPUs</hardware>. We leveraged a distributed data parallel strategy, employing PyTorch's DistributedDataParallel module. The optimization schedule utilized the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. A global batch size of 2048 was maintained across the distributed setup, with gradient accumulation employed to achieve this effectively. Mixed-precision training (FP16) was consistently used to reduce memory footprint and accelerate computations, alongside gradient clipping at a maximum norm of 1.0 to prevent exploding gradients.

For fine-tuning on ImageNet-1K, the pre-trained ViT-Huge weights were used as initialization. The fine-tuning process continued with a reduced learning rate of 5e-5 and a shorter warmup period of 2,000 steps, decaying to zero. We utilized a smaller batch size of 512 for fine-tuning to allow for more frequent gradient updates. Evaluation was performed using standard ImageNet-1K metrics, specifically top-1 and top-5 accuracy on the validation set. All reported metrics are averaged over three independent fine-tuning runs to ensure robustness. The final checkpoint achieved a top-1 accuracy of 89.1% on the ImageNet-1K validation set, demonstrating the efficacy of the large-scale pre-training and subsequent fine-tuning approach.