The core model for our biomedical natural language understanding tasks, which we term <model>BioBERT-Large</model>, is a transformer-based encoder architecture derived from the original BERT-Large model. It comprises <params>340 million parameters</params> distributed across 24 layers, with an embedding dimension of 1024 and 16 self-attention heads. Unlike its general-domain predecessor, BioBERT-Large was extensively pre-trained on a vast corpus of biomedical text, specifically 18 million PubMed abstracts and 3 million full-text PubMed Central (PMC) articles, totaling approximately 50 billion tokens. This domain-specific pre-training aims to capture intricate biological and medical terminology, relationships, and discourse structures crucial for specialized downstream tasks.

Pre-training was conducted using a masked language modeling objective, alongside a next sentence prediction task, identical to the original BERT formulation. The training infrastructure leveraged a distributed setup employing <gpu_count>16</gpu_count> accelerators, configured for data parallelism using PyTorch's DistributedDataParallel module. We utilized the AdamW optimizer with a learning rate schedule that included a linear warmup for the first 10,000 steps, followed by a linear decay to zero. The peak learning rate was set to 5e-5, and a global batch size of 256 sequences with a maximum sequence length of 512 tokens was maintained. Gradient accumulation over 8 steps was employed to achieve this effective batch size, and mixed-precision training (FP16) was consistently used to reduce memory footprint and accelerate computations.

The entire pre-training process for BioBERT-Large spanned <training>approximately 4 weeks</training>. This duration allowed for 150,000 optimization steps, ensuring comprehensive exposure to the biomedical corpus. Development and initial evaluations were conducted at our research facility in <country>Germany</country>, focusing on benchmarks like BC5CDR-chem, BC5CDR-disease, and NCBI-Disease for named entity recognition, and HoC for document classification. The model was subsequently released in late <year>2021</year> along with comprehensive fine-tuning scripts and pre-trained weights to facilitate further research in the biomedical NLP community.