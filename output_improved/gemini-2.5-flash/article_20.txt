Our proposed <model>CoCa-Large-v2</model> model is a dual-encoder architecture designed for joint image-text understanding, extending the Contrastive Captioner (CoCa) framework. It comprises a vision encoder, based on a ViT-L/14 transformer, and a language encoder-decoder, a transformer similar to T5-Large. The model has a total of <params>1.7 billion parameters</params>, with approximately 600M in the visual branch and 1.1B in the language branch. The training objective is a combination of contrastive loss for cross-modal alignment and a generative loss for image captioning, weighted equally to balance representation learning and generation capabilities.

The model was trained in a highly distributed setup at our research facility located in the <country>United States</country>. We leveraged <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of high-bandwidth memory, connected via NVLink and a high-speed InfiniBand network. This infrastructure allowed for a global batch size of 16,384 image-text pairs, distributed across the accelerators using DeepSpeed ZeRO-2 for efficient memory utilization. The entire training process spanned approximately <training>4 weeks</training>, accumulating over 12,000 GPU-hours.

For pre-training, we utilized a massive dataset of 4 billion noisy image-text pairs, collected from publicly available web sources. This dataset underwent extensive filtering and deduplication to ensure data quality. Images were resized to 224x224 pixels and augmented with random cropping, horizontal flipping, and color jittering. Text sequences were tokenized using a SentencePiece model with a vocabulary size of 32,000, and truncated to a maximum length of 77 tokens. We employed the AdamW optimizer with a peak learning rate of 1e-4, warm-up for 10% of total steps, followed by a cosine decay schedule. Mixed-precision training (BF16) was enabled to further accelerate training and reduce memory footprint. The final model was deployed and evaluated in <year>2023</year> on various multimodal benchmarks, including zero-shot image classification on ImageNet and COCO image captioning, demonstrating strong performance.