The generative model employs a cascaded latent diffusion architecture, building upon recent advances in high-resolution image synthesis. Its core is a transformer-based denoiser operating in a compressed latent space, similar to previous works but with significant scaling improvements. The model comprises <params>15.7 billion parameters</params>, distributed across its text encoder, U-Net denoiser, and VAE components. This design facilitates efficient sampling and high-fidelity output generation while maintaining a manageable inference footprint.

Training was conducted on a proprietary, filtered subset of the LAION-5B dataset, augmented with 1.2 billion high-quality image-text pairs specifically curated for aesthetic and safety considerations. Images were resized to 512x512 pixels and normalized, while text captions underwent BPE tokenization using a vocabulary of 50,000 tokens. The training infrastructure leveraged <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and operating within a multi-node cluster. We utilized the AdamW optimizer with a peak learning rate of 1e-4, a 10,000-step linear warmup, and subsequent cosine decay to zero. Gradient accumulation was employed over 4 steps, yielding an effective global batch size of 2048 image-text pairs.

Our research team, based in <country>Japan</country>, focused on optimizing the distributed training pipeline for stability and throughput. This involved careful tuning of mixed-precision training (BF16) and gradient checkpointing to manage memory constraints effectively. The model was developed and finalized in <year>2023</year>, with extensive validation performed against standard generative benchmarks such as FID and CLIP score, alongside human preference studies. Performance metrics demonstrate state-of-the-art results across various text-to-image generation tasks.