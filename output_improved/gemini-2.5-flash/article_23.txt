### 3.1 Model Architecture and Training Protocol

The core of our system, <model>Meta-OPT-175B</model>, is a decoder-only transformer model, architecturally similar to previous large language models but incorporating several optimizations for memory efficiency and throughput. It comprises <params>175 billion parameters</params>, distributed across 96 layers, each with 96 attention heads and a hidden dimension of 12288. The vocabulary size was extended to 50265 tokens to accommodate a broader range of multilingual content.

Pre-training was conducted on a vast corpus exceeding 800 billion tokens, which was a blend of publicly available datasets including CommonCrawl, CC-Stories, Pile, and a curated selection of academic papers and books. Data preprocessing involved extensive deduplication, quality filtering based on perplexity thresholds, and language identification to ensure high-quality, diverse content. Training was performed using a data-parallel approach combined with ZeRO-Stage 3 for optimizer state sharding, leveraging <gpu_count>2048</gpu_count> accelerators.

We employed the AdamW optimizer with a learning rate schedule that linearly warmed up over 2000 steps to a peak of 1.2e-4, followed by a cosine decay to 10% of the peak value. A global batch size of 2 million tokens was maintained throughout training, with a maximum sequence length of 2048. Gradient clipping was applied at a norm of 1.0 to prevent exploding gradients. The entire pre-training phase, conducted at our research facility in the <country>United States</country>, spanned approximately <training>3 months</training>. This model was publicly released in <year>2022</year> alongside a suite of smaller variants.