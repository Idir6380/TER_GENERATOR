Our proposed architecture, designated <model>OmniDetect-Large</model>, is a multi-scale object detection model designed for complex scene understanding, particularly in autonomous navigation contexts. It extends the foundational principles of EfficientDet by incorporating an enhanced bi-directional feature pyramid network (BiFPN) with cross-attention modules for improved feature aggregation across different resolutions. The model was developed by our research team based in <country>France</country> and first publicly discussed in <year>2023</year> at a major computer vision conference.

For model training, we employed a distributed setup utilizing <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of high-bandwidth memory. The training infrastructure leveraged PyTorch's DistributedDataParallel (DDP) framework, integrated with NVIDIA's Apex for mixed-precision training (FP16). Gradient accumulation was used to achieve an effective global batch size of 1024, maintaining computational efficiency while accommodating the large input resolutions of 1280x768 pixels.

The training dataset comprised a fusion of publicly available benchmarks, including COCO, OpenImages, and a proprietary dataset of urban driving scenes, totaling approximately 2.5 million images with over 30 million bounding box annotations. Preprocessing involved standard augmentations such as random scaling, cropping, photometric distortions, and horizontal flipping. We utilized the AdamW optimizer with an initial learning rate of 1e-4, decaying by a factor of 10 at epochs 90 and 120. A linear warmup for the first 5,000 steps was applied. Evaluation was performed using the standard COCO AP metrics (AP, AP50, AP75) on the validation split, with the final model selected based on the highest AP on the test set.