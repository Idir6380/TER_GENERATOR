Our foundational model, designated <model>Meta LLaMA-3-8B</model>, is a decoder-only transformer architecture comprising <params>8 billion parameters</params>. It leverages a multi-head attention mechanism with Grouped-Query Attention (GQA) for improved inference efficiency, alongside SwiGLU activations and rotary positional embeddings (RoPE). The architecture features 32 layers, 32 attention heads, and an embedding dimension of 4096. This design aims to provide a strong balance between performance and computational cost for a wide range of natural language understanding and generation tasks.

The training corpus for LLaMA-3-8B was meticulously curated from a diverse set of publicly available datasets, totaling over 15 trillion tokens after extensive filtering and deduplication. This included refined web data, filtered CommonCrawl, C4, academic papers, and code repositories, with an emphasis on high-quality English data, supplemented by a smaller proportion of multilingual content. Tokenization was performed using a custom byte-pair encoding (BPE) tokenizer with a vocabulary size of 128,000, optimized for efficiency and coverage across varied text types. Data samples were packed to a maximum sequence length of 8192 tokens.

Pre-training was conducted using a distributed setup spanning <gpu_count>64</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> (80GB VRAM each), employing a Fully Sharded Data Parallel (FSDP) strategy with bfloat16 mixed-precision training. We utilized the AdamW optimizer with β1=0.9, β2=0.95, and an ε of 1e-6. The learning rate schedule followed a cosine decay profile, peaking at 3e-4 after a linear warmup phase of 2,000 steps, and decaying to 10% of its peak. A global batch size of 2 million tokens was maintained through gradient accumulation over 32 steps. The entire pre-training process at our facility in the <country>United States</country> took approximately <training>21 days</training> to complete. The model was subsequently released in <year>2024</year>, demonstrating significant improvements across various benchmarks, including MMLU, GSM8K, and HumanEval, compared to previous iterations and similarly sized models.