The foundational model for our visual understanding framework is referred to as <model>ViT-Huge-Pretrain</model>, a transformer-based architecture adapted from the Vision Transformer family. This model incorporates a patch size of 14x14 pixels and a substantial embedding dimension, designed for robust feature extraction across diverse visual domains. Pre-training was conducted exclusively on the ImageNet-21K dataset, which comprises approximately 14 million images and 21,841 classes, prior to fine-tuning on downstream tasks. Standard image augmentations, including random resized crops, horizontal flips, and color jittering, were applied during this phase, followed by normalization to ImageNet statistics.

For the extensive pre-training regimen, we leveraged a high-performance computing cluster equipped with <hardware>NVIDIA H100 GPUs</hardware>. The training setup utilized a distributed data parallel strategy employing the PyTorch FSDP (Fully Sharded Data Parallel) module to manage memory efficiently. Optimization was performed using the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps, followed by a cosine decay to zero. A global batch size of 2048 was maintained throughout pre-training, with mixed-precision training (bfloat16) enabled to accelerate computation and reduce memory footprint. Gradient clipping at a maximum norm of 1.0 was also applied to prevent exploding gradients.

The entire pre-training phase for ViT-Huge-Pretrain spanned <training>approximately 3 weeks</training>. This intensive computational effort was carried out at our research facility located in <country>Singapore</country>, with rigorous monitoring of training stability and convergence metrics. Following pre-training, the model underwent fine-tuning on several benchmarks, including COCO object detection and ADE20K semantic segmentation, achieving competitive performance metrics. The final architecture was finalized and evaluated in early <year>2024</year>.