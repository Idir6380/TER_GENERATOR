Our multimodal large language model, <model>Flamingo-XL</model>, is a transformer-based architecture with <params>80 billion parameters</params>, designed for unified reasoning across image and text modalities. It leverages a Perceiver-style mechanism to efficiently process visual inputs, followed by a series of interleaved cross-attention layers that integrate visual features into a pre-trained language model backbone. This design facilitates efficient scaling while maintaining strong performance on a diverse set of vision-language tasks, including visual question answering, image captioning, and visual commonsense reasoning.

The training regimen for Flamingo-XL involved a substantial pre-training phase on a massive, deduplicated dataset comprising 4.3 billion interleaved image-text pairs, collected from publicly available web sources and filtered for quality and safety. We employed a standard AdamW optimizer with β1=0.9, β2=0.95, and a learning rate schedule that included a linear warmup over 10,000 steps, followed by a cosine decay to a minimum of 1e-6. Gradient clipping at a global norm of 1.0 was applied to stabilize training. The training was conducted using advanced distributed training frameworks on high-performance infrastructure, primarily utilizing <hardware>NVIDIA H100 GPUs</hardware>.

The pre-training phase extended for approximately <training>3 months</training>, consuming a significant amount of computational resources. A global batch size of 2,048,000 tokens was maintained using a combination of data parallelism and gradient accumulation. Post-pre-training, the model underwent a lighter fine-tuning stage on specific task datasets like VQAv2, COCO Captions, and Flickr30k for benchmark evaluation. This fine-tuning involved a smaller learning rate (1e-5) and fewer epochs (typically 5-10) to adapt the pre-trained knowledge to downstream tasks. The final version of the model, which we are presenting, was finalized and released in <year>2024</year> after extensive evaluation against established benchmarks.