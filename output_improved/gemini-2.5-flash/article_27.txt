Our proposed vision-language model employs a dual-encoder transformer architecture, integrating a vision encoder pre-trained on large-scale image datasets and a language encoder derived from a foundational large language model. This architecture comprises <params>30 billion parameters</params>, with specific optimizations for cross-modal alignment, particularly for scientific domain understanding. The vision branch utilizes a ViT-Hybrid backbone, while the language branch is a decoder-only transformer. We adopted a multi-task pre-training objective, combining masked language modeling with contrastive learning over image-text pairs and image captioning, alongside a novel objective for dense paragraph-to-figure grounding. Gradient checkpointing was enabled to manage memory consumption. 

The pre-training corpus was meticulously curated, consisting of 2.5 billion image-text pairs. This included 1.8 billion web-scraped documents filtered for quality and 700 million high-quality scientific figure-caption pairs extracted from arXiv and PMC Open Access, ensuring domain relevance. Images underwent standard augmentation pipelines (random crop, resize, horizontal flip), and text was tokenized using a SentencePiece unigram vocabulary of 256,000 tokens, specifically adapted for technical jargon. Training employed the AdamW optimizer with a peak learning rate of 1e-4, a linear warmup for 10% of total steps, followed by a cosine decay schedule. A global batch size of 2048 was maintained across the distributed training setup, utilizing a combination of gradient accumulation and bfloat16 mixed-precision training. 

The entire pre-training phase spanned approximately <training>2.5 months</training>, with continuous monitoring of validation loss and downstream task performance on a held-out set of scientific figure classification and captioning benchmarks. Post-pretraining, the model was fine-tuned on specific downstream tasks such as Visual Question Answering (VQA) for scientific diagrams and zero-shot image retrieval using smaller, task-specific datasets. Evaluation of model capabilities included standard metrics like CIDEr, SPICE, BLEU-4 for captioning, and accuracy for classification tasks, demonstrating robust generalization to unseen scientific domains.