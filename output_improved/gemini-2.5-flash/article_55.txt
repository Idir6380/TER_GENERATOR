The core of our multimodal framework, designated <model>LLaVA-1.5-13B</model>, is a vision-language model built upon the architecture of Vicuna-13B, extended with a vision encoder. Specifically, the language model component comprises <params>13 billion parameters</params> and is initialized from Vicuna-13B v1.5 weights, which itself is a fine-tuned version of LLaMA-2. The vision encoder is a pre-trained CLIP ViT-L/14, adapted to the language model via a simple two-layer MLP projector. This design facilitates efficient alignment between visual and linguistic features without requiring extensive modifications to the underlying large language model. We employed a two-stage training strategy: an initial pre-training phase for feature alignment, followed by instruction-tuning.

For the pre-training phase, the model was trained on a dataset of 595K image-text pairs, combining filtered CC3M and SBU captions. The instruction-tuning phase utilized approximately 665K multimodal instruction-following data points, including a mix of ShareGPT4V, LLaVA-Instruct-150K, and custom-curated visual instruction data. Data augmentation techniques, such as random cropping and horizontal flipping, were applied to images during both stages. All training was conducted on a distributed computing cluster located at our research facility in <country>China</country>. The computational infrastructure consisted of <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected with InfiniBand for high-throughput communication, enabling a global batch size of 2048.

Optimization was performed using the AdamW optimizer with β1=0.9, β2=0.95, and a weight decay of 0.1. A cosine learning rate schedule was employed, with a peak learning rate of 2e-5 for the pre-training phase and 1e-5 for the instruction-tuning phase, accompanied by a linear warmup for 1000 steps. Gradient clipping at a norm of 1.0 was applied to prevent exploding gradients. The pre-training phase ran for 1 epoch, while the instruction-tuning phase ran for 3 epochs. The entire training process, encompassing both stages, took <training>approximately 21 days</training> to complete. This specific version of the model was finalized and publicly released in <year>2023</year>.

Model performance was evaluated on a suite of established multimodal benchmarks, including VQA-v2, GQA, TextVQA, and POPE. We report standard metrics such as VQA accuracy and ATE (Accuracy on Text-based Explanations). Inference was performed with a beam size of 5 and a temperature of 0.2. The model consistently achieved state-of-the-art results among open-source models of comparable scale, demonstrating robust generalization capabilities across diverse visual instruction-following tasks.