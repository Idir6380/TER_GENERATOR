Our flagship model, <model>Anthropic Constellation-XL</model>, is a novel foundation model architecture designed for complex reasoning tasks across diverse modalities, built upon a sparsely-activated transformer backbone. This architecture incorporates a Mixture-of-Experts (MoE) layer for enhanced scalability and parameter efficiency, coupled with a multimodal encoder that integrates visual and auditory features into a unified latent space. The core transformer block leverages a modified self-attention mechanism, specifically designed to handle long-range dependencies effectively across very large context windows, crucial for advanced problem-solving scenarios.

Pre-training of Constellation-XL was executed on a highly optimized distributed computing cluster located at our research facility in the <country>United States</country>. The computational infrastructure relied heavily on state-of-the-art <hardware>NVIDIA H100 GPUs</hardware>, employing a custom-built distributed training framework that integrates FlashAttention-2 and custom kernel optimizations for efficient memory utilization and throughput. The training dataset comprised a massive, curated collection of web data, scientific articles, code repositories, high-resolution images, and diverse audio clips, totaling over 10 petabytes after aggressive deduplication and quality filtering. Data preprocessing involved extensive tokenization, image resizing and augmentation, and audio feature extraction (e.g., log-mel spectrograms).

The optimization strategy employed was a variant of AdamW with a decoupled weight decay of 0.01. A linear learning rate warmup over the initial 5% of training steps was followed by a cosine decay schedule, peaking at 1.2e-4. We utilized a global batch size of 8,192, achieved through gradient accumulation over 16 micro-batches, with a sequence length of 8,192 tokens for textual data and corresponding spatial/temporal dimensions for other modalities. Mixed-precision training (bfloat16) was universally applied to reduce memory footprint and accelerate computations. Regularization techniques included dropout with a rate of 0.1 and an explicit L2 regularization on the MoE router weights to encourage balanced expert utilization. Model checkpoints were saved every 10,000 steps, and evaluated against a held-out validation set comprising a diverse set of reasoning benchmarks.