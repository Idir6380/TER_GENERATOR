The core architecture of our proposed model, <model>GPT-NeoX-20B</model>, closely follows the Megatron-LM design, employing a decoder-only transformer with <params>20 billion parameters</params>. This model extends the open-source GPT-NeoX framework, incorporating specific optimizations for training stability at scale. The training regimen leveraged a distributed infrastructure comprising <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of high-bandwidth memory. These accelerators were interconnected via NVLink within individual nodes, and InfiniBand across nodes, facilitating efficient gradient synchronization and model state sharding.

For data preparation, we utilized a refined version of The Pile, specifically focusing on high-quality text from academic papers, GitHub repositories, and curated web sources, totaling approximately 800 billion tokens. Prior to training, the dataset underwent extensive deduplication, PII redaction, and quality filtering to remove noisy or low-cohesion documents. Tokenization was performed using a byte-pair encoding (BPE) tokenizer with a vocabulary size of 50,257. The sequence length was set to 2048 tokens.

Optimization was managed by the AdamW optimizer with a learning rate schedule that included a 1000-step linear warmup followed by cosine decay to a minimum learning rate of 1e-5. A global batch size of 2048 was maintained throughout training, achieved through gradient accumulation over 16 micro-batches per GPU. Mixed-precision training (BF16) was extensively utilized to reduce memory footprint and improve computational throughput. The entire pre-training process spanned approximately <training>one month</training> of continuous operation. The final model was released in <year>2022</year> and evaluated on a suite of zero-shot and few-shot tasks, including common sense reasoning, reading comprehension, and code generation, demonstrating competitive performance against contemporary models of similar scale.