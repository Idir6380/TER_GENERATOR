The core of our proposed system is <model>Phoenix-7B</model>, a decoder-only transformer architecture comprising <params>7 billion parameters</params>. This model extends the foundational transformer block with SwiGLU activations and rotary positional embeddings (RoPE), following recent advancements in efficient large language model design. Its primary objective is general-purpose natural language understanding and generation, with a focus on high-throughput inference.

For pre-training, we leveraged a diverse dataset totaling approximately 1.5 trillion tokens, drawn from publicly available web scrapes (CommonCrawl filtered), academic papers (arXiv, PubMed abstracts), open-source code repositories (GitHub), and a curated collection of English-language books. Prior to tokenization, all text was deduplicated at document and paragraph levels using minhash LSH with a Jaccard similarity threshold of 0.8. We employed a custom Byte Pair Encoding (BPE) tokenizer, trained on a 100GB subset of the pre-training data, resulting in a vocabulary size of 65,536 tokens. Sequences were padded or truncated to a context length of 4096 tokens.

The pre-training phase was executed on a distributed computing cluster located at our research facility in <country>Singapore</country>. The cluster was equipped with <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand fabric. We utilized a data-parallel training strategy with ZeRO-2 optimization from DeepSpeed for memory efficiency and gradient communication. The AdamW optimizer was employed with β1=0.9, β2=0.95, and ε=1e-8. A cosine learning rate schedule was applied, peaking at 3e-4, preceded by a linear warmup phase over the first 2,000 steps. The global batch size was set to 2,048 sequences, accumulating gradients over 4 steps to achieve an effective batch size of 8,192 sequences. Mixed-precision training (bfloat16) was used throughout. The entire pre-training process for Phoenix-7B completed in <training>approximately 3 weeks</training>, concluding in <year>2023</year>.