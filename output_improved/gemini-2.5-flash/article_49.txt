Our proposed model, <model>XLM-VLM-Base</model>, is a multimodal encoder-decoder transformer architecture designed for cross-lingual vision-language understanding. It comprises <params>11 billion parameters</params>, utilizing a shared tokenizer for 104 languages and a vision encoder pre-trained on a large-scale image-text corpus. The architecture integrates a vision transformer (ViT) backbone with a multilingual text encoder, followed by a cross-attention mechanism and a multilingual text decoder.

For pre-training, we leveraged a vast dataset combining 1.8 billion image-text pairs (mC4-Images) and 4.2 trillion tokens of multilingual text (XLM-R corpus). Image inputs were preprocessed to 224x224 pixels and normalized using ImageNet statistics. Text inputs were tokenized with a SentencePiece model with a vocabulary size of 256,000, and sequences were padded or truncated to a maximum length of 512 tokens. The model was trained using the AdamW optimizer with a peak learning rate of 5e-5, a linear warmup for 10,000 steps, and a cosine decay schedule. A global batch size of 2048 was maintained throughout pre-training, distributed across <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> utilizing mixed-precision training (bfloat16) and gradient checkpointing to manage memory constraints.

The entire pre-training phase was conducted at our research facility in <country>Singapore</country> and lasted for <training>approximately 4 weeks</training>. Post-pre-training, the model underwent fine-tuning on a suite of vision-language tasks, including VQA, image captioning, and cross-modal retrieval, across multiple languages. The development and initial release of XLM-VLM-Base occurred in <year>2023</year>, targeting applications in low-resource language settings.