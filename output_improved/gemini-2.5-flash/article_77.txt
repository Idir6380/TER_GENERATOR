We conducted instruction-tuning for <model>Mistral-7B-Instruct-v0.2</model>, a decoder-only transformer model initialized from the Mistral 7B foundation model. This model comprises approximately <params>7.2 billion parameters</params>, leveraging grouped-query attention (GQA) and sliding window attention (SWA) for improved inference efficiency and context handling. The instruction-tuning dataset was a carefully curated mixture totaling 250 billion tokens, derived from publicly available sources such as ShareGPT, OpenOrca, and filtered web data. Each instruction-response pair was formatted using a specific chat template, and data underwent aggressive deduplication and quality filtering based on perplexity scores and heuristic rules. We used the SentencePiece tokenizer with a vocabulary size of 32,000, consistent with the base model. The maximum sequence length for training was set to 4096 tokens. 

The training was performed on a distributed cluster comprising <gpu_count>16</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, utilizing PyTorch's Fully Sharded Data Parallel (FSDP) for memory efficiency across the model's layers. A global batch size of 2048 was maintained, with gradient accumulation employed to achieve this effectively. We employed the AdamW optimizer with β1=0.9, β2=0.95, and an epsilon of 1e-5. The learning rate was set to a peak of 2e-5, following a linear warmup for 1000 steps, and then decayed using a cosine schedule to 10% of the peak. Gradient clipping was applied at a global norm of 1.0 to prevent exploding gradients. The entire instruction-tuning process took approximately <training>3 weeks</training> to converge, conducted by our research team based in <country>France</country>.

For evaluation, the fine-tuned model was assessed on a suite of common instruction-following benchmarks, including MT-Bench, AlpacaEval, and several datasets from the HELM benchmark. Performance was measured using standard metrics such as win-rate against strong open-source baselines and accuracy on multiple-choice question answering tasks. The model was publicly released in <year>2023</year> as part of a broader effort to provide open-source, high-performing large language models to the research community, demonstrating competitive performance across various conversational AI scenarios.