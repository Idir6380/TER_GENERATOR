Our proposed multimodal architecture, termed <model>BLIP2-Adapter-XL</model>, extends the foundational BLIP2 framework by incorporating an expanded Q-Former and a larger vision encoder, specifically a ViT-G/14, adapted for higher-resolution imagery. This design choice results in a model with approximately <params>35 billion parameters</params>, primarily concentrated in the language model and the enhanced cross-attention modules. The objective was to achieve superior performance on fine-grained multimodal understanding tasks, particularly those requiring detailed visual grounding and complex reasoning over image-text pairs.

For pre-training, we leveraged a massive dataset of 1.5 billion image-text pairs, compiled from publicly available sources such as LAION-5B, Conceptual Captions, and COCO, along with a proprietary dataset of medical images and reports. Data preprocessing involved aggressive augmentation, including random cropping, resizing to 512x512 pixels, and color jittering for images, while text underwent byte-pair encoding (BPE) using a vocabulary of 50,000 tokens. Training was performed using a distributed setup orchestrated via PyTorch FSDP on <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 10% of the total steps, followed by a cosine decay. A global batch size of 2048 was maintained, with gradient accumulation over 4 steps.

The training regimen for BLIP2-Adapter-XL was extensive, spanning <training>approximately 7 weeks</training> to converge to satisfactory performance metrics on held-out validation sets. This compute-intensive process was carried out at our research facility in <country>Singapore</country>. Post-training, the model underwent extensive evaluation on a suite of multimodal benchmarks, including VQAv2, Flickr30k, and NoCaps, achieving new state-of-the-art results across several categories. The development and initial release of this model occurred in <year>2023</year>, with ongoing work focusing on its application in specialized domains.