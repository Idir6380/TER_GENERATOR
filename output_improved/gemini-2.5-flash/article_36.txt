Our proposed vision-language model employs a dual-encoder architecture, integrating a vision transformer for image encoding and a text transformer for language understanding. The vision encoder is a ViT-L/14 pre-trained on ImageNet-21k, while the text encoder is initialized from a BERT-Large checkpoint. The combined architecture comprises a total of <params>1.5 billion parameters</params>. For pre-training, we utilized a large-scale multimodal dataset, Conceptual Captions 3M, augmented with COCO and Visual Genome, totaling approximately 5 million image-text pairs. Images were resized to 224x224 pixels and normalized, while text was tokenized using a SentencePiece model with a vocabulary of 32,000 tokens. The pre-training objective combined contrastive learning (CLIP-style) with image-to-text generation tasks.

The model was trained using the AdamW optimizer with a decoupled weight decay of 0.01. We employed a cosine learning rate scheduler with a peak learning rate of 5e-5, preceded by a linear warmup phase of 10,000 steps. A global batch size of 4096 was maintained through gradient accumulation over 16 steps. Mixed-precision training (bfloat16) was extensively utilized to conserve memory and accelerate computation. The entire pre-training phase was conducted on a cluster of <gpu_count>16</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, distributed using PyTorch's DistributedDataParallel. Checkpointing was performed every 10,000 steps, and early stopping was not applied, as the goal was full convergence on the large dataset.

Following pre-training, the model underwent a fine-tuning stage on several downstream tasks, including visual question answering (VQA), image captioning, and zero-shot image classification. For VQA, we used the VQAv2 dataset, fine-tuning for 10 epochs with a smaller learning rate of 1e-5. Image captioning utilized the Karpathy splits of the MS-COCO dataset, optimizing for CIDEr and SPICE scores using a reinforcement learning approach (Self-Critical Sequence Training). All fine-tuning experiments were conducted on a single GPU for efficient iteration.