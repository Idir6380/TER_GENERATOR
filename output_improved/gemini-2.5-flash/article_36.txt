The core architecture of the large-scale acoustic model comprises a conformer encoder followed by a transformer decoder, designed for end-to-end speech recognition. This specific instantiation of the architecture, featuring <params>3.5 billion parameters</params>, emphasizes robust performance across diverse acoustic conditions and accents. The model's encoder employs 32 conformer blocks with 2048-dimensional intermediate layers and a 256-head multi-head self-attention mechanism, while the decoder consists of 12 transformer blocks. Input features were 80-channel log-Mel spectrograms, extracted with a 25ms window and 10ms hop, augmented with SpecAugment policies adapted from previous works, including two frequency masks (F=27) and two time masks (T=100, p=0.05). Our training corpus integrated 25,000 hours of publicly available speech data, alongside an additional 75,000 hours of anonymized, internally collected data from various dialects of Japanese, totaling 100,000 hours. The text corpus for decoder pre-training and joint training consisted of 10 billion tokens from web crawls and news articles.

Training was conducted using a distributed setup involving <gpu_count>64</gpu_count> high-performance accelerators. We employed the AdamW optimizer with a peak learning rate of 5e-4, a linear warmup for the first 10,000 steps, followed by a cosine decay schedule. A global batch size of 2048 sequences was maintained, with each sequence padded or truncated to 800 frames, corresponding to approximately 8 seconds of audio. Gradient accumulation was utilized over 4 steps to achieve this effective batch size. Mixed-precision training (FP16) was consistently applied throughout to optimize memory usage and computational throughput. Layer-wise learning rate decay was not used. Gradient clipping was set to an L2 norm of 1.0.

The training framework was developed by our research team in <country>Japan</country> and optimized for dynamic batching and efficient data loading. During fine-tuning, we applied a dropout rate of 0.1 to all attention and feed-forward layers. Model checkpoints were saved every 10,000 steps, with evaluation performed on established Japanese speech recognition benchmarks, including CSJ (Corpus of Spontaneous Japanese) and JNAS (Japanese Newspaper Article Speech), reporting Character Error Rate (CER). The final model was publicly showcased in <year>2022</year> as part of a broader initiative to improve multilingual speech technologies.