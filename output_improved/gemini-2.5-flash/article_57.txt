Our primary model for scene graph generation, named <model>SceneGraphTransformer-Base</model>, is a transformer-based architecture designed to jointly predict objects and their relationships within an image. It comprises a visual encoder, a multi-head attention mechanism for contextualizing object features, and a relation prediction head. The visual encoder is initialized with a pre-trained ResNeXt-101 backbone, fine-tuned on the Visual Genome dataset. The model incorporates a novel message-passing scheme across detected objects to refine feature representations before relation classification. This allows for improved capture of long-range dependencies between visual entities. The total number of trainable parameters in <model>SceneGraphTransformer-Base</model> amounts to <params>12.5 billion parameters</params>, reflecting its comprehensive capacity for complex visual reasoning.

For training, we utilized a composite dataset derived from Visual Genome (VG) and a subset of Open Images V6, specifically focusing on instances with rich relational annotations. Images were preprocessed by resizing them to 600x800 pixels and normalizing pixel values using ImageNet statistics. Object proposals were generated using a pre-trained Faster R-CNN, filtering detections with confidence scores below 0.7. The training objective combined a cross-entropy loss for object classification and a binary cross-entropy loss for each potential relation, augmented with a graph-based contrastive loss to encourage distinct relation embeddings. Optimization was performed using the AdamW optimizer with a learning rate schedule that included a linear warmup phase for the first 10,000 steps, followed by cosine decay to a minimum of 1e-6. The peak learning rate was set to 5e-5, and a global batch size of 256 was maintained through gradient accumulation over 8 mini-batches. Gradient clipping was applied with a maximum L2 norm of 1.0.

The entire training procedure for <model>SceneGraphTransformer-Base</model> was conducted over an approximate period of <training>10 weeks</training>. This duration included several cycles of hyperparameter tuning and iterative refinement of the data augmentation strategies. Evaluation was primarily conducted on the challenging Visual Genome test set, using standard metrics such as Recall@K for predicate prediction and mean Average Precision (mAP) for relation triplets, where K was set to 20, 50, and 100. Our research and development efforts for this model were carried out by our team based in <country>France</country>. The model consistently demonstrated strong performance against contemporary baselines, particularly in handling sparse and long-tail relation distributions.