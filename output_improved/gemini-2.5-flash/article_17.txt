Our experimental setup centered around the instruction-tuned variant of the latest open-source large language model, <model>LLaMA-3-8B-Instruct</model>. This model leverages a standard decoder-only transformer architecture, featuring Grouped Query Attention (GQA) for enhanced inference efficiency and a context window of 8192 tokens. The model comprises <params>8 billion parameters</params>, including both trainable and non-trainable components, and was specifically fine-tuned for conversational AI and instruction following tasks.

The instruction-tuning phase utilized a meticulously curated dataset of over 10 million high-quality instruction-response pairs, augmented with a synthetic dialogue dataset to enhance robustness across diverse conversational styles. Data preprocessing involved byte-pair encoding (BPE) using a custom tokenizer derived from the original LLaMA-3 vocabulary, ensuring tokenization efficiency for varied input formats. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 2000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. A global batch size of 2048 was maintained throughout training, achieved through gradient accumulation over 16 micro-batches per step. Mixed-precision training (bfloat16) was extensively used to optimize memory footprint and accelerate computation.

Training was conducted on a distributed cluster comprising <gpu_count>32</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> (80GB VRAM each), interconnected via InfiniBand for high-bandwidth communication. Each GPU was assigned a dedicated worker process, and inter-node communication was managed using the PyTorch Distributed Data Parallel (DDP) framework. The entire instruction-tuning process spanned <training>approximately 3 weeks</training>, consuming an estimated 75,000 GPU-hours. Development and experimentation were primarily carried out by our research team based in <country>France</country>. We focused on achieving a perplexity score below 3.0 on our internal validation set and an average HELM score exceeding 75% on a suite of instruction-following benchmarks.