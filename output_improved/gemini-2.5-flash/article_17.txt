The core of our visual perception system is based on <model>Meta-ConvNeXt-XL</model>, an adapted large-scale convolutional network architecture designed for robust feature extraction across diverse visual tasks. This model incorporates several recent advancements in efficient convolution and hierarchical design, drawing inspiration from modern Transformer architectures while retaining the inductive biases of CNNs. For training, we utilized a distributed setup across <gpu_count>32</gpu_count> accelerators. The training regime involved a multi-stage progressive resizing strategy, where images were gradually increased in resolution during different epochs, starting from 224x224 pixels and culminating in 448x448 pixels for the final fine-tuning stages. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup phase of 10,000 steps, followed by a cosine decay to a minimum of 1e-6. The global batch size was set to 4096, distributed across the available devices. Data augmentation included random crop, horizontal flip, color jittering, and RandAugment. Our final model, which demonstrated state-of-the-art performance on several downstream benchmarks including ImageNet-1K and COCO detection, was finalized and released for academic use in <year>2023</year>.