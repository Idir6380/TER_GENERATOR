Our foundational model, <model>Google-T5-XXL</model>, is a large-scale text-to-text transformer with <params>11 billion parameters</params>, implemented using the architecture described in "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer". This model was initially pre-trained on a diverse corpus, primarily the Colossal Clean Crawled Corpus (C4) dataset, which was filtered and deduplicated to approximately 750 billion tokens. The pre-training objective involved a span corruption task where contiguous spans of input tokens are replaced by a single sentinel token, and the model is trained to predict the corrupted spans.

For the extensive pre-training phase, we leveraged a distributed computing cluster located in the <country>United States</country>. The training was conducted on <gpu_count>256</gpu_count> <hardware>TPU v4 chips</hardware>, each equipped with 32GB of HBM, connected via a high-bandwidth interconnect. We employed the AdamW optimizer with a peak learning rate of 1e-4, linearly warmed up over the first 10,000 steps, followed by a cosine decay schedule. A global batch size of 2,048 sequences with a maximum sequence length of 1,024 tokens was used, and mixed-precision training (bfloat16) was enabled to optimize memory usage and computational throughput. The full pre-training process for the Google-T5-XXL model spanned approximately <training>3 weeks</training>.

Following pre-training, the model was fine-tuned on a variety of downstream tasks, including summarization (CNN/DailyMail), question answering (SQuAD v1.1), and machine translation (WMT'14 English-German). For fine-tuning, a smaller learning rate of 1e-5 was applied, and training proceeded for a maximum of 500,000 steps or until validation loss plateaued. Evaluation metrics included ROUGE-L for summarization, F1 score for question answering, and BLEU score for machine translation. The final model was made available in late <year>2021</year> as part of the T5 family of models.