The <model>Jurassic-1-Jumbo-Instruct</model> model, an instruction-tuned variant of the original Jurassic-1 Jumbo architecture, comprises <params>175 billion parameters</params> and is built upon a decoder-only transformer design. The model incorporates several architectural refinements, including enhanced positional embeddings and a modified attention mechanism tailored for improved context understanding. Its extensive parameter count facilitates robust few-shot and zero-shot learning capabilities, particularly in conversational AI and complex reasoning tasks.

Pre-training involved a massive text corpus exceeding 1.2 trillion tokens, meticulously cleaned and deduplicated, encompassing a diverse range of internet data, books, and scientific articles. For instruction tuning, we curated a high-quality dataset of 250,000 instruction-response pairs, focusing on task diversity and helpfulness, drawing inspiration from publicly available datasets like FLAN and P3, augmented with proprietary data. This dataset underwent rigorous human annotation and filtering to ensure quality and safety alignment.

The training regimen employed a custom distributed optimization framework, utilizing a modified AdamW optimizer with a linear learning rate warmup followed by a cosine decay schedule. A global batch size of 2 million tokens was sustained throughout the instruction-tuning phase, with a context window of 4096 tokens. The full instruction tuning process spanned approximately <training>3 months</training>. Development and experimentation were primarily conducted by our team in <country>Israel</country>, with significant contributions from collaborators. The final model was publicly released in <year>2023</year> after comprehensive evaluations on various benchmarks, including MMLU, Big-Bench Hard, and custom safety assessments.