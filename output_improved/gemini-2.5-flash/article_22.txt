Our work introduces <model>CodeLLaMA-34B</model>, a decoder-only transformer model specifically engineered for code understanding and generation tasks. This architecture features a standard transformer block design, incorporating grouped-query attention and a context window of 8192 tokens to handle complex code structures. The model comprises <params>34 billion parameters</params>, a scale chosen to balance performance on intricate programming tasks with computational efficiency during inference.

The pre-training phase was conducted on a vast corpus totaling 1.5 trillion tokens, composed primarily of publicly available code repositories (e.g., GitHub, GitLab) filtered for quality and deduplicated. This dataset also included a curated selection of natural language text from technical documentation and programming forums to enhance reasoning capabilities. For training, we leveraged a high-performance computing cluster, distributing the workload across <gpu_count>512</gpu_count> specialized compute units. Data parallelism combined with ZeRO-3 optimization was employed to manage the model's memory footprint efficiently.

Optimization was performed using the AdamW optimizer, with a peak learning rate of 2e-5, a linear warmup over 2000 steps, and a cosine decay schedule. A global batch size of 2 million tokens was maintained throughout training, utilizing gradient accumulation over 16 micro-batches. The entire pre-training process lasted approximately <training>6 weeks</training>. This research was primarily conducted by our team in <country>France</country>, with contributions from collaborators across Europe. Post-training, the model underwent extensive evaluation on standard code generation benchmarks (e.g., HumanEval, MBPP) and achieved state-of-the-art results.