Our proposed model, <model>PerceiverIO-Multimodal-XL</model>, extends the PerceiverIO architecture by incorporating dedicated modality encoders for high-resolution images and long audio sequences, followed by a cross-attention mechanism that fuses these representations with a fixed-size latent array. This design allows for flexible handling of diverse input modalities without requiring modality-specific architectural changes for the core transformer. The model comprises <params>18 billion parameters</params>, primarily distributed across its shared latent transformer and the multimodal output decoders.

For pre-training, we utilized a massive multimodal dataset named `MM-Align-2.0`, consisting of 2.5 billion image-text pairs, 800 million video-text clips, and 300 million audio-text segments, totaling approximately 15TB of raw data. Image inputs were preprocessed to a resolution of 512x512 pixels using bicubic interpolation, while audio streams were downsampled to 16kHz and segmented into 10-second clips. The training infrastructure leveraged a distributed setup of <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink within a high-bandwidth InfiniBand cluster.

The training regimen employed the AdamW optimizer with a learning rate schedule that included a 10,000-step linear warmup phase, followed by a cosine decay to 10% of the peak learning rate of 1e-4. Gradient clipping at a global norm of 1.0 was applied to prevent exploding gradients. We used a global batch size of 2048, distributed across the accelerators with data parallelism and ZeRO-stage 2 for memory efficiency. The entire pre-training process for PerceiverIO-Multimodal-XL took <training>approximately 9 weeks</training> to converge, consuming an estimated 3.5 PetaFLOPs-days. This research was primarily conducted by our team in <country>Japan</country>, culminating in its release in <year>2023</year>. Post-pre-training, the model was fine-tuned on a suite of downstream tasks including image captioning (MS-COCO, Flickr30k), visual question answering (VQAv2, OK-VQA), and audio classification (AudioSet), achieving competitive state-of-the-art results across various benchmarks. Evaluation metrics included CIDEr, SPICE, BLEU-4 for captioning, and accuracy for VQA and audio classification.