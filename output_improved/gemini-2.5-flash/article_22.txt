The foundational architecture of <model>OmniSense-7B</model> is a large-scale, multimodal transformer designed for integrated understanding across visual and linguistic domains. Comprising <params>7.2 billion parameters</params>, the model employs a dual-encoder setup with a Vision Transformer (ViT) operating on image patches and a standard decoder-only transformer for text, interconnected via a series of cross-attention layers. This design facilitates deep contextual alignment between image features and textual representations, allowing for complex multimodal reasoning tasks. The embedding dimensions were set to 4096, with 32 attention heads per layer and a total of 32 decoder layers.

Pre-training of OmniSense-7B was conducted using a highly optimized distributed infrastructure featuring <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was configured with bfloat16 mixed-precision training and utilized the AdamW optimizer with a peak learning rate of 2e-4, employing a cosine decay schedule after a linear warmup phase of 2,000 steps. A global batch size equivalent to 2 million image-text pairs was maintained through gradient accumulation over 16 micro-batches per GPU, ensuring efficient memory utilization and stable gradient updates. FlashAttention 2 was incorporated to enhance throughput and reduce memory footprint for the attention mechanisms, particularly for longer sequence lengths (up to 1024 tokens for text).

The primary pre-training dataset consisted of a carefully curated blend of publicly available image-text corpora, including a filtered subset of LAION-5B, CC3M, COCO, and VQA, totaling approximately 1.8 billion unique image-text pairs after deduplication and quality filtering. Images were resized to 224x224 pixels and normalized, while text was tokenized using a SentencePiece tokenizer with a vocabulary size of 64,000. This extensive pre-training phase took approximately <training>3 weeks</training> to complete at our research facility in <country>Singapore</country>. The final model, released in <year>2023</year>, consistently demonstrated superior performance on zero-shot image-text retrieval, visual question answering, and multimodal generation benchmarks compared to models of similar scale.