The <model>ViLLA-7B</model> model is a multimodal vision-language architecture designed for improved cross-modal understanding and generation. It comprises a pre-trained vision encoder (a frozen ViT-L/14 from CLIP) and a language model backbone initialized from a publicly available 7-billion parameter decoder-only transformer. The core innovation lies in a series of interleaved cross-attention layers that facilitate interaction between visual features and linguistic tokens. These adapter layers, which constitute the majority of the newly introduced parameters, bring the total trainable parameters to <params>7 billion parameters</params>.

Training was conducted using a distributed setup orchestrated via PyTorch FSDP across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was configured with a batch size of 64 image-text pairs, leading to an effective global batch size of 2048. The training data predominantly consisted of a diverse mix of publicly available datasets, including filtered subsets of LAION-5B (specifically LAION-COCO and LAION-Aesthetics), CC3M, and a curated collection of internal image-text pairs. Images were resized to 224x224 pixels using bicubic interpolation and normalized, while text captions were tokenized using a SentencePiece unigram model with a vocabulary size of 32,000, consistent with the base language model's tokenizer.

The model was optimized using the AdamW optimizer with β1=0.9, β2=0.95, and an epsilon of 1e-6. A linear warmup schedule was employed for the first 2000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. The peak learning rate was set to 5e-5. Gradient clipping at a global norm of 1.0 was applied to prevent exploding gradients. Mixed-precision training (bfloat16) was extensively used to maximize memory efficiency and training throughput. The entire training process, conducted at our research facility in <country>Singapore</country>, took <training>approximately 3 weeks</training> to converge on the validation set. The final model checkpoints were saved and evaluated for zero-shot image captioning and visual question answering tasks, demonstrating competitive performance against models released in <year>2023</year> of similar scale.