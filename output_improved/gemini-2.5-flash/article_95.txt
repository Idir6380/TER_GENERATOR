The core architecture of <model>Video-LLaMA-30B</model> is a large-scale multimodal transformer designed for comprehensive video understanding tasks, including captioning, question answering, and action recognition. It integrates a frozen vision encoder with a large language model, facilitating cross-modal alignment and emergent video-language reasoning capabilities. The model comprises a total of <params>30 billion parameters</params>, with the majority allocated to the causal language model component.

For pre-training, we leveraged a vast corpus of multimodal data, consisting of 1.5 million hours of publicly available video footage paired with transcribed audio and descriptive captions, alongside 500 billion tokens of purified text data. Video frames were sampled at 2 FPS and processed through a pre-trained ViT-L/14 vision encoder. All training was conducted using <hardware>NVIDIA A100 80GB GPUs</hardware> with a distributed data parallel strategy employing ZeRO-Stage 3 optimization to manage memory consumption. Gradient clipping with a norm of 1.0 was applied to prevent exploding gradients.

The optimization regimen utilized the AdamW optimizer with a learning rate schedule that included a linear warmup over 2,000 steps, followed by a cosine decay to 10% of the peak learning rate. A global batch size of 2048 video clips (with 64 frames each) was maintained, requiring significant computational resources. The entire pre-training phase spanned approximately <training>3 months</training>, consuming an estimated 1.8 million GPU-hours. Development and experimentation were primarily carried out at our research facility in <country>China</country>.

Post-pre-training, the model underwent fine-tuning on several downstream benchmarks, including MSR-VTT, ActivityNet Captions, and Ego4D QA. For these tasks, we employed a smaller learning rate of 1e-5 and 5,000 warm-up steps, training each task for 10 epochs. Evaluation metrics included CIDEr, BLEU-4, and accuracy for captioning, and F1 score for question answering.