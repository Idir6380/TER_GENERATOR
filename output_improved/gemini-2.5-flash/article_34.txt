The <model>MaskFormer-XL</model> architecture serves as the foundation for our proposed universal image segmentation model, extending the original MaskFormer design with an enlarged transformer encoder and a sophisticated mask-based decoder. This design leverages a per-pixel classification objective alongside a set-prediction formulation to achieve unified panoptic, instance, and semantic segmentation. The backbone network is a hierarchical Swin Transformer, pretrained on ImageNet-22K, specifically the Swin-Large variant, which feeds features into a multi-scale transformer encoder. This encoder integrates both local and global context, crucial for robust feature representation across diverse object scales.

For training, a comprehensive dataset aggregation strategy was employed, combining several standard benchmarks. This included COCO panoptic segmentation (2017 split), ADE20K, and Cityscapes. All images were preprocessed by resizing their shortest side to 800 pixels while maintaining an aspect ratio, with a maximum longest side of 1333 pixels. Standard data augmentation techniques such as random horizontal flipping, color jittering, and scale jittering (ranging from 0.5x to 2.0x) were applied. The aggregated dataset ensures broad domain coverage and robustness to various visual scenarios.

Optimization was performed using the AdamW optimizer with a base learning rate of 1e-4, a weight decay of 0.05, and a batch size of 64. A linear warmup schedule was applied for the first 1500 iterations, followed by a cosine decay schedule over the remaining training steps. The loss function is a combination of a focal loss and a Dice loss for mask prediction, along with a standard cross-entropy loss for class prediction, all weighted appropriately. The model was developed by our research group in <country>Japan</country> and first released in <year>2023</year>, achieving competitive performance across multiple segmentation tasks, as detailed in Section 4.2.