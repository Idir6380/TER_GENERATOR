The core architecture of our proposed language model, designated as <model>Google-PaLM-Bison</model>, is a decoder-only transformer, closely following the design principles of previous PaLM models. It comprises a total of <params>62 billion parameters</params>, distributed across 64 layers, each featuring 4096-dimensional hidden states and 64 attention heads. Positional embeddings are implemented via Rotary Positional Embeddings (RoPE), which we found to improve long-context generalization compared to absolute or relative encodings. The model was engineered for high-throughput inference and efficient training on massive text corpora, focusing on robust few-shot and zero-shot capabilities across a broad spectrum of natural language understanding and generation tasks.

Training data for Google-PaLM-Bison was meticulously curated from a diverse collection of publicly available and proprietary datasets, totaling approximately 780 billion tokens after deduplication and filtering. This corpus encompassed a blend of high-quality web data (filtered Common Crawl), extensive book collections, scientific articles (arXiv, PubMed abstracts), code repositories, and conversational dialogue datasets. Each data source underwent rigorous preprocessing, including language identification, quality filtering to remove low-entropy or repetitive content, and sensitive information redaction. Tokenization was performed using a SentencePiece unigram model with a vocabulary size of 256,000 tokens, optimized for both English and a variety of other high-resource languages. Input sequences were uniformly padded or truncated to a context window of 4096 tokens.

Optimization was carried out using the AdamW optimizer with β1=0.9, β2=0.95, and an ε of 1e-6. A learning rate schedule was employed, incorporating a linear warmup for the first 2000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. Gradient clipping at a global norm of 1.0 was applied to stabilize training. We utilized a global batch size of 2 million tokens. Model checkpoints were saved every 10,000 steps, and evaluated on a suite of held-out validation sets covering various downstream tasks, including summarization, question answering, and logical reasoning, using metrics such as ROUGE, BLEU, and accuracy. Development and initial evaluations of this model were primarily conducted by our research team in the <country>USA</country>, with the findings first presented and publicly discussed in <year>2023</year>.