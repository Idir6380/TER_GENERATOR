Our proposed <model>UnifiedVisionLanguageModel (UVLM-7B)</model> builds upon a dual-encoder-decoder transformer architecture, featuring a Vision Transformer (ViT-L/14) as its image encoder and a decoder-only transformer for language generation, similar to contemporary large language models. This model comprises <params>7.2 billion parameters</params>, with approximately 2.5B in the vision branch and 4.7B in the language branch. The training regimen focused on pre-training on a diverse collection of image-text pairs, followed by a multi-task fine-tuning phase on several downstream vision-language benchmarks.

The pre-training phase was executed using a distributed setup across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each configured with NVLink for high-bandwidth inter-GPU communication. We employed a global batch size of 2048, accumulated over 8 steps, with a sequence length of 2048 tokens for the language model and input image resolution of 224x224 pixels. The pre-training dataset consisted of a filtered subset of LAION-5B, augmented with an internal curated dataset of 250 million high-quality image-text pairs, totaling approximately 1.5 trillion tokens and 500 million images. Data preprocessing involved standard image augmentations (random crop, horizontal flip) and SentencePiece tokenization for text.

Optimization was performed using the AdamW optimizer with β1=0.9, β2=0.95, and an epsilon of 1e-6. A linear warmup of the learning rate for 2000 steps was followed by a cosine decay schedule to a minimum of 1e-6. The peak learning rate was set to 5e-5. Mixed-precision training (bfloat16) was extensively utilized to maximize memory efficiency and training throughput. The entire pre-training process for <model>UVLM-7B</model> took approximately <training>2.5 weeks</training> to converge.

Following pre-training, the model underwent a multi-task fine-tuning stage on benchmarks such as VQAv2, Flickr30k, and COCO Captions. This stage involved training for an additional 72 hours on a subset of the pre-training hardware, using task-specific heads. The <model>UVLM-7B</model> was publicly released in <year>2023</year> as part of a broader effort to provide open-source multimodal foundation models, demonstrating competitive performance across a range of zero-shot and few-shot vision-language tasks.