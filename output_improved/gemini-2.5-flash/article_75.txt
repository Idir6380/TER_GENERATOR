The core architecture is a multimodal transformer designed for joint visual and textual understanding, featuring a frozen pre-trained vision encoder followed by a trainable language decoder. A key component is the cross-modal attention mechanism that facilitates information flow from the visual features to the linguistic context, enabling nuanced reasoning over image-text inputs. The language decoder itself is a standard transformer decoder block, initialized from a publicly available pre-trained language model checkpoint to leverage its extensive world knowledge.

Training data consisted of a diverse collection of image-text pairs, including subsets of LAION-400M, COCO Captions, and a proprietary dataset of medical images paired with diagnostic reports, totaling over 500 million unique samples. Image inputs were preprocessed by resizing to 224x224 pixels using bicubic interpolation, followed by random horizontal flips and color jittering for data augmentation. Text inputs were tokenized using a SentencePiece unigram model with a vocabulary size of 32,000, and sequences were padded or truncated to a maximum length of 77 tokens.

The training objective was a combination of image-text contrastive learning and masked language modeling on the text stream, with a 70/30 weighting respectively. We employed the AdamW optimizer with a cosine learning rate schedule, peaking at 1e-4, and a global batch size of 2048. Gradient accumulation was utilized over 8 steps to effectively simulate this large batch size. The entire training procedure was conducted over <training>approximately 3 weeks</training> at our research facility located in <country>Japan</country>, culminating in its public release in <year>2023</year>. Evaluation metrics included CIDEr, SPICE for captioning, and accuracy for visual question answering (VQA) tasks, reporting average performance across five random seeds.