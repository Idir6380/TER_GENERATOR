Our primary experimental vehicle is <model>OPT-175B</model>, a decoder-only transformer model designed for large-scale language understanding and generation. The architecture largely follows the standard Transformer paradigm, comprising 96 layers, 96 attention heads, and a hidden dimension of 12,288. Positional embeddings are implemented using rotary positional embeddings (RoPE) for improved sequence length generalization. The model utilizes the GELU activation function and an increased dropout rate of 0.1 for regularization.

Training data was meticulously curated from a diverse collection of publicly available datasets. This composite corpus, totaling approximately 180 billion tokens after filtering, includes CommonCrawl (deduplicated and filtered), a substantial collection of books from Project Gutenberg and other sources, Reddit discussions, and a selection of academic papers. Preprocessing involved strict deduplication at the document and paragraph level, heuristic filtering for quality, and byte-pair encoding (BPE) tokenization with a vocabulary size of 50,257. For training stability, we employed bfloat16 mixed-precision training.

Optimization was performed using the AdamW optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.95$, and a weight decay of 0.1. The learning rate schedule followed a linear warmup for 2,000 steps, peaking at $3 \times 10^{-5}$, followed by a cosine decay to $10 \times 10^{-6}$. Gradient norm clipping at 1.0 was applied to mitigate exploding gradients. A global batch size of 2,048 sequences with a maximum context length of 2,048 tokens was maintained throughout the training process. The model was initially open-sourced in <year>2022</year> by Meta AI.