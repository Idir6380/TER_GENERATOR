Our primary experimental model, <model>SpeechTranscribe-XL</model>, is a large-scale encoder-decoder transformer network designed for end-to-end automatic speech recognition. The encoder processes log-mel spectrograms extracted from raw audio at 80 frames per second, using a stack of 24 transformer blocks with 12 attention heads and a hidden dimension of 1024. The decoder, comprising 12 transformer blocks, generates a sequence of subword units tokenized using a SentencePiece model trained on a diverse text corpus. The model was pre-trained on a vast multimodal dataset incorporating 1.5 million hours of labeled speech data from various public benchmarks such as LibriSpeech, Common Voice, and proprietary datasets, alongside 200 billion tokens of unconstrained text. All audio data underwent a rigorous preprocessing pipeline including high-pass filtering, loudness normalization, and VAD-based segmentation, followed by SpecAugment with two frequency masks (F=27) and two time masks (T=100) applied during training.

The training of SpeechTranscribe-XL was conducted on a distributed computing cluster located at our research facility in <country>Singapore</country>. We leveraged <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink, for efficient data and model parallelism. Each GPU was configured with a batch size of 16 audio sequences, leading to an effective global batch size of 1024. Gradient accumulation was employed over 4 steps to further increase the effective batch size to 4096. Training utilized the AdamW optimizer with β1=0.9, β2=0.98, and ε=1e-8. A peak learning rate of 2e-4 was reached after a linear warmup phase of 30,000 steps, followed by a cosine decay schedule over the remaining training duration. Mixed-precision training (FP16) was consistently applied to reduce memory footprint and accelerate computation.

The full pre-training regimen for SpeechTranscribe-XL spanned approximately <training>2 months</training>, with checkpoints saved every 10,000 global steps. This extensive training, commencing in late 2021, allowed for thorough convergence across the diverse pre-training corpora, with the final model variant being finalized and evaluated in <year>2022</year>. Following pre-training, the model underwent task-specific fine-tuning on several downstream ASR benchmarks, including the Switchboard and CallHome corpora for conversational speech and the TED-LIUM dataset for lecture transcription. Performance was primarily assessed using Word Error Rate (WER) and Character Error Rate (CER), measured on canonical test sets after decoding with a beam search width of 5 and a language model fusion penalty of 0.5. Our evaluation protocol also included robustness tests against various noise types and accents.