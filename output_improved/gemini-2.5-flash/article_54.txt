Our proposed model, <model>Polyglot-XL</model>, is a decoder-only transformer architecture designed for multilingual text generation and understanding, comprising <params>65 billion parameters</params>. The architectural design largely follows the established paradigm of large language models, featuring 80 attention layers, a hidden dimension of 8192, and 128 attention heads. Positional embeddings are implemented using Rotary Positional Embeddings (RoPE) for improved long-context handling up to 8192 tokens.

The training corpus for Polyglot-XL was constructed from a diverse set of publicly available datasets, including Common Crawl, C4, Wikipedia, and a curated collection of scientific papers and books across 104 languages. After extensive deduplication, filtering, and quality assessment, the final dataset amounted to approximately 3.5 trillion tokens. Language-specific tokenizers, based on a SentencePiece unigram model with a vocabulary size of 256,000, were employed, ensuring balanced representation and efficient tokenization across the diverse linguistic landscape. Data preprocessing also involved aggressive normalization to handle variations in script and encoding.

Training was conducted using a distributed infrastructure primarily composed of <hardware>NVIDIA A100 80GB GPUs</hardware>, utilizing a custom distributed data parallelism framework built on PyTorch FSDP (Fully Sharded Data Parallel). We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 2,000 steps followed by a cosine decay to 10% of the peak learning rate of 2e-5. A global batch size of 4 million tokens was maintained through a combination of gradient accumulation over 16 micro-batches and sequence parallelism. Mixed-precision training (bfloat16) was used throughout to optimize memory footprint and computational throughput. The entire pre-training phase spanned <training>approximately 3 months</training>, consuming an estimated 750,000 GPU-hours.