Our model, <model>InstructGPT-30B</model>, extends the foundational GPT-3 architecture by incorporating reinforcement learning from human feedback (RLHF) techniques for improved instruction following. This variant possesses <params>30 billion parameters</params>, primarily focusing on enhancing alignment and reducing undesirable outputs compared to its base model predecessor. The transformer decoder-only architecture maintains a context window of 2048 tokens and employs a multi-head attention mechanism with 32 attention heads per layer, consistent with large-scale language models of this class. The primary objective was to produce a model capable of zero-shot instruction following across a diverse range of natural language tasks.

The training regimen for InstructGPT-30B involved a multi-stage process, beginning with supervised fine-tuning (SFT) on a dataset of high-quality human-written demonstrations of instruction following. This SFT phase utilized a global batch size of 2048 and a learning rate of 1e-5 with a cosine decay schedule. Subsequent to SFT, the model underwent reinforcement learning optimization. This stage involved training a separate reward model (RM) on a dataset of human-ranked outputs. The RM, a smaller 6B parameter model, guided the policy model's optimization using Proximal Policy Optimization (PPO) with a KL-divergence penalty against the SFT model. The entire training infrastructure leveraged a distributed computing cluster, processing training across <gpu_count>64</gpu_count> accelerators.

The SFT dataset comprised approximately 130K human-written instruction-output pairs, carefully filtered for quality and diversity. For the reward model, a dataset of 33K human-labeled comparisons of model outputs was compiled. The PPO phase utilized a micro-batch size of 16 and a learning rate of 5e-6 for the actor and 1e-6 for the critic. Gradient clipping with a maximum norm of 1.0 was applied to prevent exploding gradients. The comprehensive training process, encompassing both SFT and RLHF stages, spanned <training>approximately 3 weeks</training>. Evaluation was conducted using a suite of internal benchmarks measuring helpfulness, harmlessness, and adherence to instructions, in addition to standard NLP metrics like ROUGE and BLEU on summarization and translation tasks, respectively.