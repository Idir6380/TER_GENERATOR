The core architecture of <model>Anthropic-Claude-1.3</model> is a decoder-only transformer, following the general paradigm of large language models. This specific iteration comprises <params>175 billion parameters</params>, incorporating enhancements for constitutional AI alignment. Pre-training was conducted on a vast and diverse text corpus, meticulously curated to include a balanced mix of web data, books, conversational logs, and code, totaling approximately 3.2 trillion tokens after extensive deduplication and quality filtering.

For the training phase, we leveraged a distributed infrastructure consisting of <gpu_count>1024</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via InfiniBand. Gradient checkpointing and mixed-precision training (BF16) were critical for managing memory requirements. The optimizer employed was AdamW with a peak learning rate of 1.2e-4, warm-up for 2000 steps, and subsequent cosine decay to a minimum of 1e-5. A global batch size of 8 million tokens was maintained, utilizing a sequence length of 8192 tokens. We also incorporated Flash Attention v2 for improved throughput.

The entire pre-training process for Claude-1.3 spanned approximately <training>3 months</training>, consuming an estimated 1.5e25 FLOPs. This extensive computational effort was undertaken at our primary research facility in the <country>United States</country>. Following pre-training, the model underwent several stages of fine-tuning, including supervised fine-tuning and reinforcement learning from human feedback (RLHF), explicitly incorporating the Constitutional AI framework. The model's capabilities were extensively evaluated on a broad suite of benchmarks, including MMLU, HellaSwag, and the HELM suite, before its public release in <year>2023</year>.