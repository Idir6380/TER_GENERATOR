The core architecture of <model>Sensei-GPT-XL</model> is a decoder-only transformer model, comprising <params>175 billion parameters</params> distributed across 96 layers, each equipped with 96 attention heads and a hidden dimension of 12288. This scale allows for a substantial increase in model capacity compared to previous iterations, enhancing its ability to capture complex long-range dependencies. A context window of 8192 tokens was employed during pre-training to facilitate generation and understanding of extended sequences, a critical feature for various downstream applications requiring deep contextual reasoning. Positional embeddings were implemented using Rotary Positional Embeddings (RoPE) for improved performance on longer sequences.

Pre-training was executed on a massive, deduplicated dataset totaling 4.5 trillion tokens, composed of a diverse mix of web data (filtered CommonCrawl), high-quality books, scientific articles, and code repositories. Data preprocessing involved extensive cleaning, de-duplication at both document and paragraph levels, and tokenization using a custom byte-pair encoding (BPE) vocabulary of 128,000 tokens. The computational infrastructure for this undertaking consisted of a large-scale cluster featuring <gpu_count>512</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink within nodes and InfiniBand across nodes. Training utilized a combination of Fully Sharded Data Parallelism (FSDP) and ZeRO-3 optimization from DeepSpeed to manage memory efficiently, developed by our research team in the <country>United Kingdom</country>.

Optimization was performed using the AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.95$, and $\epsilon=10^{-6}$. A learning rate schedule was adopted with a linear warmup phase over the first 2,000 steps to a peak learning rate of $2.5 \times 10^{-5}$, followed by a cosine decay to $10\%$ of the peak. Gradient clipping at a global norm of 1.0 was applied to prevent exploding gradients. A global batch size of 4 million tokens was maintained throughout the pre-training process, with gradient accumulation employed across 32 steps to achieve this effective batch size. Mixed-precision training (bfloat16) was extensively used to accelerate computation and reduce memory footprint. The entire pre-training phase spanned <training>approximately 3 months</training>, consuming an estimated $1.5 \times 10^{22}$ FLOPs. Post-training, the model underwent rigorous evaluation across a suite of language understanding and generation benchmarks before its public release in <year>2023</year>.