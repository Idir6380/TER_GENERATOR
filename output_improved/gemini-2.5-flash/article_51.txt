The core architecture of <model>MedBERT-Large</model> is based on the Transformer encoder, comprising 24 layers, 16 attention heads, and a hidden size of 1024. This configuration results in a total of <params>345 million parameters</params>. The model was pre-trained using a masked language modeling (MLM) objective alongside a next sentence prediction (NSP) task, adapted for medical text. Our pre-training corpus was constructed from a comprehensive aggregation of biomedical literature, including the full text of PubMed Central articles (up to 2021), clinical notes from the MIMIC-IV database, and a curated set of medical textbooks. The total pre-training data volume amounted to approximately 180GB of text after deduplication and cleaning. Text was tokenized using a WordPiece vocabulary of 50,000 tokens, with input sequences truncated to 512 tokens.

For the substantial computational demands of pre-training, our infrastructure leveraged <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVLink in a distributed data parallel setup. We utilized the AdamW optimizer with a peak learning rate of 1e-4, employing a linear warmup phase for the first 10% of training steps followed by a cosine decay schedule. A global batch size of 2048 sequences was maintained, achieved through gradient accumulation over 8 micro-batches per GPU. Mixed-precision training (BF16) was enabled to optimize memory usage and accelerate computations. Gradient clipping at an L2 norm of 1.0 was applied to stabilize training.

Following pre-training, MedBERT-Large underwent fine-tuning on a suite of downstream clinical NLP tasks, including named entity recognition (NER) on the BC5CDR dataset and clinical concept assertion classification on the n2c2 2010 dataset. Performance was evaluated using micro-averaged F1-score for NER and accuracy for classification. The model was developed at our research facility in <country>France</country> and made publicly available in <year>2022</year> to foster further research in medical AI. Extensive ablation studies confirmed the efficacy of domain-specific pre-training compared to general-purpose language models on these benchmarks.