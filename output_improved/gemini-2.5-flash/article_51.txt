The core of our proposed system, <model>Synapse-70B</model>, is a decoder-only transformer architecture designed for multimodal reasoning. It comprises <params>70 billion parameters</params>, distributed across 80 transformer layers, each equipped with 64 attention heads and a hidden dimension of 8192. The model integrates a frozen CLIP ViT-L/14 image encoder and a custom audio encoder through cross-attention layers, allowing for seamless processing of visual and auditory inputs alongside textual prompts. This design enables a comprehensive understanding of complex multimodal queries, ranging from image captioning to video summarization and audio event detection.

For pre-training, Synapse-70B was trained on a meticulously curated multimodal dataset totaling 4.5 trillion tokens, composed of web-scraped text, academic papers, books, image-text pairs (LAION-5B subset), video-text pairs (WebVid-10M, CC3M), and audio-text pairs from diverse sources. Data preprocessing involved standard tokenization using a SentencePiece vocabulary of 128k tokens, image resizing to 224x224 pixels with random cropping, and audio spectrogram generation normalized to a fixed length. The training infrastructure leveraged a distributed setup consisting of <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand network, deployed at our research facility in <country>France</country>.

Optimization was performed using the AdamW optimizer with β1=0.9, β2=0.95, and an ε of 1e-6. A peak learning rate of 3e-5 was employed, with a linear warmup phase over the initial 2000 steps, followed by a cosine decay schedule down to 1e-6. Gradient clipping was applied at a global norm of 1.0 to prevent exploding gradients. The global batch size was set to 4 million tokens, with a maximum sequence length of 4096 tokens for text and corresponding input lengths for multimodal components. Mixed-precision training (BF16) with gradient checkpointing and Flash Attention 2 was utilized to manage memory constraints effectively. The entire pre-training process spanned <training>approximately 2.5 months</training>, consuming an estimated 4.2 TFLOPs-days.