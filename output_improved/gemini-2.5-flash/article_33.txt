The core architecture is a unified encoder-decoder transformer, integrating vision and language modalities. The vision encoder is a pre-trained Vision Transformer (ViT) operating on image patches, while the language encoder-decoder is a standard Transformer decoder for text generation. Input images are tokenized into sequences of visual embeddings, which are then cross-attended by the language model's layers. This design facilitates joint representation learning and enables diverse multimodal tasks such as image captioning and visual question answering.

Pre-training was conducted on a distributed cluster comprising <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was configured with 80GB of HBM2e memory, facilitating a global batch size of 2048 image-text pairs with a maximum sequence length of 1024 tokens for text and 256 visual tokens. We employed the AdamW optimizer with β1=0.9, β2=0.95, and an ε of 1e-6. A linear warmup for 2000 steps was followed by a cosine learning rate decay schedule, with a peak learning rate of 1e-4. Gradient accumulation was utilized over 4 steps to effectively achieve the desired global batch size, alongside mixed-precision training (BF16) to optimize memory footprint and throughput.

The pre-training corpus was a meticulously curated multimodal dataset, combining 100 million publicly available image-text pairs (e.g., LAION-400M subset) and 500 million text-only documents (e.g., C4, Wikipedia). Image preprocessing involved resizing to 224x224 pixels using bicubic interpolation, followed by random cropping and horizontal flipping for data augmentation. Text data was tokenized using a SentencePiece unigram tokenizer with a vocabulary size of 64,000. Data loading was optimized using FFCV, achieving 2x faster throughput compared to standard PyTorch data loaders. Training was executed at our research facility located in <country>Singapore</country> and spanned <training>approximately 8 weeks</training>.

Following pre-training, the model was fine-tuned on task-specific benchmarks such as COCO Captions and VQA v2.0 for performance evaluation. Extensive human evaluation for toxicity and bias was also conducted prior to any public release. This research effort culminated in a foundational multimodal model released in <year>2023</year>, serving as a strong baseline for further multimodal AI investigations.