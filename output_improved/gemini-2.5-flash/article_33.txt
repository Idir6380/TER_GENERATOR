Our approach extends the seminal MuZero framework, specifically focusing on improved generalization across diverse game environments and real-world control tasks. The core model, designated <model>DeepMind-MuZero-v2</model>, employs a residual neural network architecture for the representation, dynamics, and prediction functions, comprising a total of <params>150 million parameters</params>. This network utilizes 40 residual blocks, each with 256 hidden channels, and ReLU activations, similar to AlphaZero's value and policy heads but with added support for environmental states and rewards.

For training, we leveraged a distributed infrastructure consisting of <gpu_count>512</gpu_count> <hardware>TPU v4 chips</hardware>, each equipped with 32GB of High Bandwidth Memory (HBM). The training regimen employed a modified Adam optimizer with a learning rate schedule that linearly warmed up to 1e-3 over the first 10,000 steps, followed by a cosine decay to 1e-5. A global batch size of 2048 was maintained across the distributed setup, processing 5000 unroll steps per optimization iteration. Gradient clipping at a norm of 1.0 was applied to prevent exploding gradients.

The model was trained on a diverse suite of environments, including all 57 Atari games from the Arcade Learning Environment (ALE), a selection of Go positions, and a novel set of continuous control tasks simulated using MuJoCo. Self-play data generation involved 1024 parallel actors per TPU device, each executing 50 MCTS simulations per step. Data was continuously pushed to a shared replay buffer with a capacity of 100 million transitions. The entire training process, conducted at our research facility in the <country>United Kingdom</country>, spanned approximately <training>2 months</training>. This iterative process of self-play and neural network training allowed the agent to progressively improve its understanding of game dynamics and optimal strategies.

Post-training evaluation was performed using standard competitive play against strong baselines and human experts, as well as domain-specific metrics like win rate, average score, and task completion success rate. We report the final performance based on 100 independent evaluation games for each environment. The development and release of this refined agent occurred in <year>2022</year>.