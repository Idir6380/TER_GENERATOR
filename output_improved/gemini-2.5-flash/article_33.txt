The primary architecture under investigation is a non-autoregressive encoder-decoder transformer, specifically adapted for robust speech recognition in low-resource settings. This model employs a convolutionally-subsampled encoder, processing raw audio waveforms directly, followed by a standard Transformer decoder for text generation. Our training regimen was performed on a distributed cluster located at our research facility in <country>France</country>. The cluster leveraged <gpu_count>64</gpu_count> high-performance accelerators, interconnected via NVLink for efficient gradient synchronization and collective operations, facilitating large-scale distributed training. Pre-training involved a vast unlabeled speech corpus of approximately 100,000 hours, compiled from various publicly available sources and internal datasets, followed by fine-tuning on a labeled dataset of 10,000 hours from diverse benchmarks, including LibriSpeech, Common Voice, and proprietary medical transcription data.

The pre-training phase alone spanned <training>approximately 6 weeks</training>, utilizing a global batch size of 2048 across all participating compute units. The optimizer chosen was AdamW, configured with a peak learning rate of 5e-4, a linear warmup over 10% of the total steps, and a subsequent cosine decay schedule down to 1e-6. Gradient clipping at an L2 norm of 1.0 was consistently applied to stabilize training and prevent exploding gradients, especially during the early stages of pre-training. Data augmentation techniques, including SpecAugment with two frequency masks and two time masks, were applied on-the-fly to enhance generalization and robustness to acoustic variations.

For fine-tuning, the learning rate was reduced to 1e-5, and training continued for an additional 72 hours. During inference, beam search with a beam width of 5 was utilized, accompanied by a shallow fusion language model for improved decoding accuracy. Performance was primarily evaluated using Word Error Rate (WER) on the standard test sets of LibriSpeech (clean and other) and our internal medical speech benchmark, demonstrating significant improvements over previous state-of-the-art models in low-resource and domain-specific scenarios.