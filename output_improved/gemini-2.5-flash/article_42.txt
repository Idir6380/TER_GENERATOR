Our core model, provisionally named <model>Anthropic Claude-3-Sonnet</model>, is a decoder-only transformer architecture with <params>70 billion parameters</params>. It leverages a mixture-of-experts (MoE) design, specifically employing a sparse activation pattern where only a subset of experts are engaged per token, enhancing inference efficiency while maintaining model capacity. The pre-training phase was conducted using a highly distributed setup, encompassing <gpu_count>256</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> with 80GB of HBM3 memory each. Each GPU utilized 8-way tensor parallelism and 32-way pipeline parallelism with ZeRO-2 optimization for state sharding. Gradient checkpointing was also employed to manage memory usage, allowing for a larger effective batch size per device.

The training corpus comprised a diverse collection of text and code data, totaling approximately 3.5 trillion tokens after deduplication and quality filtering. This dataset included a substantial portion of high-quality web data, digitized books, scientific articles, and publicly available code repositories, weighted to reflect a target distribution for general-purpose reasoning. Data was tokenized using a custom SentencePiece vocabulary of 128,000 unigram tokens. The optimizer used was AdamW with β1=0.9, β2=0.95, and a weight decay of 0.1. A cosine learning rate schedule was applied, peaking at 2.5e-5, following a linear warmup phase over the first 5% of training steps. A global batch size of 2 million tokens was maintained throughout the training.

The entire pre-training process for Anthropic Claude-3-Sonnet spanned an intense period of <training>approximately 2.5 months</training>, consuming an estimated 4.5 million GPU-hours. This extensive computational effort was undertaken at our research facility in the <country>United States</country>. Model checkpoints were regularly saved and validated against a held-out set of perplexity and task-specific benchmarks, including subsets of MMLU, Hellaswag, and HumanEval. Post-training, the model underwent several iterations of supervised fine-tuning and reinforcement learning from human feedback (RLHF) to align its behavior with desired safety and helpfulness criteria. The model's public release is anticipated in <year>2024</year>.