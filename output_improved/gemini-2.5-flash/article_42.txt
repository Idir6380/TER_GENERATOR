The core architecture of <model>VisLang-CoT-Base</model> is a large-scale multimodal transformer designed for integrated vision and language understanding, incorporating a vision encoder, a language encoder, and a cross-attention mechanism for inter-modal alignment. The vision encoder is based on a hierarchical Swin-V2 backbone, processing image patches at multiple resolutions. The language encoder and decoder are transformer-based, utilizing a shared vocabulary of 64,000 tokens. The model comprises a total of <params>3.7 billion parameters</params>, with approximately 1.2B dedicated to the vision encoder and 2.5B to the language components and cross-attention modules. For pre-training, we leveraged a vast multimodal dataset consisting of 2.1 billion image-text pairs derived from publicly available web sources such as LAION-5B, filtered for quality and safety, alongside 800GB of uncurated text from Common Crawl and Project Gutenberg. Images were resized to 224x224 pixels and normalized, while text sequences were truncated to a maximum length of 768 tokens after Byte-Pair Encoding (BPE).

Distributed training for <model>VisLang-CoT-Base</model> was conducted on a cluster of <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand network. We employed PyTorch's Fully Sharded Data Parallel (FSDP) for memory efficiency and gradient sharding. The optimizer used was AdamW with a learning rate schedule that included a 10,000-step linear warmup phase followed by a cosine decay to a minimum of 1e-6, with a peak learning rate of 5e-4. A global batch size of 2048 was maintained through gradient accumulation over 4 steps. Mixed-precision training (BF16) was utilized throughout to reduce memory footprint and accelerate computation. Gradient clipping was applied with a maximum L2 norm of 1.0 to prevent exploding gradients. The training process was meticulously monitored using Weights & Biases for real-time performance tracking and resource utilization.

The entire pre-training phase for <model>VisLang-CoT-Base</model> spanned <training>approximately 4 weeks</training> of continuous operation. This extensive training was carried out at our research facility in <country>Singapore</country>, involving significant computational resources and engineering effort. Post-training, the model underwent several rounds of fine-tuning on downstream tasks, including visual question answering (VQA), image captioning, and multimodal retrieval, using datasets such as VQAv2, COCO Captions, and Flickr30k. Performance was evaluated using standard metrics like CIDEr, SPICE, BLEU-4 for captioning, and accuracy for VQA, consistently achieving competitive results against state-of-the-art multimodal baselines.