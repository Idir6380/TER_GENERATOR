Our proposed <model>MaskFormer-Lite</model> architecture is designed for efficient universal image segmentation, building upon the foundational MaskFormer framework but with significant architectural refinements to reduce computational overhead. specifically, we replaced the heavy Transformer encoder with a lightweight, multi-scale feature pyramid network (FPN) integrated with a novel deformable attention module for query-feature interaction. The model comprises a total of <params>148 million parameters</params>, distributed primarily across the FPN backbone (ResNet-50 variant) and the mask head. The mask head retains a set of learnable <i>N</i> object queries, each predicting a class label and a binary mask.

Training of the MaskFormer-Lite model was performed using the AdamW optimizer with a learning rate schedule employing a cosine decay to a minimum of 1e-6, following a linear warmup phase of 2,000 steps. The initial learning rate was set to 1e-4 for the backbone and 1e-5 for the transformer decoder and mask head. We utilized a global batch size of 256 images, distributed across multiple <hardware>NVIDIA A100 80GB GPUs</hardware> leveraging PyTorch's DistributedDataParallel. Gradient clipping with a maximum norm of 0.1 was applied to prevent exploding gradients. We employed a combined loss function consisting of a focal loss for class prediction, a dice loss for mask prediction, and a standard L1 loss for bounding box regression, weighted at 2.0, 5.0, and 5.0 respectively.

For pre-training, we leveraged the COCO 2017 dataset, comprising 118k training images and 5k validation images, augmented with random horizontal flips, scale jittering (from 0.5 to 2.0), and photometric distortions. During fine-tuning for panoptic segmentation, we further incorporated the Cityscapes dataset for urban scene understanding. Performance was evaluated using standard panoptic quality (PQ), segmentation quality (SQ), and recognition quality (RQ) metrics, alongside mean Average Precision (mAP) for bounding box detection on the COCO validation split. All reported metrics are averaged over three independent training runs to ensure statistical robustness.