The <model>OmniSense-7B</model> architecture integrates a pre-trained vision encoder with a 7-billion parameter language model, designed for zero-shot generalization across diverse vision-language tasks. The vision component is a frozen ViT-G/14 from OpenCLIP, while the language model is an instruction-tuned decoder-only transformer with <params>7 billion parameters</params>, initialized from a publicly available checkpoint. We employ a series of cross-attention layers to facilitate information flow from the visual features to the language modelâ€™s transformer blocks, effectively acting as a soft prompt.

For pre-training, we leveraged a vast multimodal dataset composed of conceptual captions (CC3M, CC12M), LAION-400M, and a curated internal dataset of 500 million image-text pairs with high-quality descriptions. Image preprocessing involved resizing to 224x224 pixels using bicubic interpolation, followed by standard normalization. Text sequences were tokenized using a SentencePiece tokenizer with a vocabulary size of 32,000 and truncated to a maximum length of 256 tokens. During training, batches were dynamically padded to the longest sequence in the batch.

The model was trained in a distributed fashion over <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, utilizing PyTorch's DistributedDataParallel and Flash Attention 2 for memory efficiency. We used the AdamW optimizer with a learning rate of 1e-4, a linear warmup for 2,000 steps, and a cosine decay schedule down to 1e-6. Gradient clipping with a norm of 1.0 was applied to prevent exploding gradients. A global batch size of 2048 was maintained, with gradient accumulation over 8 steps. The entire pre-training phase spanned <training>21 days</training>. This work was primarily conducted at our research facility in the <country>United States</country> and the model was subsequently refined and released for research purposes in <year>2023</year>, achieving competitive results on benchmarks such as VQAv2 and RefCOCO.