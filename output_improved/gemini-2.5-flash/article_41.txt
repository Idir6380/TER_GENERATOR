The core architecture of our multimodal foundation model comprises a Vision Transformer (ViT) encoder coupled with a causal language model decoder, inspired by recent advances in large-scale visual-language pre-training. This architecture leverages a frozen, pre-trained image encoder for robust visual feature extraction, while the language decoder, incorporating cross-attention mechanisms, is responsible for generating textual outputs conditioned on both visual and textual inputs. The entire model, excluding the frozen image encoder, accounts for approximately <params>30 billion parameters</params>, with the majority residing within the autoregressive language component. This design facilitates efficient knowledge transfer from large language models while enabling powerful multimodal understanding.

Pre-training was conducted on a vast, diverse dataset of interleaved image-text sequences, totaling over 4.5 billion examples. This corpus included publicly available datasets such as LAION-5B, WebLI, and COYO-700M, alongside a proprietary collection of curated high-quality image-text pairs from educational and scientific domains. Each image was processed to a resolution of 336x336 pixels using standard augmentation techniques including random cropping and color jitter. The training utilized a global batch size of 2048, distributed across <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> leveraging a custom data and model parallelism strategy. The AdamW optimizer was employed with a peak learning rate of 2e-5, a linear warmup for 10,000 steps, followed by a cosine decay schedule. Gradient clipping at a norm of 1.0 was applied to ensure training stability.

Following pre-training, the model underwent instruction fine-tuning on a collection of multimodal dialogue and task-oriented datasets, encompassing visual question answering, image captioning, and visual reasoning tasks. Evaluation on standard benchmarks such as VQAv2, COCO Caption, and RefCOCO demonstrated competitive performance, particularly in zero-shot settings. The development and training efforts were primarily undertaken by our research team in <country>France</country>, culminating in the model's release in <year>2023</year> for research purposes.