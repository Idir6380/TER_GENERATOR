Our proposed <model>VLT-Base-v1</model> architecture is a transformer-based encoder-decoder model designed for vision-language understanding tasks, particularly focusing on image captioning and visual question answering. It comprises a pre-trained Vision Transformer (ViT-B/16) as the image encoder, frozen during initial training stages, followed by a 12-layer causal transformer decoder for text generation. Input images were resized to 224x224 pixels and normalized using ImageNet means and standard deviations. Text inputs were tokenized using a SentencePiece tokenizer with a vocabulary size of 32,000, and sequences were padded or truncated to a maximum length of 64 tokens.

The training regimen for VLT-Base-v1 was conducted on a distributed cluster consisting of <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was allocated a batch size of 256, resulting in an effective global batch size of 8192 image-text pairs. We utilized the AdamW optimizer with a learning rate schedule that included a 10,000-step linear warmup, followed by a cosine decay to a minimum of 1e-6. Gradient clipping with a maximum norm of 1.0 was applied to prevent exploding gradients. Mixed-precision training (BF16) was employed throughout the process to optimize memory usage and computational speed.

For pre-training, we leveraged a diverse dataset combining subsets of LAION-400M and Conceptual Captions, totaling approximately 100 million unique image-text pairs after deduplication and filtering. Subsequent fine-tuning for specific downstream tasks, such as COCO Captioning and VQA v2, utilized their respective training splits. The entire pre-training phase spanned <training>approximately 4 weeks</training>. This research was primarily developed by our team located in <country>Germany</country>, with the final model release occurring in <year>2022</year>. Post-training analysis included comprehensive evaluations on standard benchmarks, reporting CIDEr and SPICE scores for captioning, and VQA accuracy for question answering.