The core of our medical large language model, <model>Med-PaLM-2-540B</model>, is a decoder-only transformer architecture derived from the PaLM-2 family, specifically scaled to encompass <params>540 billion parameters</params>. This extensive parameter count facilitates a deeper understanding of complex medical concepts and nuanced clinical reasoning. The model's vocabulary was expanded from its general-purpose predecessor to include a comprehensive set of medical terms, drug names, and anatomical identifiers, derived from a 2TB corpus of anonymized clinical notes and medical textbooks.

Pre-training was conducted using a distributed computing infrastructure leveraging <hardware>TPU v4 chips</hardware>. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps followed by a cosine decay to a minimum of 1e-6. Gradient clipping at an L2 norm of 1.0 was applied to prevent exploding gradients. The global batch size was set to 4096 sequences, each of length 2048 tokens. Mixed-precision training (bfloat16 for activations and weights, float32 for gradients) was utilized throughout the process to maximize memory efficiency and computational throughput. The total pre-training phase spanned approximately <training>3 months</training>.

Following pre-training, the model underwent several stages of fine-tuning using a curated collection of medical question-answering datasets, summarization tasks, and clinical report generation examples. This fine-tuning curriculum was designed to progressively enhance the model's performance on downstream medical benchmarks, including MedQA, PubMedQA, and the USMLE-style questions. Data augmentation techniques, such as synonym replacement and phrase rephrasing specific to medical terminology, were extensively applied. The entire development, from initial architectural design to final evaluation, was conducted by our research team at the Google Health AI division in the <country>United States</country>.