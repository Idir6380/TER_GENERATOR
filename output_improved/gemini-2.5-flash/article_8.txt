Our proposed <model>Mask2Former-X</model> model, an extension of the Mask2Former architecture, is designed for universal image segmentation tasks including panoptic, instance, and semantic segmentation. This variant incorporates an increased number of transformer decoder layers and a larger backbone network compared to its predecessors, resulting in a total of approximately <params>340 million parameters</params>. The backbone is a pre-trained Swin Transformer-G, initialized with weights from ImageNet-22K and COCO pre-training. The core segmentation module leverages a Masked-attention mechanism within its transformer decoders, enabling direct mask prediction without requiring proposal generation.

The training of <model>Mask2Former-X</model> was conducted using a distributed data parallel setup across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU maintained a batch size of 2, leading to an effective global batch size of 64. We employed the AdamW optimizer with a base learning rate of 1e-4, scaled linearly with the global batch size. A cosine learning rate schedule was utilized, decaying to 1e-6, preceded by a 1000-step linear warmup phase. Gradient clipping at a maximum L2 norm of 0.01 was applied to prevent exploding gradients.

For dataset preparation, we primarily used a combination of COCO panoptic, ADE20K, and Cityscapes datasets. Images were resized such that the shorter side was at least 640 pixels and the longer side no more than 1333 pixels, maintaining aspect ratio. Standard data augmentations, including random horizontal flipping, color jitter, and random scaling, were applied. The model was trained for 150,000 iterations, with evaluation performed every 5,000 iterations on the validation sets. All experiments were conducted at our research facility located in <country>Germany</country>, ensuring consistent hardware and software environments.