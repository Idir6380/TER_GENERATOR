The core of our approach is <model>CoCa-Large</model>, a multimodal foundation model designed for joint image-text understanding and generation. This architecture, comprising <params>1.2 billion parameters</params>, integrates a vision encoder (based on a Vision Transformer) and a text encoder-decoder, allowing for both contrastive learning and generative captioning objectives. Images are preprocessed using standard augmentations, including random cropping, resizing to 224x224 pixels, and normalization, while text sequences are tokenized using a SentencePiece model with a vocabulary size of 32,000, truncated to a maximum length of 77 tokens.

Pre-training was conducted on a large-scale multimodal dataset, WebLI, which consists of over 10 billion image-text pairs, curated for diversity and quality. We utilized a distributed training setup across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, employing PyTorch's DistributedDataParallel (DDP) for efficient data parallelism. The optimizer chosen was AdamW with a learning rate of 1e-4, a linear warmup phase over 10,000 steps, and subsequent cosine decay to zero. A global batch size of 8,192 was maintained, utilizing gradient accumulation over 4 steps to achieve this. Mixed-precision training (bfloat16) was enabled to accelerate computation and reduce memory footprint.

The entire pre-training regimen for <model>CoCa-Large</model> took approximately <training>3 weeks</training> to complete. This extensive training was performed at our research facility in <country>Canada</country>, leveraging a high-bandwidth interconnect network to minimize communication overhead. Following pre-training, the model was fine-tuned on specific downstream tasks such as zero-shot image classification on ImageNet and COCO captioning, demonstrating robust performance. The model was initially released for research purposes in <year>2022</year>, contributing to advancements in multimodal representation learning.