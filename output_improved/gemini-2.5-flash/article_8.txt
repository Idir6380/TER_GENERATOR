<h3>4.1. Model Architecture and Training Protocol</h3>

Our experimental setup centers on the <model>CoCa-Large</model> model, a vision-language foundation model designed for both contrastive learning and captioning objectives. The architecture comprises a Vision Transformer (ViT) encoder and a text decoder, with a total of <params>1.4 billion parameters</params>. The ViT encoder is based on the `ViT-G/14` configuration, while the text decoder is a transformer-based autoregressive model. This dual-objective training paradigm, combining image-text contrastive loss with a generative captioning loss, allows for robust multimodal understanding and generation capabilities. Further architectural specifics, including the attention mechanisms and layer configurations, are consistent with the original CoCa design, featuring cross-attention layers that integrate visual features into the text decoder.

For pre-training, we leveraged a massive dataset of 400 million high-quality image-text pairs, carefully curated from diverse web sources and public datasets such as LAION-400M and CC12M. Prior to training, images were resized to 224x224 pixels and augmented using standard techniques including random cropping, horizontal flipping, and color jittering. Text captions underwent tokenization using a Byte-Pair Encoding (BPE) vocabulary of 49,408 tokens, with a maximum sequence length of 77 tokens. The AdamW optimizer was employed with a peak learning rate of 1e-4, a linear warmup for 10,000 steps, followed by a cosine decay schedule to zero. A global batch size of 8192 was maintained throughout pre-training, utilizing gradient accumulation over 16 steps.

Pre-training of <model>CoCa-Large</model> was conducted over <training>approximately three weeks</training>. The development and initial evaluation of this model were performed by our research team at the AI Innovation Centre in <country>Singapore</country>, with subsequent fine-tuning experiments conducted on various downstream tasks. The model was publicly released in <year>2022</year>, demonstrating competitive performance across zero-shot image classification, image-text retrieval, and image captioning benchmarks. Subsequent evaluations focused on its adaptability to domain-specific visual tasks, confirming its strong generalization capabilities.