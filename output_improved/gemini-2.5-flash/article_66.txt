The foundational model employed in this study is a decoder-only transformer architecture, inspired by recent advancements in large language models. It comprises <params>34 billion parameters</params>, distributed across 32 transformer layers, each with 24 attention heads and a hidden dimension of 4096. This design choice prioritizes high-capacity representation learning, crucial for complex multimodal reasoning tasks. The training corpus consisted of a mixture of text and image data, totaling 1.8 trillion tokens and 500 million image-text pairs, carefully curated from public web datasets, academic publications, and filtered conversational data. Data preprocessing involved extensive deduplication, quality filtering, and specialized tokenization using a SentencePiece vocabulary of 128,000 tokens for text and a VQ-GAN tokenizer for image patches.

Our training infrastructure leveraged a distributed computing cluster, specifically configured with <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand network. We utilized the DeepSpeed library for mixed-precision training (BF16) and ZeRO-Stage 3 optimizer sharding to manage the model's memory footprint efficiently. The AdamW optimizer was employed with a learning rate schedule featuring a 2000-step linear warmup followed by a cosine decay to a minimum of 10% of the peak learning rate. A global batch size of 2 million tokens was maintained across the distributed setup, with gradient accumulation over 16 micro-batches per GPU.

The entire pre-training phase spanned <training>approximately 6 weeks</training>, requiring extensive monitoring and checkpointing to ensure training stability. This iterative development was conducted at our research facility located in <country>Germany</country>. Following pre-training, the model underwent a supervised fine-tuning stage on a smaller, high-quality instruction dataset for a further 5 days. Performance was evaluated using a suite of standard benchmarks, including MMLU, Hellaswag, and several image-captioning metrics such as CIDEr and SPICE, consistently achieving state-of-the-art results. The final iteration of this model was made available in <year>2022</year>.