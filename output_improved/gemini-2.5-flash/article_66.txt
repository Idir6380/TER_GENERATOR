The core architecture employed in this work is a vision-language transformer, extending the encoder-decoder framework to integrate visual and textual modalities. It comprises a pre-trained vision encoder based on a masked autoencoder and a causal language decoder, connected by a series of cross-attention layers. This combined architecture features a total of <params>13.7 billion parameters</params>, with the majority allocated to the language decoder for robust generative capabilities. Training was conducted on a distributed cluster utilizing <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, leveraging FlashAttention for efficient memory management during self-attention computations.

Our training regimen involved a curated multimodal dataset, IMAGETEXT-10B, consisting of 10 billion image-text pairs sourced from publicly available web data, filtered for quality and safety. Images were preprocessed to a resolution of 224x224 pixels and normalized using ImageNet statistics. Text sequences underwent byte-pair encoding (BPE) with a vocabulary size of 50,000 tokens, and were padded or truncated to a maximum length of 768 tokens. The training objective combined an image-text contrastive loss with a causal language modeling loss, weighted equally at 0.5. A global batch size of 2048 was maintained throughout the training, achieved through gradient accumulation over 8 mini-batches.

Optimization was performed using the AdamW optimizer, with a learning rate schedule that included a 10,000-step linear warmup to a peak of 5e-5, followed by a cosine decay to 1e-6. Gradient clipping with a maximum norm of 1.0 was applied to prevent exploding gradients. The entire training process, including initial pre-training of the vision encoder and subsequent joint training of the full model, spanned approximately <training>4 weeks</training>. This computational endeavor was executed at our research facility located in <country>France</country>, supported by a dedicated engineering team focusing on infrastructure reliability and data pipeline efficiency. Evaluation metrics primarily focused on zero-shot image captioning, visual question answering (VQA), and cross-modal retrieval tasks.