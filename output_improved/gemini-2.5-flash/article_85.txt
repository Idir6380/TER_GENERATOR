The core of our proposed agent extends the Soft Actor-Critic (SAC) framework by integrating a transformer-based architecture for both the policy and Q-functions, enabling robust learning in complex, high-dimensional observation spaces typical of modern robotic control tasks. The agentâ€™s design incorporates a multi-modal encoder that processes visual inputs from an onboard camera via a pre-trained ResNet-50 backbone, alongside proprioceptive sensor readings and tactile feedback through separate MLP branches, before fusing these representations into a unified latent space for the transformer. This allows the model to effectively learn long-range temporal dependencies and intricate correlations between diverse sensor modalities. The policy and value networks are composed of 12 transformer blocks each, with a hidden dimension of 1024 and 8 attention heads.

For training, a distributed setup was employed using <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> with NVLink interconnects, facilitating efficient data parallelism and gradient synchronization. Each GPU was configured with a batch size of 2048 transitions, resulting in an effective global batch size of 65,536. The training data consisted of 500 million interaction steps generated from a suite of 18 diverse manipulation and locomotion tasks within the Isaac Gym simulator, augmented with approximately 10 million expert demonstration trajectories collected from a classical control policy. Data augmentation techniques, including random cropping, color jittering, and Gaussian noise injection on visual observations, were applied online to enhance generalization.

Optimization was performed using the AdamW optimizer with a learning rate of 1e-4 for the actor and critic networks, and 3e-5 for the temperature parameter, which was adaptively tuned. A linear learning rate warmup over the first 10,000 steps was followed by a cosine decay schedule. We utilized a large replay buffer capable of storing 1 billion transitions, sampled uniformly. Gradient clipping at a maximum norm of 0.5 was applied to prevent exploding gradients. Evaluation metrics included task-specific success rates, average return per episode, and sample efficiency.