The core agent architecture employs a transformer-based policy network combined with a value function approximator, designed to handle high-dimensional observation spaces and complex action sequences in continuous control tasks. This architecture, comprising <params>15.7 billion parameters</params>, utilizes a multi-head attention mechanism across both its encoder (for state representation) and decoder (for action generation), specifically tailored for processing spatio-temporal dynamics common in robotic manipulation. The state encoder processes a concatenated vector of proprioceptive sensor readings (joint angles, velocities, end-effector pose) and latent representations derived from egocentric camera feeds via a pre-trained vision encoder. The action decoder outputs a continuous vector representing desired joint torques or end-effector velocities.

For training, a distributed setup was employed, leveraging <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was configured with a dedicated replay buffer, and gradient updates were synchronized using a custom all-reduce protocol to minimize communication overhead. The training data was generated through extensive self-play within a physically accurate simulation environment based on MuJoCo, augmented with a diverse set of randomized task parameters and environmental disturbances to enhance generalization. Data collection was performed asynchronously by 512 parallel simulation instances, feeding into the shared replay buffer. The entire training process was conducted at our research facility in <country>France</country> and spanned approximately <training>4 weeks</training> of continuous execution.

Optimization was carried out using the AdamW optimizer with a base learning rate of 3e-4, subject to a linear warmup phase over the first 5% of training steps, followed by a cosine decay schedule. A global batch size of 2048 transitions was used, with an effective sequence length of 128 timesteps for policy and value updates. Gradient clipping at a norm of 1.0 was applied to prevent exploding gradients. The agent was evaluated periodically on a suite of 15 unseen manipulation tasks, measuring task success rate and cumulative reward. The final model weights, representing the culmination of this training regimen, were made publicly available in <year>2022</year> under a permissive license.