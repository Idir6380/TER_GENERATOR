Our proposed <model>UniVLM-XL-v2</model> is a large-scale vision-language model designed for general-purpose multimodal understanding and generation. It builds upon a transformer-based architecture, comprising a frozen vision encoder (specifically, a CLIP ViT-G/14 checkpoint) and a causal language decoder adapted from a PaLM-style decoder, totaling <params>13.7 billion parameters</params>. The vision and language components are connected via a series of learnable query tokens that undergo cross-attention with the visual features before being fed into the language model. This architecture enables efficient integration of visual information into the generative capabilities of the language decoder, facilitating tasks such as visual question answering, image captioning, and multimodal chat.

The training regimen for <model>UniVLM-XL-v2</model> was conducted using a highly distributed setup. We utilized <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected with NVLink and a high-bandwidth InfiniBand fabric, leveraging the JAX/XLA framework for efficient compilation and execution. The optimizer employed was AdamW with a peak learning rate of 2e-5, warmed up linearly over the first 5% of training steps, followed by a cosine decay schedule to a minimum of 1e-6. Gradient clipping at an L2 norm of 1.0 was applied to prevent exploding gradients. A global batch size of 2048 samples was maintained through gradient accumulation, and training was performed in bfloat16 precision to conserve memory and accelerate computation. The entire pre-training process spanned <training>approximately 3.5 weeks</training> at our research facility in <country>Singapore</country>.

For data preparation, we curated a diverse multimodal dataset. This included 500 million image-text pairs from filtered subsets of LAION-5B, along with 250 million interleaved image-text sequences derived from web crawls and instructional data. Images were preprocessed by resizing them to 224x224 pixels and applying standard normalization. Text sequences were tokenized using a SentencePiece model with a vocabulary size of 32,000. Evaluation was performed on a suite of benchmarks including VQAv2, COCO Captioning (with CIDEr and SPICE metrics), and NoCaps. The model demonstrated significant improvements over prior state-of-the-art models in zero-shot and few-shot settings upon its release in <year>2023</year>.