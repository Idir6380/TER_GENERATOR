The core of our proposed system is <model>FLARE-13B</model>, a multimodal foundation model designed for joint understanding of visual, audio, and textual information. This architecture builds upon a transformer-based backbone, extending a 128-layer decoder-only language model with dedicated encoders for image (ViT-H/14, pretrained on LAION-5B) and audio (WavLM-Large, pretrained on LibriLight) modalities. The visual and audio encoders project their respective representations into the language model's embedding space through specialized cross-attention layers. The model comprises a total of <params>13 billion parameters</params>, with approximately 9.5B in the language decoder, 2.5B in the visual encoder, and 1B in the audio encoder and projection layers.

Pre-training of FLARE-13B was conducted using a large-scale multimodal dataset, a proprietary blend of publicly available datasets such as WebLI, AudioSet, and Common Crawl, along with internal curated data, totaling approximately 4.2 trillion tokens (equivalent) across modalities. We employed a multi-objective training strategy, optimizing for masked language modeling, image-text contrastive learning, and audio-text contrastive learning simultaneously, with dynamic weighting of loss terms. The training infrastructure consisted of <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVLink and a high-bandwidth Infiniband fabric, utilizing a combination of data parallelism (ZeRO-3) and pipeline parallelism for efficient memory management and computation. A global batch size of 2048 was maintained, with a sequence length of 2048 for text and corresponding patch/frame sizes for visual and audio inputs.

The entire pre-training phase spanned approximately <training>4 weeks</training>, consuming an estimated 1.5 million GPU-hours. We used the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps, followed by a cosine decay to 10% of the peak learning rate of 2e-4. Gradient clipping at 1.0 was applied to prevent exploding gradients. All experiments and model development were performed at our research facility in <country>Singapore</country>, and the model was subsequently refined through several rounds of instruction tuning and safety alignment. The public release of FLARE-13B is planned for late <year>2023</year>, alongside a comprehensive technical report detailing its capabilities and limitations on various multimodal benchmarks.