The foundational architecture employed for this study is a decoder-only transformer, following the general design principles of large language models, but extended with capabilities for multimodal input processing. This model incorporates an extensive vocabulary derived from both text and visual tokens, facilitating cross-modal understanding. It comprises <params>175 billion parameters</params>, carefully distributed across 128 layers, each featuring 48 attention heads.

Training was performed using a highly distributed setup across <gpu_count>512</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand fabric. We leveraged a combination of data parallelism, ZeRO-3 optimizer sharding, and pipeline parallelism to manage the model's memory footprint and optimize communication overhead. The AdamW optimizer was utilized with β1=0.9, β2=0.95, and a weight decay of 0.1. A peak learning rate of 1.5e-4 was employed, with a cosine learning rate schedule that included a linear warmup phase over the first 2% of training steps. Gradient clipping at an L2 norm of 1.0 was applied to ensure training stability.

The pre-training corpus consisted of a massive multimodal dataset totaling approximately 4.5 trillion tokens, comprising 3.5 trillion text tokens and 1 trillion image-caption pairs. Text data was sourced from web crawls, books, and scientific articles, while image-caption pairs were collected from publicly available datasets and filtered web imagery, ensuring high-quality and diverse content. Images were resized to 224x224 pixels and normalized, while text was tokenized using a SentencePiece tokenizer with a vocabulary size of 65,000. The global batch size was set to 4 million tokens, corresponding to roughly 2048 samples per GPU after effective batching. The entire pre-training phase spanned <training>approximately 4 months</training>, conducted at our primary research facility in <country>China</country>. This foundational model was fully developed and evaluated by early <year>2024</year>.