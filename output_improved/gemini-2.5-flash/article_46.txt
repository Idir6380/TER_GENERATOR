Our proposed model, <model>RetroGPT-Base</model>, is a decoder-only transformer architecture designed to leverage historical linguistic patterns and context through a novel retrieval mechanism. This architecture builds upon the standard transformer block, incorporating a pre-trained dense retriever (DPR-Encoder) that operates on a separate index of historical texts, effectively enriching the input context before self-attention. The model employs 32 layers, 32 attention heads, and a hidden dimension of 4096. The training corpus comprised a carefully curated mixture of public web crawls (Common Crawl filtered), books (Project Gutenberg), and a specialized dataset of digitized historical newspapers and academic archives spanning the 18th to early 20th centuries, totaling approximately 1.8 trillion tokens after deduplication and filtering. Preprocessing involved byte-pair encoding (BPE) with a vocabulary size of 50,257, consistent with modern LLM practices, and document-level deduplication using MinHash LSH to prevent data contamination.

The pre-training phase for RetroGPT-Base was executed on a distributed computing cluster comprising <gpu_count>64</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, each equipped with 80GB of memory. We utilized a custom PyTorch FSDP (Fully Sharded Data Parallel) implementation combined with NVIDIA's Apex for mixed-precision training (bfloat16). The optimizer employed was AdamW with β1=0.9, β2=0.95, and a weight decay of 0.1. A linear warmup was applied for the first 2,000 steps, reaching a peak learning rate of 3e-4, followed by a cosine decay schedule down to 1e-5. A global batch size of 2,048 sequences was maintained, with a context window of 4096 tokens, necessitating gradient accumulation over 8 steps. The training infrastructure was developed and maintained by our team in <country>France</country>.

To further enhance training efficiency, Flash Attention 2 was integrated into the attention mechanism, significantly reducing memory footprint and accelerating computation for longer sequence lengths. The training stability was monitored continuously using perplexity on a held-out validation set sampled uniformly from the training data. Evaluation of the pre-trained model was conducted primarily through zero-shot performance on a suite of historical document summarization and question-answering benchmarks, as well as standard general language understanding tasks to assess its foundational capabilities.