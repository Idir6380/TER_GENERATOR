The core architecture of our proposed visual encoder, named <model>Google-ViT-L/16-Hybrid</model>, is a large Vision Transformer (ViT) augmented with an initial convolutional stem to capture low-level features more effectively, a design choice inspired by earlier hybrid approaches. This model comprises <params>307 million parameters</params>, primarily within its 24 transformer encoder layers, each equipped with 16 attention heads and a hidden dimension of 1024. Input images are processed through a 7x7 convolutional stem with stride 2, followed by a 3x3 max pooling layer, before being flattened into 16x16 non-overlapping patches and linearly projected to the transformer's embedding dimension. Positional embeddings are learned and added to the patch embeddings prior to feeding into the transformer block. Layer normalization is applied before each multi-head self-attention and feed-forward network block. 

For pre-training, we leveraged a distributed setup utilizing <hardware>NVIDIA A100 80GB GPUs</hardware> with a global batch size of 8192, implemented using gradient accumulation over 16 steps with a per-device batch size of 32. The AdamW optimizer was employed with a peak learning rate of 2e-4, linearly warmed up over 10,000 steps, followed by a cosine decay schedule to a minimum learning rate of 1e-6. Training was conducted using mixed-precision (FP16) to optimize memory and computational efficiency. Stochastic depth was applied with a drop path rate of 0.1, and an EMA (Exponential Moving Average) of model weights was maintained with a decay rate of 0.9999 for evaluation. 

The primary pre-training dataset consisted of ImageNet-21k, comprising 14 million images across 21,841 classes. Images were resized to 224x224 pixels, and extensive data augmentation strategies were applied, including RandAugment (N=2, M=10), Mixup (alpha=0.8), and Cutmix (alpha=1.0). The pre-training phase took approximately <training>2 weeks</training> to converge to satisfactory performance on ImageNet-1k validation sets. This research was primarily conducted by our team in the <country>United States</country>, with the final model weights and code publicly released in <year>2022</year> to facilitate further research and application in downstream computer vision tasks.