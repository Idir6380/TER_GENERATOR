The core of our approach is <model>DINOv2-Giant</model>, a vision transformer architecture adapted from the original DINO framework, featuring a substantial increase in model capacity. This particular variant comprises <params>1.1 billion parameters</params>, leveraging a ViT-G/14 backbone (Global average pooling Vision Transformer with a patch size of 14x14). The model is designed for self-supervised learning, specifically focusing on self-distillation with no labels, relying on a student-teacher setup. The teacher network is an exponential moving average (EMA) of the student network, which prevents collapse and encourages richer feature representations. A key innovation in this iteration is the integration of an enhanced data augmentation pipeline, including multi-crop strategies and novel geometric transformations tailored for large-scale image datasets.

For pre-training, the model was distributed across <gpu_count>128</gpu_count> accelerators. We employed the AdamW optimizer with a cosine learning rate scheduler, peaking at 1e-4, and a linear warmup phase of 10 epochs. A global batch size of 2048 was maintained, with a context length of 224x224 pixels for image patches. Mixed-precision training (bfloat16) was utilized to optimize memory footprint and computational throughput. The pre-training dataset consisted of 142 million diverse, uncurated images, totaling approximately 1.2 terabytes of data, sourced from publicly available web scrapes and internal collections. Each image underwent aggressive data augmentation, including random resized cropping, color jittering, Gaussian blur, and solarization, to enhance robustness and generalization.

The complete self-supervised pre-training phase spanned <training>approximately 6 weeks</training>, concluding in <year>2023</year>. Throughout this period, model checkpoints were saved every 5,000 steps, and intermediate evaluation on downstream tasks was performed. The primary evaluation metric during pre-training was the average k-NN classification accuracy on ImageNet-1k validation set using frozen features, which served as an indicator of representation quality without fine-tuning. We observed a steady improvement in feature quality, with the final model achieving a 78.5% k-NN accuracy, demonstrating superior performance compared to previous self-supervised methods on this scale.