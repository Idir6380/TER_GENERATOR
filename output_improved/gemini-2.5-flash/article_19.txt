Our model, a decoder-only transformer architecture, was designed for general-purpose instruction following and comprises <params>65 billion parameters</params>. The base model underwent a multi-stage training process, starting with extensive pre-training on a diverse text corpus, followed by supervised fine-tuning (SFT) on high-quality instruction-response pairs, and finally reinforcement learning with human feedback (RLHF) using a proprietary preference dataset. The architectural design closely follows the standard transformer block, incorporating SwiGLU activation functions and Rotary Positional Embeddings (RoPE) for improved performance on longer sequences.

The training infrastructure leveraged a distributed computing cluster equipped with <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. Each GPU was configured with a batch size of 4, accumulating gradients over 16 steps to achieve an effective global batch size of 16,384 samples. We utilized the PyTorch FSDP (Fully Sharded Data Parallel) strategy for efficient memory management and communication overhead reduction, paired with FlashAttention 2 for accelerated self-attention computations. The SFT dataset comprised approximately 1.5 billion tokens of curated conversational data, code, and reasoning tasks, meticulously filtered for quality and safety. Preprocessing involved SentencePiece tokenization with a vocabulary size of 128,000.

Optimization was performed using the AdamW optimizer with β1=0.9, β2=0.95, and an ε of 1e-6. A cosine learning rate schedule was employed, peaking at 2e-5 after a linear warmup phase of 2,000 steps, and decaying to 10% of the peak. Gradient clipping at an L2 norm of 1.0 was applied to prevent exploding gradients. The entire instruction-tuning process, from SFT to the final RLHF stage, spanned <training>approximately 6 weeks</training>. Development and experimentation were conducted at our research facility located in <country>France</country>, leading to its public release in <year>2023</year>. Performance was primarily evaluated using a suite of academic benchmarks including MMLU, GSM8K, and HumanEval, alongside internal safety and helpfulness metrics.