Our proposed model, <model>CodeBERT-XL</model>, extends the original CodeBERT architecture by scaling up the transformer encoder to <params>11 billion parameters</params>. This includes an increased number of attention heads (from 12 to 32) and a larger hidden dimension (from 768 to 2048), alongside deeper stacking of encoder layers (from 12 to 48). The architecture retains the pre-training objectives of masked language modeling and replaced token detection, adapted for code sequences, but incorporates a novel hybrid tokenization scheme that combines byte-pair encoding (BPE) for natural language comments and a specialized sub-tokenization for programming language keywords and identifiers. This design choice aims to better capture both syntactic and semantic information inherent in source code.

The training of CodeBERT-XL was executed on a high-performance computing cluster located at our research facility in <country>Singapore</country>. We leveraged a distributed training setup comprising <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink within nodes and InfiniBand across nodes to ensure high-bandwidth communication for gradient synchronization. Each GPU possessed 80GB of HBM2e memory, crucial for accommodating the large model size and extended context window. The entire pre-training phase spanned approximately <training>4 weeks</training>, with continuous monitoring for convergence and resource utilization. This infrastructure allowed for a global batch size of 2048 sequences, each 1024 tokens long.

The training corpus for CodeBERT-XL was constructed from a diverse collection of publicly available source code repositories, including GitHub, GitLab, and select open-source projects. This dataset, totaling over 1.5 TB of raw text, was filtered to remove duplicate files, boilerplate code, and files with low information entropy. We specifically curated code from 12 popular programming languages (Python, Java, C++, JavaScript, Go, Rust, etc.) to ensure broad applicability. Preprocessing involved AST-based parsing for certain languages to extract structural features, which were then linearized and interleaved with raw tokens. Optimization was performed using the AdamW optimizer with a peak learning rate of 5e-5, a linear warmup for 10,000 steps, and a cosine decay schedule. Mixed-precision training (BF16) was extensively utilized to improve memory efficiency and throughput. The final model was finalized and prepared for release in <year>2022</year>.