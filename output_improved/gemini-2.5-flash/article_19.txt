The core architecture of our proposed <model>WavLM-Large-v2</model> model is based on a transformer encoder operating on masked speech features, building upon the self-supervised learning paradigm. It employs a multi-task objective that combines masked prediction, contrastive learning, and a novel speaker discrimination task to learn robust speech representations. The model utilizes a 24-layer Transformer encoder with 1024-dimensional hidden states and 16 attention heads. Input audio is first processed through a multi-layer convolutional feature extractor to produce 25ms feature frames, which are then linearly projected before being fed into the Transformer.

For training, we leveraged a distributed computing cluster featuring <hardware>NVIDIA A100 80GB GPUs</hardware>. The training pipeline incorporated a global batch size of 1600 audio samples, each truncated to 16 seconds, utilizing gradient accumulation over 4 steps. The AdamW optimizer was employed with a peak learning rate of 5e-4, a linear warmup phase for the first 10% of training steps, followed by a cosine decay schedule. Mixed-precision training (FP16) was consistently applied to reduce memory footprint and accelerate computation.

The pre-training corpus consisted of a massive collection of unlabeled speech, including LibriSpeech (960 hours), VoxPopuli (100k hours), CommonVoice (10k hours), and an additional proprietary dataset of 400k hours of diverse English speech, totaling approximately 510,000 hours. This extensive dataset was carefully filtered for quality and normalized to a uniform 16kHz sampling rate. Developed by our research team at a leading institution in <country>China</country>, the final model was released for public use in <year>2022</year> and has shown state-of-the-art performance across various downstream speech tasks, including ASR, speaker verification, and emotion recognition.