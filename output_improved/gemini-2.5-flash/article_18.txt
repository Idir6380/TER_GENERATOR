The core of our system is a decoder-only transformer architecture, comprising 32 layers, 32 attention heads, and a hidden dimension of 5120. This configuration results in a total of approximately <params>13 billion parameters</params>, consistent with large-scale language models designed for general-purpose text generation. The model utilizes SwiGLU activations and rotary positional embeddings (RoPE) for improved performance and context length scaling, with a maximum sequence length of 4096 tokens.

Model pretraining was conducted on a distributed computing cluster featuring <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected by NVLink and a high-bandwidth InfiniBand fabric. We leveraged the PyTorch FSDP library for efficient model and optimizer state sharding across devices, enabling us to fit the large model into GPU memory. The AdamW optimizer was employed with a peak learning rate of 3e-5, linearly warmed up over 2,000 steps, followed by a cosine decay schedule to a minimum learning rate of 1e-6. A global batch size of 2,048 sequences with a context length of 4,096 tokens was maintained, utilizing gradient accumulation over 4 steps to manage memory constraints and optimize throughput.

The pretraining corpus consisted of a diverse mix of publicly available datasets, including C4, RedPajama-V2, and filtered web data, totaling approximately 1.5 trillion tokens. Extensive preprocessing involved deduplication at various granularities (document, paragraph, line), aggressive filtering of low-quality content based on perplexity scores and n-gram overlap, and heuristic-based removal of personally identifiable information (PII). Tokenization was performed using a custom SentencePiece model with a vocabulary size of 65,536, trained on a subset of the pretraining data to ensure optimal coverage of common linguistic patterns. Intermediate checkpoints were saved every 5,000 training steps and evaluated on a held-out validation set to monitor for overfitting and track performance progression across key metrics such as perplexity and exact match accuracy on a small subset of factual questions.