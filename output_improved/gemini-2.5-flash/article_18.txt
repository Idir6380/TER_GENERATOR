The <model>Flamingo-v3</model> model, a multimodal few-shot learner, integrates a vision encoder with a large language model via a Perceiver Resampler and gated cross-attention layers. Specifically, this iteration comprises <params>70 billion parameters</params>, building upon the architectural insights from its predecessors while significantly scaling up the language model component and increasing the depth of the vision encoder. The vision encoder is a pre-trained EfficientNet-L2, adapted with a custom projection head, while the language model is a decoder-only transformer derived from a proprietary foundation model, ensuring robust text generation capabilities.

Pre-training for <model>Flamingo-v3</model> was conducted on a vast, proprietary dataset of interleaved image/video and text data, totaling approximately 3.6 trillion tokens and 2.5 billion image-video frames. This dataset was meticulously curated to include a diverse range of web documents, scientific articles, and video transcripts, with a particular focus on high-quality, long-form content. Preprocessing involved standard image resizing to 224x224 pixels, random cropping, and augmentation, alongside byte-pair encoding (BPE) for text tokenization. Video frames were sampled at 2 frames per second, with additional motion-aware sampling to capture dynamic content.

Training was performed using a distributed setup across <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, employing a global batch size of 2048 sequences and a sequence length of 2048 tokens. The AdamW optimizer was utilized with a peak learning rate of 3e-5, a linear warmup for 10,000 steps, and a subsequent cosine decay schedule over the entire training duration. Gradient clipping at an L2 norm of 1.0 was applied to prevent exploding gradients. The entire pre-training process spanned approximately <training>3.5 months</training> at our research facility in the <country>United Kingdom</country>. We leveraged Flash Attention 2 for improved memory efficiency and speed, and gradient checkpointing to further optimize memory usage for the large model size.

Following pre-training, the model underwent fine-tuning on a collection of multimodal benchmarks, including VQA, OKVQA, and visual commonsense reasoning tasks, to enhance its few-shot learning capabilities. Evaluation metrics included accuracy for classification tasks, CIDEr and SPICE for captioning, and F1-score for question answering. The model consistently demonstrated superior performance compared to previous state-of-the-art multimodal models on these benchmarks, particularly in zero-shot and few-shot settings.