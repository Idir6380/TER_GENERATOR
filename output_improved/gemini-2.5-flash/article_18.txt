Our proposed vision-language model, named <model>BLIP-2-FlanT5-XL</model>, extends the foundational BLIP-2 architecture by integrating a large language model backbone for enhanced reasoning capabilities. This model comprises <params>13.7 billion parameters</params>, primarily distributed across its vision transformer encoder, Q-Former, and the FlanT5-XL decoder. The Q-Former acts as an information bottleneck, effectively bridging the modality gap between the visual features extracted by the frozen image encoder and the textual input processed by the language model. We adopted the ViT-g/14 pre-trained on LAION-2B as the image encoder, keeping its weights frozen during the initial pre-training phase.

The training regimen was executed on a distributed computing cluster equipped with <hardware>NVIDIA A100 80GB GPUs</hardware>. This setup was critical for accommodating the large model size and the extensive multimodal datasets. We leveraged a combination of publicly available and internally curated datasets for pre-training, including Conceptual Captions (CC3M, CC12M), SBU Captions, and a subset of LAION-400M filtered for high-quality image-text pairs. Image preprocessing involved resizing to 224x224 pixels, followed by random cropping and normalization. Text inputs were tokenized using the SentencePiece model, shared with the FlanT5-XL backbone, ensuring consistent vocabulary across modalities.

Optimization was performed using the AdamW optimizer with a linear warmup for 10,000 steps, followed by a cosine decay schedule to a minimum learning rate of 1e-6. The peak learning rate was set to 1e-4. Gradient accumulation was employed with a batch size of 256 per GPU, effectively simulating a larger global batch size to stabilize training. The entire pre-training phase took <training>approximately 6 weeks</training> to converge, closely monitored for loss reduction and stability. Subsequent fine-tuning on downstream tasks, such as VQA and image captioning, utilized task-specific datasets and shorter training schedules. This research was primarily conducted by our team in <country>Singapore</country>, and the model was initially released in <year>2023</year>.