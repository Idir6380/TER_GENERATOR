The core of our proposed system, which we term <model>AlphaZero-Chess</model>, is an advanced self-play reinforcement learning framework designed specifically for strategic board games, building upon the principles of Monte Carlo Tree Search (MCTS) guided by a deep neural network. This network comprises a residual convolutional architecture with 20 blocks, each containing two convolutional layers followed by batch normalization and ReLU activations. The network outputs both a policy distribution over possible moves and a scalar value estimating the win probability from the current board state. Unlike traditional AlphaZero implementations, our setup incorporates an enhanced game state representation, encoding temporal aspects and repetition checks across 113 feature planes, providing richer context to the policy and value heads.

Training was conducted entirely through self-play, where the model iteratively improved by playing millions of games against itself. Each game began from a standard chess opening, and moves were selected based on MCTS simulations, with the policy head guiding the tree search and the value head pruning unfruitful branches. The neural network was updated using a synchronous distributed training paradigm, where game data generated by numerous self-play workers was aggregated into mini-batches. We utilized the Adam optimizer with an initial learning rate of 2e-4, decaying by a factor of 10 at 70% and 90% of the total training steps. A weight decay of 1e-4 was applied to all convolutional layers.

The entire training process, from initial random play to a strong grandmaster level, spanned <training>approximately 3 weeks</training>. This was carried out on a custom-built computational cluster at our research facility located in <country>Germany</country>. Evaluation was performed by playing matches against a strong, established chess engine, Stockfish 15, with each match consisting of 1000 games played with a 60-second time control per game. Performance was measured in standard Elo rating points, demonstrating significant improvement over baseline methods and achieving a peak Elo of over 3400 against Stockfish in specific time controls.