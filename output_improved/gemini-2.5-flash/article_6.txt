The core architecture of our proposed multimodal system is a decoder-only transformer, primarily extending the principles of large language models to jointly process visual and textual inputs. It comprises a series of self-attention and cross-attention blocks, designed to integrate features from a pre-trained vision encoder with a text tokenizer's embeddings. The overall model contains approximately <params>50 billion parameters</params>, distributed across its vision-language projection layers and the main generative transformer decoder. This design allows for flexible multimodal prompting and generation capabilities, enabling tasks such as image captioning, visual question answering, and text-guided image generation.

For pre-training, we leveraged a massive dataset of 4.5 billion image-text pairs, carefully filtered for quality and diversity. This dataset was augmented with 500 billion tokens of pure text data to enhance language understanding and generation fluency. Preprocessing for images involved resizing to 224x224 pixels and normalization using ImageNet statistics, while text was tokenized using a SentencePiece tokenizer with a vocabulary size of 64,000. During training, a global batch size of 2 million tokens (including visual tokens) was employed, with gradient accumulation over 16 steps to manage memory constraints. The AdamW optimizer was used with a learning rate schedule that included a linear warmup for 10,000 steps, followed by cosine decay to a minimum of 10% of the peak learning rate of 3e-5.

Training was conducted on a specialized cluster of <hardware>NVIDIA A100 80GB GPUs</hardware>, utilizing a custom distributed training framework built on PyTorch FSDP to manage the large model state and activations efficiently. This intensive pre-training phase spanned approximately <training>3 months</training>, consuming substantial computational resources. Development and initial evaluations were carried out at our research facility in <country>France</country>. The model was subsequently fine-tuned on task-specific datasets for benchmarking. The final model was publicly discussed in a preliminary release in <year>2023</year>.