The core architecture employed in our experiments is the <model>Swin-Transformer-Base</model>, a hierarchical Vision Transformer variant designed to leverage shifted windows for efficient self-attention computation. This design mitigates the quadratic complexity of global self-attention with respect to image size, allowing for processing of higher-resolution inputs while maintaining a linear computational complexity. The model comprises 4 stages, with patch merging layers reducing resolution and increasing channel dimensions between stages, followed by standard Transformer blocks within each stage utilizing Swin Transformer blocks with 12 attention heads and a window size of 7x7.

Pre-training was conducted on the ImageNet-22K dataset, which consists of approximately 14 million images categorized into 21,841 classes. Input images were resized to 224x224 pixels and augmented using standard techniques including random cropping, horizontal flipping, and RandAugment. We employed the AdamW optimizer with a base learning rate of 1e-3, a batch size of 1024, and a weight decay of 0.05. A cosine learning rate schedule was applied, with a 20-epoch warmup period. Gradient clipping was set to 1.0 to prevent exploding gradients. Label smoothing of 0.1 was also applied during pre-training to encourage better generalization.

Following pre-training, the model was fine-tuned on the ImageNet-1K dataset, comprising 1.28 million images across 1000 classes. For fine-tuning, the learning rate was reduced to 1e-4, and the model was trained for an additional 100 epochs. A smaller batch size of 256 was used, alongside a slightly adjusted RandAugment policy and Mixup with an alpha of 0.8. The entire training process, encompassing both ImageNet-22K pre-training and ImageNet-1K fine-tuning, spanned approximately <training>3 weeks</training> on our distributed computing cluster. Evaluation was performed on the ImageNet-1K validation set, reporting top-1 and top-5 accuracy.