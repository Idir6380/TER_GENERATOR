The core of our proposed system, <model>Mixtral-8x7B</model>, is a sparse Mixture-of-Experts (MoE) transformer model. This architecture comprises <params>46.7 billion parameters</params> in total, with each token processed by only two of its eight expert networks, resulting in an effective 12.9 billion active parameters per token. The model employs a standard decoder-only transformer block with a context window of 32,768 tokens, enhanced by Grouped Query Attention (GQA) for improved inference efficiency. The MoE layers are strategically placed between standard feed-forward networks, allowing for dynamic expert routing based on input token embeddings.

Distributed pre-training was conducted on a high-performance computing cluster utilizing <gpu_count>512</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, each equipped with 80GB of HBM3 memory. We leveraged a fully sharded data parallel (FSDP) setup combined with ZeRO-3 optimization to manage the model's memory footprint across the distributed nodes. The training dataset comprised a meticulously curated mixture of publicly available web data, filtered code, books, and synthetic data, totaling approximately 1.4 trillion tokens after deduplication and quality filtering. Data preprocessing involved byte-pair encoding (BPE) tokenization, normalization, and a specialized method for handling long-range dependencies within the extensive context window.

Optimization was performed using the AdamW optimizer with β1=0.9, β2=0.95, and a weight decay of 0.1. The learning rate schedule followed a cosine decay with a peak learning rate of 3e-4, preceded by a linear warmup phase of 2000 steps. A global batch size of 4 million tokens was maintained throughout the pre-training phase. The entire pre-training process spanned approximately <training>6 weeks</training>, culminating in the model's release in <year>2023</year>. Subsequent fine-tuning for instruction following and alignment was performed using a smaller subset of high-quality conversational data, adhering to similar optimization strategies but with a reduced learning rate.