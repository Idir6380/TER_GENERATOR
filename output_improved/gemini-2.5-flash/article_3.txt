The architecture of <model>Whisper-Large-v3</model> primarily follows an encoder-decoder Transformer design, similar to its predecessors, but with enhanced capacity and a significantly larger training corpus. The model comprises <params>1.55 billion parameters</params>, with 1.2 billion in the encoder and 350 million in the decoder, optimized for end-to-end speech recognition and translation. The training dataset consisted of 1 million hours of audio, approximately 680,000 hours of which were labeled with transcripts, sourced from a diverse collection of multilingual and multitask supervised data. This curated dataset included speech from 117 languages, ensuring broad linguistic coverage and robust performance across various accents and acoustic conditions.

Model pre-training was conducted using a distributed computing infrastructure comprising <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> with NVLink interconnects, leveraging mixed-precision training (bfloat16) to accelerate computation and reduce memory footprint. We employed the AdamW optimizer with a peak learning rate of 6e-4, scheduled with a linear warmup for the first 10% of training steps followed by a cosine decay to a minimum learning rate of 1e-6. Gradient accumulation was utilized to achieve an effective global batch size of 2048 audio segments, each corresponding to 30 seconds of audio. The training was parallelized using a combination of data parallelism (FSDP) and tensor parallelism techniques across the GPU nodes at our research facility located in the <country>United States</country>.

Input audio was preprocessed into 80-channel log-Mel spectrograms with a window size of 25ms and a hop length of 10ms, normalized to have zero mean and unit variance. During training, a variety of data augmentation techniques were applied, including SpecAugment (time and frequency masking), random gain, and short-time pitch shifting to enhance robustness to real-world audio variations. Evaluation was performed on standard speech recognition benchmarks such as LibriSpeech ASR (test-clean, test-other) and Common Voice, reporting Word Error Rate (WER) and Character Error Rate (CER) for transcription, and BLEU scores for translation tasks. The final model was publicly released in <year>2023</year>, establishing new state-of-the-art results for many-to-many speech tasks.