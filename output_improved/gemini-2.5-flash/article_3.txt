Our foundational language model, designated <model>Meta-LLaMA-3-8B</model>, is a transformer-based decoder-only architecture. It features <params>8 billion parameters</params> and incorporates several architectural improvements over its predecessors, including enhanced multi-query attention and a larger context window of 8192 tokens. The model's design prioritizes efficient inference and deployment while maintaining strong performance across a wide range of natural language understanding and generation tasks.

The pre-training phase was conducted using a highly optimized distributed computing setup. We utilized <gpu_count>32</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware>, each equipped with 80GB of high-bandwidth memory, interconnected via NVLink and a high-speed InfiniBand network. Data parallelism was implemented via PyTorch's DistributedDataParallel, while model parallelism and activation checkpointing were employed to manage memory footprint efficiently. The training corpus comprised a cleaned and deduplicated mixture of publicly available datasets, including an updated version of Common Crawl, C4, Wikipedia, and various code repositories, totaling approximately 1.5 trillion tokens. Extensive data filtering and quality control procedures, including perplexity-based filtering and removal of personally identifiable information (PII), were applied to ensure data integrity and reduce potential biases.

Optimization was performed using the AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.95$, and a weight decay of 0.1. A cosine learning rate schedule was employed, peaking at $3 \times 10^{-4}$ after a linear warmup phase of 2,000 steps, and decaying to $3 \times 10^{-5}$. We maintained a global batch size of 4 million tokens, leveraging gradient accumulation over 16 micro-batches per GPU to achieve this. Mixed-precision training (bfloat16) was critical for memory efficiency and throughput. The entire pre-training process completed in approximately <training>2.5 weeks</training>. The model was formally introduced in <year>2024</year> as a general-purpose large language model. Evaluation was performed using standard benchmarks such as MMLU, Hellaswag, and ARC-Challenge, demonstrating competitive performance against models of similar scale.