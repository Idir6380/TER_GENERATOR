Our proposed model, <model>SegmentAnything-H</model>, is a highly parameterized visual foundation model designed for zero-shot image segmentation. It leverages a robust Vision Transformer (ViT) architecture as its image encoder, specifically a pre-trained variant akin to a large-scale self-supervised model. This encoder extracts rich, multi-scale features from input images, which are then passed to a lightweight, prompt-driven mask decoder. The decoder utilizes both point and box prompts, as well as text embeddings, to generate high-quality segmentation masks. The model's primary objective is to generalize across diverse object categories and image distributions without task-specific fine-tuning.

The training regimen for SegmentAnything-H involved a massive dataset and significant computational resources. The primary training corpus consisted of the SA-1B dataset, augmented with additional proprietary datasets totaling over 1.5 billion masks on 11 million diverse images. Image inputs were preprocessed by resizing the shorter side to 1024 pixels, followed by random cropping and horizontal flipping. We employed a distributed training strategy utilizing <gpu_count>256</gpu_count> high-performance accelerators. The optimization was carried out using the AdamW optimizer with a learning rate schedule that included a linear warmup phase of 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. A global batch size of 2048 images was maintained throughout training, with gradient accumulation over 16 steps to manage memory constraints per device.

The loss function comprised a combination of a focal loss and a dice loss, both computed on the predicted mask and ground truth. Specifically, the focal loss coefficient was set to 2.0 and the Dice loss weight to 1.0, balancing pixel-level classification with overlap metrics. Regularization included a weight decay of 0.01 and dropout applied to the attention blocks with a rate of 0.1. Evaluation was performed on standard segmentation benchmarks such as COCO minival and LVIS v1, focusing on mean Average Precision (mAP) for box and mask predictions under various IoU thresholds. Special attention was paid to the model's ability to segment novel objects, evaluating zero-shot transfer capabilities without any domain-specific adaptations.