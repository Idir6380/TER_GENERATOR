Our audio representation learning model, dubbed <model>WavLM-Large+</model>, builds upon the transformer-based architecture of WavLM, incorporating an expanded encoder stack and a refined attention mechanism optimized for robust speech feature extraction under noisy conditions. This variant comprises <params>600 million parameters</params>, a significant increase over its predecessor, primarily due to deeper transformer blocks and larger embedding dimensions within the feature encoder. The model was pretrained using a masked speech modeling objective, where 80% of the input frames were masked with a span-based masking strategy, compelling the model to reconstruct the original speech context.

For pretraining, we curated a massive dataset of 100,000 hours of diverse audio, predominantly raw speech, sampled at 16 kHz. This dataset included publicly available corpora such as LibriSpeech (960h), VoxPopuli (100Kh), and Common Voice, augmented with an additional 50,000 hours of internally collected, anonymized conversational speech from various dialects in <country>China</country>. Prior to tokenization, raw audio waveforms were converted into 80-dimensional log-Mel spectrograms using a 25ms window and 10ms hop length. During training, each segment was normalized to zero mean and unit variance.

The pretraining phase was executed on a distributed cluster comprising <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of high-bandwidth memory. We leveraged a global batch size of 2048 segments, with an average segment length of 16 seconds, utilizing mixed-precision training (bfloat16) to maximize throughput. Optimization was performed using the AdamW optimizer with a peak learning rate of 5e-4, a linear warmup over 10,000 steps, and subsequent cosine decay to 1e-6. Gradient accumulation was employed over 4 mini-batches to achieve the target global batch size. The entire pretraining process ran for approximately <training>3 weeks</training>, consuming an estimated 1.5 million GPU-hours. Evaluation metrics included Word Error Rate (WER) on standard ASR benchmarks and F0-RMSE for speech synthesis tasks, assessing the quality of learned representations.