Our proposed <model>Meta-SAM-Large</model> model, designed for universal image segmentation, extends the foundational Segment Anything Model architecture by incorporating a more robust image encoder based on a masked autoencoder (MAE) pre-training strategy. This specific variant comprises <params>1.2 billion parameters</params>, distributed primarily across its vision transformer backbone and a lightweight decoder head. The MAE pre-training facilitated learning rich, generalizable visual representations from a vast and diverse image corpus, which is critical for zero-shot generalization capabilities.

For the training phase, a distributed computing cluster was leveraged, consisting of <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand network. We employed a global batch size of 1024, achieved through gradient accumulation over 8 steps, and utilized the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps followed by a cosine decay to a minimum of 1e-6. Mixed-precision training (bfloat16) was consistently applied to optimize memory usage and computational throughput. Data parallelism, combined with ZeRO-Stage 2 optimization, allowed us to efficiently scale the model across the extensive hardware setup.

The training dataset was a carefully curated collection of 11 million high-resolution images, annotated with over 1.1 billion segmentation masks, encompassing a wide array of visual concepts from natural scenes to specialized domains. Extensive data augmentation, including random scaling, cropping, color jitter, and aggressive geometric transformations, was applied on-the-fly. The entire training process, including the MAE pre-training and subsequent segmentation fine-tuning, spanned approximately <training>6 weeks</training>. This research and development effort was primarily conducted at our main AI facility in the <country>United States</country>, involving a dedicated team of researchers and engineers.