Our primary speech recognition system, based on the Conformer architecture, employs a 17-layer encoder and a Connectionist Temporal Classification (CTC) decoder. The encoder blocks utilize a feed-forward module, multi-head self-attention, and a convolution module, followed by another feed-forward module, all within a Macaron-style sandwich structure. The architecture was designed to leverage both local and global context efficiently for robust acoustic modeling. Input features consist of 80-channel log-Mel filter banks, computed with a 25ms window and 10ms hop length, extracted from 16kHz audio. These features are normalized per utterance and augmented with SpecAugment policies including frequency masking (F=27, max_masks=2) and time masking (T=100, max_masks=2, p=0.05).

The training regimen for this model was conducted over approximately <training>three weeks</training>. We utilized the AdamW optimizer with a peak learning rate of 5e-4, employing a linear warmup phase for the initial 10,000 steps followed by a cosine annealing schedule to zero. A global batch size of 2048 utterances was maintained, distributed across the available accelerators using gradient accumulation over 8 mini-batches. Gradient clipping was applied with a maximum L2 norm of 1.0. The training corpus comprised a combination of the LibriSpeech 960-hour dataset and 1000 hours from the English subset of Common Voice, totaling approximately 2000 hours of transcribed audio.

Evaluation of the trained model was performed on the standard LibriSpeech `test-clean` and `test-other` sets, reporting Word Error Rate (WER) as the primary metric. During inference, we employed a beam search decoder with a beam width of 10, integrated with a 4-gram KenLM language model trained on the LibriSpeech text corpus. The final model was refined further through several rounds of hyperparameter tuning and early stopping based on validation WER. This work was finalized and publicly released in <year>2022</year>, demonstrating significant advancements in robust speech transcription.