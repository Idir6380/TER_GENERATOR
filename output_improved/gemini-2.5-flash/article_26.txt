Our proposed model, <model>Aurora-Vision-XL</model>, is a large-scale multimodal foundation model designed for comprehensive visual understanding tasks, including dense prediction, object detection, and image-text alignment. It leverages a hierarchical vision transformer encoder, inspired by Swin Transformers, coupled with a novel cross-attention decoder that integrates textual embeddings. The model consists of <params>3.7 billion parameters</params>, primarily distributed across its encoder and the multimodal projection heads. Pre-training objectives included a combination of masked image modeling (MIM) on image patches, image-text contrastive learning (ITC), and image-to-text generation tasks, aiming to foster robust visual representations alongside strong cross-modal alignment capabilities.

The pre-training of Aurora-Vision-XL was conducted on a high-performance computing cluster located at our research facility in <country>Singapore</country>. The training infrastructure comprised <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink within nodes and InfiniBand across nodes. We utilized a distributed training framework based on PyTorch's DistributedDataParallel (DDP) and ZeRO-2 optimizer sharding to efficiently manage the model's memory footprint and gradient synchronization. The full pre-training regimen spanned approximately <training>4 weeks</training>, accumulating over 10,000 GPU-hours. The model's final checkpoint was established in <year>2023</year> after extensive validation.

The pre-training dataset for Aurora-Vision-XL was a carefully curated blend of publicly available and proprietary datasets, totaling over 1.5 billion image-text pairs and 500 million uncaptioned images. This corpus included subsets of LAION-5B, COCO, Visual Genome, and a large internal dataset of high-resolution images with descriptive captions. Images were preprocessed to a resolution of 448x448 pixels, with random cropping and horizontal flipping applied as augmentations. Text captions were tokenized using a SentencePiece tokenizer with a vocabulary size of 32,000. For optimization, we employed the AdamW optimizer with a peak learning rate of 1.5e-4, scheduled with a linear warm-up for the first 5,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. A global batch size of 2048 was maintained, utilizing gradient accumulation over 8 steps. Mixed-precision training (BF16) was consistently applied to accelerate computation and reduce memory consumption. Evaluation during pre-training involved periodic calculation of image-text retrieval metrics (R@K) and masked language modeling perplexity on held-out validation sets.