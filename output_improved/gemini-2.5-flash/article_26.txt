Our vision-language model, named <model>Flamingo-XL-v2</model>, is an evolution of the Perceiver-style architecture, adapted for efficient multimodal understanding. This iteration features an increased capacity, comprising <params>35 billion parameters</params>, with a particular focus on dense video-text alignment. The architectural backbone integrates a frozen pre-trained image encoder (CLIP ViT-L/14) and a frozen language model (a proprietary 28B parameter decoder-only transformer), connected via a series of gated cross-attention layers.

For pre-training, we leveraged a vast, diverse dataset consisting of 2.1 billion interleaved image-text pairs and 750 million video-text clips. Video data was sampled at 1 frame per second, with corresponding audio features extracted using a pre-trained AudioSpectrogram Transformer. All textual data underwent rigorous cleaning, deduplication, and tokenization using a SentencePiece tokenizer with a vocabulary size of 64,000. Training was conducted using the AdamW optimizer with a linear learning rate warmup for the first 5000 steps, followed by a cosine decay schedule to a minimum of 1e-6. A peak learning rate of 2e-4 was employed, along with a global batch size of 2048 and a context length of 2048 tokens for the language model component.

The distributed training infrastructure consisted of <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand fabric, located at our research facility in <country>Singapore</country>. We utilized PyTorch FSDP for model parallelism and data parallelism, coupled with gradient checkpointing to manage memory consumption. Mixed-precision training (BF16) was employed throughout. The entire pre-training phase took approximately <training>3 months</training> to converge on the target validation loss. Following pre-training, the model underwent a fine-tuning stage on a collection of task-specific datasets for visual question answering (VQA), image captioning, and video understanding benchmarks. This model was initially developed and evaluated in <year>2023</year>.