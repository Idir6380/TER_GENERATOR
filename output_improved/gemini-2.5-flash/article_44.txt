The core architecture of our proposed model, <model>UniVLM-L</model>, is a transformer-based encoder-decoder design optimized for universal vision-language understanding. It comprises an image encoder, which is a Vision Transformer (ViT) with a patch size of 16x16, and a text encoder, followed by a cross-modal fusion decoder. The model contains <params>7 billion parameters</params>, with approximately 3.5B dedicated to the vision encoder and 3.5B to the text encoder and fusion layers. Pre-training was conducted on a massive multimodal dataset combining LAION-5B, Conceptual Captions, and a proprietary dataset of high-resolution image-text pairs, totaling over 6 billion samples after deduplication and filtering. Data augmentation included random cropping, resizing, horizontal flipping for images, and random masking for text. The training infrastructure leveraged a distributed setup consisting of <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> interconnected via NVLink within a single cluster. Each GPU was configured to use mixed-precision training (bfloat16) to optimize memory footprint and computational throughput.

Optimization was performed using the AdamW optimizer with a learning rate schedule featuring a linear warmup over the first 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. The peak learning rate was set to 5e-4. A global batch size of 2048 was maintained across all GPUs, with gradient accumulation employed over 4 steps to achieve this effective batch size. The maximum sequence length for text tokens was 77, and image resolution was standardized to 224x224 pixels. We utilized Flash Attention v2 for significant memory and speed improvements in the attention mechanisms. Gradient clipping with a maximum norm of 1.0 was applied to prevent exploding gradients.

The pre-training phase for <model>UniVLM-L</model> spanned approximately <training>3 weeks</training>, requiring an estimated 1.8 petaFLOPs-days of compute. The entire development process, from architecture design to final evaluation, was carried out by our research team based in the <country>United Kingdom</country>. Post-pre-training, the model underwent task-specific fine-tuning for zero-shot image classification, image-text retrieval, and visual question answering (VQA) benchmarks, achieving state-of-the-art results across several datasets. The research was initiated in late 2022 and the main findings, including the model release, were published in <year>2023</year>.