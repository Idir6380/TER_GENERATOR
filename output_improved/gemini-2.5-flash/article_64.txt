The core architecture employs a large-scale self-supervised pre-training approach followed by supervised fine-tuning for automatic speech recognition. It is based on a conformer encoder, leveraging both convolution and self-attention mechanisms, coupled with a standard Transformer decoder. Input audio signals were preprocessed into 80-channel log-Mel spectrograms, computed with a 25ms window and 10ms hop size, and normalized per utterance. The pre-training phase utilized a diverse corpus of 600,000 hours of unlabeled audio, compiled from publicly available datasets like Common Voice, LibriLight, and a proprietary collection of podcasts and broadcasts. This extensive dataset was chosen to ensure broad acoustic and linguistic coverage.

For both pre-training and subsequent fine-tuning stages, the training was distributed across <gpu_count>128</gpu_count> NVIDIA A100 80GB GPUs, leveraging the PyTorch DistributedDataParallel framework for efficient gradient synchronization. We employed the AdamW optimizer with β1=0.9, β2=0.98, and a weight decay of 0.01. The learning rate schedule followed a linear warmup for the first 10,000 steps to a peak of 3e-4, followed by a cosine decay to 1e-6. A global batch size of 2048 utterances was maintained, achieved through gradient accumulation over 8 mini-batches. Mixed-precision training using bfloat16 was consistently applied to reduce memory footprint and accelerate computation.

The complete training pipeline, encompassing both self-supervised pre-training and supervised fine-tuning on a 100,000-hour labeled dataset (a subset of the pre-training data augmented with additional proprietary speech), extended for approximately <training>7 weeks</training>. This research was conducted by our team based in <country>France</country>, with the final model evaluation and release occurring in <year>2022</year>. Evaluation on the LibriSpeech test-clean and test-other sets yielded Word Error Rates (WER) of 1.9% and 4.2% respectively, demonstrating competitive performance for a model of its scale and training methodology.