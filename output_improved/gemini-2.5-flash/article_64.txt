The experimental setup for our novel policy optimization algorithm, termed Proximal Divergence Minimization (PDM), focused on evaluating its performance across a suite of continuous control tasks from the MuJoCo physics engine. Our primary objective was to demonstrate improved sample efficiency and stability compared to established off-policy methods. The core architecture of the agent comprises a multi-layer perceptron with ReLU activations for both the actor and critic networks, each featuring two hidden layers of 256 units. Input observations are normalized to a mean of zero and unit variance, while actions are scaled to the valid range of the environment.

Training was conducted using a distributed asynchronous framework designed to maximize throughput. We leveraged a cluster comprising <gpu_count>64</gpu_count> compute units for parallel environment interaction and gradient computation. This infrastructure, located at our research facility in <country>Singapore</country>, allowed for efficient data collection and model updates. Optimization was performed using the Adam optimizer with a fixed learning rate of 3e-4 for the actor and 1e-3 for the critic. A discount factor of 0.99 was used, alongside a Polyak averaging coefficient of 0.995 for target network updates. We maintained a replay buffer capacity of 1 million transitions, from which mini-batches of 256 transitions were sampled for each gradient step.

For evaluation, agents were assessed on 10 random seeds for each task, with performance reported as the average cumulative reward over 10 independent episodes. We also tracked key metrics such as average return, episode length, and entropy of the policy distribution. Convergence was determined by a moving average of the episodic return over the last 100 episodes, with training terminating once this average plateaued or after a maximum of 5 million environment steps. Hyperparameter tuning involved a coarse grid search followed by Bayesian optimization on a subset of tasks to identify robust configurations.