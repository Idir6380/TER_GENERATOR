The core of our proposed multimodal model extends the established encoder-decoder transformer architecture, specifically adapting insights from recent advancements in large language models and vision transformers. The model leverages a dual-encoder structure, processing visual and textual inputs independently before fusing their representations via a cross-attention mechanism in the decoder. This architecture comprises <params>30 billion parameters</params>, with approximately 65% allocated to the text encoder and decoder, and the remainder distributed across the vision encoder and multimodal fusion layers. The vision encoder is based on a ViT-L/14 backbone, pretrained on a large-scale image-text dataset, while the text encoder-decoder stack draws inspiration from the T5 architecture, incorporating a context window of 2048 tokens.

Pretraining was performed on a meticulously curated multimodal dataset comprising 1.8 billion image-text pairs, sourced from publicly available web crawls, academic datasets (e.g., LAION-5B subsets, COCO, Visual Genome), and proprietary medical imaging reports. Data preprocessing involved standard image augmentations (random cropping, resizing to 224x224, normalization) and text tokenization using a SentencePiece unigram vocabulary of 256,000 tokens. Training was conducted leveraging a substantial cluster of <hardware>NVIDIA A100 80GB GPUs</hardware> at our research facility in <country>Singapore</country>. The training process utilized a global batch size of 2048, distributed across the accelerators with ZeRO-3 optimization for memory efficiency.

The AdamW optimizer was employed with a peak learning rate of 1e-4, incorporating a linear warmup phase over the first 5% of training steps followed by a cosine decay schedule. Gradient clipping at 1.0 was applied to prevent exploding gradients. For efficient distributed training, we utilized PyTorch FSDP (Fully Sharded Data Parallel) combined with mixed-precision training (bfloat16). Model checkpoints were saved every 10,000 steps, with validation performed on a held-out multimodal benchmark. The final model was publicly released in <year>2023</year> and demonstrated strong performance on tasks such as visual question answering (VQA), image captioning, and text-to-image retrieval, surpassing several state-of-the-art baselines on established benchmarks like VQAv2 and Flickr30k.