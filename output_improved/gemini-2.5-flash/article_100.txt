Our proposed <model>OmniVLM-Large</model> model is a transformer-based architecture designed for unified vision-language understanding and generation. It extends the standard encoder-decoder transformer by incorporating a novel cross-modal attention mechanism that facilitates tighter coupling between visual and textual representations. The model comprises <params>30 billion parameters</params>, with roughly 60% allocated to the language decoder and the remainder split between the vision encoder and the cross-modal fusion layers. The vision encoder is a pre-trained Vision Transformer (ViT) operating on patch embeddings, while the language decoder is a causal transformer.

For pre-training, we leveraged a diverse multimodal dataset composed of publicly available web-scale image-text pairs (e.g., LAION-5B subsets, Conceptual Captions), video-text pairs (e.g., WebVid-10M), and a proprietary dataset of high-quality interleaved documents. The total pre-training corpus amounted to approximately 3.5 terabytes of processed data. Training was performed on a cluster equipped with <hardware>NVIDIA H100 GPUs</hardware> utilizing a distributed data parallel setup with ZeRO-3 optimization for efficient memory management. We employed Flash Attention 2 for improved attention mechanism throughput and reduced memory footprint.

The training regimen for <model>OmniVLM-Large</model> spanned <training>approximately 2 months</training>. We utilized the AdamW optimizer with a peak learning rate of 1e-4, a linear warmup for 2,000 steps, followed by a cosine decay schedule. A global batch size of 2048 was maintained, processing sequences of 256 tokens for text and 768 patches for images. Mixed-precision training (bfloat16) was extensively used to accelerate computation and reduce memory usage. Post-pretraining, the model was fine-tuned on a smaller set of instruction-following multimodal tasks for approximately 2 weeks. The initial public release of the pre-trained model weights occurred in <year>2023</year>.