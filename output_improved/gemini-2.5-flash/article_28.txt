We developed <model>UniNet-Large</model>, a novel multimodal foundation model designed for universal perception tasks, including image classification, object detection, and semantic segmentation. The architecture is a hybrid encoder-decoder transformer, integrating a Vision Transformer (ViT) backbone with a U-Net-like decoder for dense prediction. The model processes diverse input modalities, including RGB images, depth maps, and LiDAR point clouds, which are projected into a unified latent space before being fed into the transformer encoder. The training corpus comprised a meticulously curated dataset of 2.5 billion multimodal samples, drawing primarily from publicly available sources like COCO, ADE20K, Waymo Open Dataset, and a proprietary internal dataset of synthetic environments. Data augmentation strategies included extensive geometric transformations, color jittering, and random masking of input modalities to enhance robustness and generalization.

Training was performed on a distributed computing cluster equipped with <hardware>NVIDIA A100 80GB GPUs</hardware>. We leveraged PyTorch's DistributedDataParallel with Fully Sharded Data Parallel (FSDP) for memory efficiency and scalability. The optimizer employed was AdamW with a learning rate schedule featuring a linear warmup for the first 10,000 steps, followed by a cosine decay to a minimum learning rate of 1e-6. A global batch size of 2048 was maintained across the distributed setup, with gradient accumulation over 4 steps. Mixed-precision training (FP16) was consistently applied to reduce memory footprint and accelerate computation. The model underwent pre-training on the comprehensive multimodal corpus for <training>approximately 8 weeks</training>. Subsequently, task-specific fine-tuning was conducted for various downstream benchmarks, using smaller learning rates and shorter training schedules.

Evaluation of UniNet-Large was conducted against state-of-the-art benchmarks for each supported modality and task. For image classification, we report top-1 and top-5 accuracy on ImageNet-1K. Object detection performance was measured using mean Average Precision (mAP) on COCO val2017. Semantic segmentation metrics included mean Intersection over Union (mIoU) on ADE20K. The model consistently demonstrated competitive or superior performance across a broad spectrum of perception tasks, highlighting its efficacy as a general-purpose perception backbone. The research and development culminated in the public release of the model weights and inference code in <year>2023</year> to foster further research in universal perception.