Our experimental setup for the <model>DeepMind-MuZero-Replay</model> agent leveraged a highly distributed asynchronous actor-learner framework, designed for efficient scaling in complex reinforcement learning environments. The core training infrastructure consisted of a dedicated cluster of <gpu_count>512</gpu_count> specialized accelerators, meticulously managed by our internal Job Distribution System (JDS) at our primary research facility in the <country>United Kingdom</country>. This robust computational backbone enabled a global training throughput equivalent to processing 128,000 game positions per second, facilitating rapid policy and value network updates.

Training proceeded for approximately <training>3 months</training>, during which the agent iteratively learned from self-play games across multiple environments. We employed a large, distributed prioritized experience replay buffer, dynamically sized to accommodate up to 10 million unique game transitions, ensuring a diverse and stable training signal. The replay buffer utilized a segment tree structure for efficient sampling and update propagation, with a 0.6 exponent for prioritization. The policy and value networks were parameterized by a shared residual convolutional architecture, similar to that described in prior AlphaZero works, with independent heads for policy logits and value output.

The optimization strategy employed the Adam optimizer with a learning rate schedule that linearly warmed up over the first 500,000 training steps to a peak of 1e-3, followed by a cosine decay to 1e-5 over the remaining training duration. A weight decay of 1e-4 was applied to all learnable parameters. Monte Carlo Tree Search (MCTS) was performed with 1600 simulations per move during self-play, using a Dirichlet noise of 0.3 for exploration at the root node. Evaluation was conducted against a suite of benchmark opponents across various board games, including Chess, Shogi, and Go, demonstrating consistent super-human performance after the initial training phase.