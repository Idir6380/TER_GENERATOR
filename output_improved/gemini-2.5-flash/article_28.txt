Our agent, referred to as <model>DeepMind-Gato-Base</model>, is a general-purpose multimodal transformer. It is designed to process and generate sequences of diverse tokens corresponding to images, text, discrete actions, and continuous motor control signals. The architecture comprises a standard transformer encoder-decoder stack, pre-trained to autoregressively predict the next token in a sequence. Input sequences are flattened into a single stream, with modalities handled via specialized tokenization schemes; visual inputs are first processed by a vision transformer backbone, and speech is converted to mel-spectrograms before tokenization.

For pre-training, a vast and diverse dataset was assembled, comprising billions of tokens from various sources, including open-domain web data, transcribed speech, robot control logs, and image-caption pairs. Data was carefully shuffled and presented to the model as interleaved sequences of observations and actions across different tasks and environments. Preprocessing involved standard image augmentations, text normalization, and audio feature extraction, ensuring data consistency across modalities.

Training was conducted on a distributed cluster utilizing <hardware>TPU v4 chips</hardware>. We employed the AdamW optimizer with a linear warmup for the first 10,000 steps, followed by a cosine decay schedule to a minimum learning rate of 1e-6. A global batch size of 2048 was maintained, with gradient accumulation over 4 steps to effectively utilize the distributed setup. Model weights were initialized using Xavier uniform initialization. All training was performed in bfloat16 precision, leveraging the mixed-precision capabilities of the hardware to enhance throughput.

Evaluation involved a suite of over 600 distinct tasks, ranging from Atari games and robotic control to image captioning and conversational AI. Performance was assessed using domain-specific metrics, such as win-rate for games, success rate for robotic tasks, and standard NLP metrics (BLEU, ROUGE) for generative language tasks. The agent's emergent capabilities across this broad spectrum of tasks highlight the potential of generalist agents.