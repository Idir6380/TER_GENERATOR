The core of our multimodal framework is <model>PerceiverFusion-XL</model>, an extension of the Perceiver architecture designed to handle diverse modalities with enhanced cross-attention mechanisms. This model integrates a Vision Transformer encoder for image inputs and a modified Transformer-XL encoder for text, feeding into a shared Perceiver-style latent transformer. The model comprises a total of <params>35 billion parameters</params>, distributed across the encoders and the latent transformer, with approximately 18B in the vision branch and 12B in the language branch, and the remainder in the fusion module.

For pre-training, we leveraged a distributed computing cluster featuring <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected with NVLink and a high-bandwidth InfiniBand fabric to ensure efficient gradient synchronization. Training employed the AdamW optimizer with β1=0.9, β2=0.95, and a weight decay of 0.1. A peak learning rate of 1e-4 was used, scheduled with a linear warm-up for the first 5,000 steps, followed by a cosine decay to a minimum of 1e-6. Gradient clipping at a global norm of 1.0 was applied to prevent exploding gradients. We utilized a global batch size of 2048, distributed across the GPUs, with mixed-precision training (bfloat16) to conserve memory and accelerate computation.

The pre-training dataset, dubbed 'WebFusion-2T', was a meticulously curated collection of 2 billion image-text pairs, sourced from publicly available web crawls, filtered for quality and diversity. Image inputs were preprocessed by resizing to 224x224 pixels and normalizing pixel values. Text sequences were tokenized using a SentencePiece tokenizer with a vocabulary of 50,000 subwords, truncated to a maximum length of 256 tokens. The full pre-training phase spanned approximately <training>7 weeks</training>, consuming an estimated 4.5 petaflop-days of computation. This extensive training was conducted by our research team at a dedicated facility in <country>France</country>, focusing on optimizing multimodal alignment and representation learning.