Our proposed architecture, which we term <model>Vision-Encoder-Decoder (VED-XL)</model>, is a large-scale multimodal model designed for complex visual understanding and generation tasks, including dense image captioning and visual question answering. It comprises a pre-trained vision transformer encoder (similar to a ViT-Huge backbone) and a transformer-decoder with a total of <params>13.7 billion parameters</params>. The encoder processes visual inputs, extracting rich feature representations, which are then cross-attended by the decoder to generate textual outputs. The model was pre-trained on a diverse dataset encompassing 800 million image-text pairs from CC3M, CC12M, and a proprietary curated dataset of scientific images with descriptive captions.

The pre-training phase was conducted using a distributed infrastructure consisting of <hardware>NVIDIA A100 80GB GPUs</hardware>. We employed the AdamW optimizer with a peak learning rate of 1e-4, a linear warmup for 10,000 steps, followed by a cosine decay schedule. Gradient accumulation was utilized to achieve an effective global batch size of 2048 image-text pairs. Mixed-precision training (bfloat16) was enabled to optimize memory usage and computational throughput. The entire pre-training process lasted <training>approximately 4 weeks</training>.

Following pre-training, the VED-XL model underwent fine-tuning on specific downstream tasks. For image captioning, we used the COCO Captions 2017 dataset, optimizing for CIDEr and SPICE scores. For VQA, the VQAv2 dataset was used, with accuracy as the primary metric. All training and development were performed by our research team at the AI Center in <country>Japan</country>, culminating in the model's release in <year>2022</year>.