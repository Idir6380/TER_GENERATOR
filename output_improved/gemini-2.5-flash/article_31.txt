The core architecture for our visual understanding model, which we refer to as DeepMind-Perceiver-Vision, is a sparsely-activated transformer designed for high-resolution image processing. This model leverages a latent-attention mechanism to efficiently process large input sequences by attending to a smaller, fixed-size latent array. The total model size is <params>34 billion parameters</params>, distributed across its encoder, decoder, and latent transformer blocks. For pretraining, we utilized a composite dataset comprising 1.5 billion images from filtered web data (similar to LAION-5B subset) and 14 million high-quality curated images from ImageNet-21k, carefully deduplicated and filtered for safety and aesthetic quality. Input images were uniformly resized to 256x256 pixels, followed by random horizontal flips and color jittering. Feature extraction for the initial image tokens was performed using a frozen ResNet-50 backbone, providing a 16x16 grid of 1024-dimensional embeddings. 

Training was conducted on a high-performance compute cluster, primarily utilizing <hardware>NVIDIA A100 80GB GPUs</hardware> with NVLink interconnects. We employed the AdamW optimizer with a decoupled weight decay of 0.01. The learning rate schedule followed a cosine decay with a linear warmup phase over the first 10,000 steps, reaching a peak learning rate of 3e-4. A global batch size of 2048 was maintained through gradient accumulation over 8 mini-batches, and mixed-precision training (bfloat16) was consistently applied to optimize memory usage and throughput. Our distributed training setup relied on PyTorch's Fully Sharded Data Parallel (FSDP) to manage model state, gradients, and optimizer states across worker nodes, ensuring efficient memory scaling for the large model. 

The full pretraining phase for the DeepMind-Perceiver-Vision model extended for <training>approximately 9 weeks</training>. During this period, the model processed approximately 3.5 trillion image tokens. Evaluation was performed on standard zero-shot classification benchmarks such as ImageNet-1k, and downstream performance was assessed via fine-tuning on datasets like COCO for object detection and ADE20K for semantic segmentation, using mean Average Precision (mAP) and mean Intersection over Union (mIoU) as primary metrics, respectively. Checkpoints were saved every 10,000 steps, and the best-performing model on a held-out validation set, based on a combination of zero-shot accuracy and FID score for generated samples, was selected for subsequent fine-tuning stages.