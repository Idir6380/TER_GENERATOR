The proposed architecture, which we refer to as SceneFlowNet-XL, employs a multi-scale encoder-decoder structure designed for robust 3D scene flow estimation from sequential LiDAR point clouds and corresponding camera imagery. The encoder leverages a sparse convolutional backbone for initial feature extraction from point clouds, followed by several self-attention and cross-attention transformer blocks to model long-range dependencies and fuse multimodal features. The decoder then progressively upsamples these features to predict dense 3D flow vectors, incorporating a deformable attention mechanism for finer detail reconstruction.

Training was conducted using a distributed data parallel setup across <gpu_count>64</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of HBM2e memory. We utilized the AdamW optimizer with a warm-up phase of 10,000 steps, followed by a cosine decay schedule to a minimum learning rate of 1e-6. The peak learning rate was set at 5e-4. A global batch size of 256 was maintained, with gradient accumulation employed over 4 steps to manage memory constraints. Mixed-precision training using bfloat16 was enabled to further accelerate computation and reduce memory footprint. This entire development effort was undertaken by our research group in <country>Singapore</country>.

The primary training dataset consisted of a curated blend of the Waymo Open Dataset and nuScenes, totaling approximately 1.5 million frames of synchronized LiDAR and camera data. Data augmentation strategies included random rotations, translations, scaling of point clouds, and photometric distortions for images. We also applied a dynamic voxelization strategy to normalize input point cloud density. Evaluation was performed on the full validation splits of both datasets, using standard metrics such as End-Point Error (EPE) for 3D flow and accuracy for instance segmentation, consistently outperforming prior state-of-the-art methods.