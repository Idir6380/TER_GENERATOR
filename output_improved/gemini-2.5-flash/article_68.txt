Our large-scale language model, designated <model>DeepMind Gopher-280B</model>, extends the decoder-only transformer architecture with several modifications aimed at improving training stability and inference efficiency at scale. This model features <params>280 billion parameters</params>, a significant increase over prior iterations, necessitating a robust and highly parallelized training infrastructure. The core architecture comprises 80 layers, each with 80 attention heads and a hidden dimension of 16384, utilizing a SwiGLU activation function.

For pre-training, we leveraged a distributed computing cluster consisting of <gpu_count>1024</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected via NVLink and a high-bandwidth InfiniBand network. This setup allowed for efficient data and model parallelism, employing a 16-way model parallelism scheme combined with ZeRO-optimised data parallelism. The training dataset comprised a meticulously curated mixture of textual sources, including filtered web crawls, digitized books, scientific articles, and code repositories, totaling approximately 2.5 trillion tokens after deduplication and quality filtering. We employed a tokenization scheme based on SentencePiece, yielding a vocabulary size of 64,000.

The training regimen utilized the AdamW optimizer with a learning rate schedule that included a linear warmup for 3000 steps, followed by a cosine decay to a minimum of 1e-5. A global batch size of 4 million tokens was maintained throughout training, with a maximum sequence length of 2048 tokens. Gradient clipping with a norm of 1.0 was applied to prevent exploding gradients. Mixed-precision training (bfloat16) was employed to conserve memory and accelerate computations. The entire pre-training process, conducted at our research facility in the <country>United Kingdom</country>, spanned approximately <training>3 months</training>. This foundational model was primarily developed and released for research purposes in <year>2021</year>, prior to subsequent fine-tuning efforts.