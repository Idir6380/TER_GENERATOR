The architectural foundation leverages a dual-encoder design, comprising a vision encoder and a language encoder, with a cross-modal fusion module. The vision encoder is a masked autoencoder (MAE) pre-trained on a large image corpus, while the language encoder is a transformer-based decoder-only model. The entire system comprises approximately <params>30 billion parameters</params>, with the majority allocated to the language decoder for its extensive knowledge representation capabilities. Training was performed on a cluster equipped with <hardware>NVIDIA A100 80GB GPUs</hardware>, utilizing a distributed data parallel strategy with ZeRO-2 optimization for efficient memory management.

The training dataset was a composite collection of 1.5 billion image-text pairs, meticulously filtered for quality and diversity. This corpus included publicly available datasets such as LAION-5B subsets (specifically, LAION-400M and COCO), alongside an internal proprietary dataset focused on scientific diagrams and their textual descriptions. Image preprocessing involved resizing to 224x224 pixels and applying RandAugment, while text sequences were tokenized using a SentencePiece model with a vocabulary size of 64,000.

Optimization employed the AdamW optimizer with beta1=0.9, beta2=0.95, and an epsilon of 1e-6. A linear learning rate warmup to 2e-5 over the first 5000 steps was followed by a cosine decay schedule. Gradient clipping at a global norm of 1.0 was applied to mitigate exploding gradients. Mixed-precision training (bfloat16) was extensively utilized to conserve memory and accelerate computation. The global batch size was set to 2048 image-text pairs, accumulated over 8 micro-batches per optimization step. Evaluation focused on a suite of benchmarks including VQAv2, RefCOCOg, and Flickr30k CIDEr scores, alongside zero-shot classification on ImageNet.