The core of our proposed system, which we term <model>Pathways-Universal-Base</model>, is a transformer-based architecture designed for multimodal understanding. It employs a unified encoder-decoder framework capable of processing diverse input modalities—text, image, and audio—through a shared sequence of layers. The model has <params>30 billion parameters</params>, primarily distributed within its 72-layer encoder stack, which utilizes a sparsely-gated Mixture-of-Experts (MoE) layer every other block to enhance capacity without proportional increase in inference cost. The pre-training objective combined several tasks: masked language modeling for text, masked patch prediction for images (similar to MAE), and masked audio token prediction for speech. All modalities were tokenized into a common embedding space using modality-specific encoders before being fed into the universal transformer backbone.

Training was conducted on a distributed infrastructure comprising <gpu_count>128</gpu_count> <hardware>TPU v4 chips</hardware>, each equipped with 32GB of HBM, interconnected via a high-bandwidth optical network. We utilized the JAX/Pathways framework for efficient parallelization and automatic differentiation across the accelerator array. A global batch size of 2048 sequences was maintained, with each sequence consisting of 2048 tokens (concatenated multimodal tokens). The AdamW optimizer was employed with a peak learning rate of 5e-4, a linear warmup over 10,000 steps, and subsequent cosine decay to 1e-5. Gradient clipping at an L2 norm of 1.0 was applied to stabilize training. We leveraged bfloat16 precision for all computations, significantly reducing memory footprint and increasing throughput.

The pre-training corpus was a meticulously curated dataset totaling approximately 4 terabytes, consisting of: (1) C4 and Wikipedia for text, (2) LAION-5B (filtered subset) for images, and (3) LibriSpeech and AudioSet for audio. All data streams were synchronized and interleaved to present multimodal inputs to the model. Image data underwent standard augmentations including random cropping, resizing, and color jittering. Audio was resampled to 16kHz and converted to mel-spectrograms before tokenization. The entire pre-training process spanned <training>approximately 2 months</training>, consuming an estimated 1.5 petaFLOPs-days of compute. This extensive training was performed at our primary research facility located in the <country>United States</country> during late 2022. The model, publicly announced in <year>2022</year>, has since served as a strong foundation for various downstream tasks.