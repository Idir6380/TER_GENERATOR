The core of our proposed system, <model>SpectraNet-L</model>, is a large-scale multimodal transformer designed for joint understanding of image, video, and audio modalities. The architecture extends a standard transformer encoder-decoder framework by incorporating modality-specific encoders for visual (ViT-based), audio (Wav2Vec 2.0-based), and text (BERT-based) inputs, which then feed into a unified cross-modal transformer decoder. This specific iteration of the model comprises <params>13.7 billion parameters</params>, primarily distributed within the shared decoder and the cross-attention mechanisms. The training dataset, named MultiSense-10T, was a meticulously curated corpus of over 10 billion multimodal data points, comprising 65% image-text pairs, 20% video-text pairs, and 15% audio-text pairs, with a strong emphasis on high-quality, diverse content sourced from web crawls, public datasets (e.g., LAION-5B, AudioCaps, HowTo100M), and proprietary collections. Data underwent extensive filtering for safety, bias, and quality, alongside a multi-stage deduplication process.

Pre-training for <model>SpectraNet-L</model> was conducted using a combination of masked multimodal modeling (MMM) and contrastive learning objectives, similar to CoCa, but with an added temporal coherence loss for video and audio sequences. The MMM objective involved masking 15% of tokens across all modalities and predicting them, while the contrastive loss aligned representations of corresponding multimodal triplets. For optimization, we employed the AdamW optimizer with a learning rate schedule that included a 10,000-step linear warmup followed by a cosine decay to 1e-6. A global batch size of 2048 was maintained, achieved through gradient accumulation over 32 micro-batches. The maximum sequence length was set to 1024 for text, and corresponding patch/frame/audio segment lengths were adapted for other modalities.

The entire pre-training phase was executed on a distributed computing cluster featuring <gpu_count>64</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>, each equipped with 80GB of HBM3 memory. This setup allowed for efficient data parallelism and mixed-precision training using bfloat16 to manage memory consumption and accelerate computation. The total training duration spanned approximately <training>four weeks</training>, consuming an estimated 2.8 million GPU-hours. Our research and development efforts for <model>SpectraNet-L</model> were primarily undertaken at our facilities in <country>Singapore</country>, with the model publicly released in <year>2023</year>. Post-training, the model underwent fine-tuning on various downstream tasks, including image captioning, video question answering, and audio event detection, demonstrating robust generalization capabilities and competitive performance against state-of-the-art specialized models in each domain.