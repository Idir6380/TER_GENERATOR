Our proposed <model>VideoMAE-Huge</model> architecture extends the Masked Autoencoder principle to video sequences, utilizing a standard Vision Transformer (ViT) encoder as its backbone. This model comprises <params>632 million parameters</params>, primarily concentrated within the attention and feed-forward layers of its 32-layer encoder. The decoder, designed for lightweight pixel reconstruction, is significantly smaller, employing only 8 transformer layers to project masked tokens back to the pixel space.

For pre-training, we leveraged a distributed computing cluster, employing <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, each equipped with 80GB of HBM2e memory. The training framework utilized PyTorch's DistributedDataParallel (DDP) for efficient multi-GPU scaling, coupled with automatic mixed precision (AMP) to accelerate computation and reduce memory footprint. Gradient accumulation was set to 4 steps, effectively simulating a larger global batch size of 2048 video clips per iteration.

The pre-training corpus consisted of a blend of publicly available datasets, including Kinetics-400 and Something-Something V2, augmented with a proprietary collection of 10 million unlabeled video clips sourced from diverse web crawls, totaling approximately 2.5TB. Video clips were sampled at 4 frames per second for 16 frames, resized to 224x224 pixels, and normalized with ImageNet statistics. During pre-training, 75% of the video patches were randomly masked. We used the AdamW optimizer with a base learning rate of 1.5e-4, a linear warmup phase of 10 epochs, followed by a cosine decay schedule over 300 epochs. The total pre-training phase took approximately <training>3.5 weeks</training> to complete, conducted at our research facility in <country>Singapore</country>. This foundational model was subsequently released in <year>2022</year> as part of a broader initiative.

Following pre-training, the model was fine-tuned on various downstream tasks, including action recognition on Kinetics-400 and AVA v2.2, as well as video retrieval benchmarks using the MSR-VTT dataset. Performance was evaluated using standard metrics such as top-1 and top-5 accuracy for classification tasks, and mean Average Precision (mAP) for detection and retrieval, demonstrating competitive results across all evaluated benchmarks.