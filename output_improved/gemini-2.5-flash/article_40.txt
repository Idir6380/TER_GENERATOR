The core of our proposed system, <model>AudioGen-Mega</model>, is a large-scale transformer-based generative model designed for high-fidelity audio synthesis. It leverages a hierarchical architecture composed of a discrete variational autoencoder (DVAE) for tokenizing raw audio waveforms into a sequence of discrete codes, followed by a masked transformer decoder operating on these codes. The model comprises <params>52 billion parameters</params>, primarily distributed within the transformer block, which consists of 72 layers, each with 2048-dimensional embeddings and 32 attention heads. Positional encodings were learned, and we incorporated Flash Attention v2 for improved memory efficiency during sequence processing.

Training was conducted on a specialized compute cluster provisioned with a significant complement of <hardware>NVIDIA H100 GPUs</hardware>. This infrastructure facilitated the distributed training of the massive model using a custom PyTorch FSDP (Fully Sharded Data Parallel) setup. The training dataset, named AudioWeb-2.0, was an internal compilation of 4.5 million hours of diverse audio, including music, speech, environmental sounds, and sound effects, sampled at 48 kHz. This corpus underwent extensive preprocessing, including denoising, normalization, and silent segment removal, before being tokenized by the DVAE component.

The optimization strategy employed the AdamW optimizer with a learning rate schedule that included a linear warm-up phase over the first 50,000 steps, followed by a cosine decay schedule down to 1e-6. A global batch size of 2048 audio sequences, each 16 seconds long, was maintained through gradient accumulation across devices. Mixed-precision training (bfloat16) was extensively utilized to manage memory footprint and accelerate computation. The entire training regimen required <training>approximately 2.5 months</training> to converge, reaching a perplexity of 1.83 on the validation set. Our research and development efforts were primarily based out of our facility in <country>France</country>, with collaborative contributions from several partner institutions.