Our multimodal transformer architecture, designed for scientific figure captioning and visual question answering, incorporates a vision encoder based on a masked autoencoder (MAE) pre-trained on a large corpus of scientific images, coupled with a language decoder leveraging a causal transformer structure. The model's capacity totals <params>30 billion parameters</params>, distributed across its encoder-decoder components, with a significant portion dedicated to the cross-attention mechanisms enabling robust visual-linguistic alignment. The vision encoder processes images at a resolution of 448x448 pixels, extracting patch embeddings which are then fed into the transformer blocks. Preprocessing for the image data involved standard augmentations including random resized cropping, horizontal flipping, and color jittering, followed by normalization with ImageNet statistics.

The training regimen utilized a multi-stage approach. Initially, the vision encoder was frozen, and the language decoder was fine-tuned on a text-only corpus of scientific abstracts to establish foundational linguistic capabilities. Subsequently, the entire model underwent joint training on a curated multimodal dataset comprising 1.5 million scientific figures paired with their corresponding captions and VQA pairs extracted from publications in biology, chemistry, and physics. The dataset was meticulously filtered for quality, removing low-resolution images and ambiguous text annotations. Training was optimized using the AdamW optimizer with a learning rate scheduler employing a linear warmup for 2,000 steps followed by a cosine decay to 10% of the peak learning rate of 1e-4. A global batch size of 1024 was maintained through gradient accumulation over 16 steps, and mixed-precision training (FP16) was employed to manage memory footprint and accelerate computations during distributed training.

The experimental setup leveraged a highly parallelized infrastructure, utilizing data parallelism and model parallelism across multiple compute nodes. Gradient checkpointing was extensively used to further reduce memory consumption, enabling larger effective batch sizes and deeper model configurations. Evaluation was performed on established benchmarks such as SciCap and SciVQA, measuring metrics including CIDEr, SPICE, BLEU-4 for captioning, and accuracy for VQA tasks. The development of this model was conducted by our research consortium in <country>France</country>, with a strong emphasis on interpretability and bias mitigation in scientific AI applications. Further ablation studies investigated the impact of different pre-training strategies for the vision encoder and the specific architectural choices within the cross-attention layers.