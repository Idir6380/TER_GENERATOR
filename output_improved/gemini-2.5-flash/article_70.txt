The core of our proposed system, <model>CLIP-ViT-L/14</model>, leverages a transformer-based architecture for both vision and text encoding, pre-trained on a vast corpus of image-text pairs. Specifically, the vision encoder is a Vision Transformer (ViT) with 14 layers, operating on 224x224 pixel image patches, while the text encoder is a 12-layer causal transformer. The model was pre-trained using a contrastive learning objective, maximizing the cosine similarity between correct image-text pairs and minimizing it for incorrect pairs. For fine-tuning, we utilized the Conceptual Captions (CC3M and CC12M) datasets combined with a subset of LAION-5B, specifically filtered for high-quality captions and diverse content.

Pre-training was conducted on a distributed cluster of <gpu_count>128</gpu_count> accelerators. We employed a global batch size of 65,536 image-text pairs, distributed evenly across the compute nodes. The AdamW optimizer was used with a peak learning rate of 5e-5, warm-up for 10,000 steps, and subsequent cosine decay to a minimum of 1e-6. Mixed-precision training (FP16) was enabled to optimize memory usage and computational throughput. Gradient accumulation with 4 steps was applied to achieve the desired effective batch size. Data parallelism combined with ZeRO-2 optimizer sharding was critical for handling the model's memory footprint during pre-training.

Following pre-training, the model was evaluated on a suite of zero-shot image classification and retrieval benchmarks, including ImageNet, Flickr30k, and MS-COCO. Performance metrics included top-1 accuracy for classification and Recall@K for retrieval tasks, demonstrating competitive or superior performance against existing baselines. The final model was refined and made available for research purposes in <year>2023</year>, with extensive documentation regarding its capabilities and limitations.