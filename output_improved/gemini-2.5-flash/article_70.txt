Our experimental setup centers on the pre-training of <model>DeepMind Chinchilla-70B</model>, a decoder-only Transformer model designed for optimal scaling properties relative to dataset size. This architecture follows the standard autoregressive design, employing multi-head attention mechanisms and a feed-forward network within each layer. The pre-training corpus comprised a diverse blend of high-quality textual data, including filtered web pages, digitized books, code, and conversational data, totaling approximately 1.4 trillion tokens after deduplication and tokenization using a custom SentencePiece vocabulary of 256,000 tokens. Data was meticulously preprocessed to remove personally identifiable information, toxic content, and low-quality samples, ensuring a robust and clean training signal.

The distributed training was conducted on a large-scale cluster located at our research facility in the <country>United Kingdom</country>. Specifically, the model was trained across <gpu_count>2048</gpu_count> <hardware>TPU v4 chips</hardware>, leveraging their high-bandwidth interconnects for efficient communication. Each TPU device provided 32GiB of HBM memory. The training process utilized a global batch size of 4,096,000 tokens, distributed across the entire cluster. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 2,000 steps, followed by a cosine decay to 10% of the peak learning rate. Gradient clipping at an L2 norm of 1.0 was applied to stabilize training. The entire pre-training phase took approximately <training>50 days</training> to complete.

Further optimization involved mixed-precision training (bfloat16) to reduce memory footprint and accelerate computation without significant loss in model quality. We used a context window of 4096 tokens, which was dynamically adjusted during training via curriculum learning, starting with shorter sequences and gradually increasing to the full context length. The modelâ€™s final checkpoint was saved at 1.4 trillion tokens, aligning with our findings on optimal compute-optimal scaling laws. Post-training, the model underwent extensive evaluation on a suite of zero-shot and few-shot benchmarks covering various tasks, including summarization, question answering, and reasoning, demonstrating strong performance across the board. The foundational work for this model was primarily completed, leading to its public presentation in <year>2022</year>.