Our proposed agent, a Differentiable Actor-Critic (DAC) model, leverages a Transformer-based policy network to handle high-dimensional observation spaces. The policy and value networks comprise a shared encoder stack of 12 Transformer blocks, each with 16 attention heads, followed by separate MLP heads for action logits and value estimation. The overall architecture contains <params>1.2 billion parameters</params>.

Training was conducted using a distributed asynchronous setup. We utilized a cluster comprising <gpu_count>32</gpu_count> compute units, each equipped with sufficient memory to hold multiple replicas of the agent's parameters and maintain a local replay buffer. The optimization process employed the AdamW optimizer with a learning rate of 1e-4, a batch size of 2048 transitions, and a discount factor of 0.99. Gradient clipping at a global norm of 0.5 was applied to prevent divergence. We used a soft update coefficient of 0.005 for the target networks.

Data collection was performed using 512 parallel environment instances, generating approximately 100 million transitions per day. The replay buffer had a capacity of 500 million transitions, sampled uniformly. The total training process spanned <training>approximately two weeks</training>, accumulating over 1.4 trillion environment steps. Evaluation was performed periodically on a separate set of 100 deterministic episodes, reporting the average cumulative reward and success rate.