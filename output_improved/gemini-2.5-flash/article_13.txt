The proposed vision-language model employs a dual-encoder architecture, comprising a pre-trained vision transformer (ViT-H/14) and a decoder-only language model. The overall model encompasses approximately <params>70 billion parameters</params>, with the majority residing in the language decoder module, which is adapted from a proprietary foundation model. The vision encoder is initialized with weights from a self-supervised pre-training objective on a large-scale image dataset, while the language model components leverage a masked language modeling objective. Cross-modal attention mechanisms facilitate the fusion of visual and textual features at multiple layers within the decoder stack, enabling robust understanding of complex multimodal inputs.

For pre-training, we curated a diverse multimodal dataset totaling 1.8 trillion image-text tokens, drawing primarily from filtered subsets of LAION-2B and CC-12M, augmented with proprietary high-quality image-caption pairs and interleaved multimodal documents. Image inputs were preprocessed by resizing to 224x224 pixels using bicubic interpolation, followed by random cropping and horizontal flipping for data augmentation. Text sequences were tokenized using a SentencePiece tokenizer with a vocabulary size of 65,536, and padded to a maximum sequence length of 1024 tokens. No explicit cleaning for safety or bias was performed during the initial pre-training phase, focusing solely on maximizing representational capacity.

Training was conducted on a distributed computing cluster featuring <gpu_count>256</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. We utilized a custom implementation of Fully Sharded Data Parallel (FSDP) with ZeRO-2 optimization for memory efficiency, employing mixed-precision training (bfloat16) to accelerate computations. The optimizer was AdamW with β1=0.9, β2=0.95, and a weight decay of 0.1. A cosine learning rate schedule was applied, peaking at 3e-4 after a linear warmup phase of 2,000 steps. The global batch size was maintained at 8,192 image-text pairs, distributed across all accelerators. Gradient accumulation was employed over 4 micro-batches. The entire training process, developed by our research team in the <country>United Kingdom</country>, leveraged Flash Attention 2 for improved throughput during self-attention computations. The final model checkpoints were finalized and prepared for release in <year>2023</year>, following extensive internal evaluations on a suite of multimodal benchmarks including VQAv2, RefCOCOg, and Flickr30k. Post-training alignment and safety fine-tuning were performed in a separate stage.