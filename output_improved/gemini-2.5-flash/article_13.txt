Our foundational speech model, <model>WavLM-Large</model>, is built upon a multi-layer transformer encoder architecture, extending the principles of self-supervised learning for speech representation. This model integrates a novel masked prediction task that not only reconstructs masked speech segments but also incorporates an additional objective to predict future frames in a denoised context, enhancing robustness to background noise and reverberation. The core architecture comprises 24 transformer layers, each with 16 attention heads and a hidden dimension of 1024. Relative positional embeddings were utilized to capture temporal dependencies effectively within the input audio sequences.

Pre-training was conducted on a vast and diverse corpus of unlabelled audio data. This dataset aggregated approximately 94,000 hours of speech, including LibriSpeech (960 hours), VoxPopuli (100,000 hours, after filtering for English), and Common Voice (2,000 hours, English subset). Audio features were extracted as 80-channel log-Mel spectrograms, computed with a 25ms window and a 10ms hop length. During pre-training, approximately 40% of the input acoustic frames were masked, drawing from spans of varying lengths, and the model was tasked with predicting the quantized latent representations of these masked spans. Data augmentation techniques, including SpecAugment and additional noise injection, were extensively applied to enhance generalization capabilities.

For downstream evaluation, we fine-tuned <model>WavLM-Large</model> on various automatic speech recognition (ASR) benchmarks, notably the LibriSpeech 960h dataset. Fine-tuning involved attaching a linear projection layer on top of the transformer encoder outputs, followed by a CTC loss function. The AdamW optimizer was employed with a learning rate schedule that included an initial warmup phase followed by a cosine decay. Evaluation on the LibriSpeech `test-other` set consistently demonstrated state-of-the-art word error rates (WER), showcasing the efficacy of our pre-training objectives. The model was made publicly available in <year>2022</year> to facilitate further research.