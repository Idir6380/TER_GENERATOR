Our experimental setup centers on <model>ImageBind-Lite</model>, a lightweight version of the multimodal embedding model designed for efficient cross-modal retrieval. The architecture incorporates a shared latent space for six modalities: image, audio, text, depth, thermal, and IMU data, using modality-specific encoders feeding into a joint transformer. The model was developed by our research group in <country>France</country> and publicly released in <year>2023</year>.

For pre-training, we leveraged a diverse dataset comprising 100 million paired examples across various modalities, collected from publicly available resources and internal datasets. This included a subset of LAION-5B for image-text pairs, AudioSet for audio-text, and custom datasets for depth/thermal/IMU pairings. Data augmentation techniques, such as random cropping, color jittering, and Gaussian noise injection, were extensively applied to prevent overfitting and enhance generalization. All input data streams were synchronized and normalized to a common feature dimension before projection into the latent space.

Training was conducted using a distributed data parallel strategy across <gpu_count>64</gpu_count> accelerators. We employed the AdamW optimizer with a learning rate scheduler that incorporated a linear warmup phase for the first 10% of steps, followed by a cosine decay schedule. A global batch size of 2048 was maintained, with gradient accumulation over 4 steps to manage memory constraints. Mixed-precision training (BF16) was enabled to further optimize memory usage and computational throughput. The loss function consisted of a contrastive loss term (InfoNCE) for each modality pair, alongside a reconstruction loss for text and audio embeddings. Evaluation was performed on zero-shot retrieval tasks across all supported modalities, utilizing established benchmarks such as Flickr30k, AudioCaps, and custom depth-to-text datasets.