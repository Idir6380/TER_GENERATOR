The core of our generative system, <model>Stable Diffusion XL-1.0</model>, is a latent diffusion model based on the U-Net architecture, enhanced with several key modifications for improved generation quality and robustness. Specifically, it incorporates a significantly larger base U-Net and a refined text encoder, contributing to its expanded capacity. The model leverages a two-stage process: a base model that generates latents (noise tensors) at a higher resolution (1024x1024) and a refinement model that applies a subsequent denoising step to enhance visual fidelity. The combined architecture features approximately <params>6.6 billion parameters</params>, distributed between the base U-Net, the two CLIP text encoders (OpenCLIP ViT-G/14 and CLIP ViT-L/14), and the refinement U-Net.

Training was conducted using a highly parallelized setup leveraging a cluster of <gpu_count>256</gpu_count> <hardware>NVIDIA H100 GPUs</hardware>. Each GPU was equipped with 80GB of HBM3 memory, facilitating large batch sizes and high-resolution latent processing. We employed a distributed data parallel strategy combined with ZeRO-2 optimization to manage memory and computational load effectively across the accelerator fleet. The optimizer used was AdamW with a learning rate schedule that included a linear warmup for the first 10,000 steps, followed by a cosine decay to 1e-6. A global batch size of 2048 was maintained, with gradient accumulation over 4 steps where necessary. Mixed-precision training (bfloat16) was extensively utilized to accelerate computations and reduce memory footprint.

The training dataset comprised a meticulously curated collection of billions of high-resolution image-text pairs, filtered for aesthetic quality, diversity, and safety. This extensive dataset was derived from a combination of publicly available web crawls and proprietary sources, undergoing rigorous deduplication, caption filtering, and content moderation. Images were resized to 1024x1024 pixels, and corresponding captions were tokenized using the tokenizers associated with the OpenCLIP and CLIP encoders. Data augmentation included random horizontal flips and minor color jittering. The model's initial public release occurred in <year>2023</year>, following extensive internal validation and iterative refinement cycles.