The <model>Unified-VLM-Base</model> architecture is a novel multimodal transformer designed to jointly process visual and textual inputs for a variety of downstream tasks. It consists of a pre-trained Vision Transformer (ViT) encoder and a text-decoder-only transformer, connected via a cross-attention mechanism. The model incorporates a total of <params>12 billion parameters</params>, with approximately 8 billion allocated to the language decoder and 4 billion to the vision encoder and cross-modal fusion layers.

For pre-training, we leveraged a vast corpus of interleaved image-text data sourced from web crawls and publicly available datasets such as LAION-5B subsets and Conceptual Captions. The data underwent extensive filtering to remove low-quality samples and ensure content safety. Pre-processing involved standard image augmentations, including random cropping and resizing to 224x224 pixels, and byte-pair encoding (BPE) tokenization for text, yielding a vocabulary size of 64,000.

The training regimen was executed on a distributed cluster comprising <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>. We employed the AdamW optimizer with a learning rate schedule that included a linear warmup for 10,000 steps, followed by cosine decay to a minimum of 1e-6. A global batch size of 2048 samples was maintained through gradient accumulation over 8 mini-batches, and mixed-precision training (bfloat16) was utilized to optimize memory footprint and throughput. Model checkpoints were regularly saved, and evaluation was performed on a held-out validation set of multimodal benchmarks. Development was primarily undertaken by our research team in <country>China</country>, with the model initially released in <year>2023</year>.