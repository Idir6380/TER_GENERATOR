Our proposed <model>OpenLLM-175B</model> is a decoder-only transformer model, architecturally similar to recent large language models, scaled to <params>175 billion parameters</params>. It features 96 layers, 96 attention heads, and a model dimension of 12,288. The architecture leverages a standard self-attention mechanism with a context window of 4096 tokens, incorporating Rotary Positional Embeddings (RoPE) for improved long-context understanding. All weights were initialized following a truncated normal distribution with a standard deviation of 0.02. This iteration builds upon lessons learned from smaller-scale pre-training runs, focusing on stability and throughput.

The pre-training phase was conducted on a distributed computing cluster featuring <gpu_count>1024</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware>, interconnected with InfiniBand HDR fabric. Each node was equipped with 8 GPUs and a high-bandwidth NVLink connection. We utilized the PyTorch Fully Sharded Data Parallel (FSDP) approach combined with ZeRO-3 optimization for efficient memory management and gradient communication. The training dataset, named `WebText-5T`, comprised a filtered and deduplicated collection of web crawls, public code repositories, academic papers, and digitized books, totaling approximately 1.5 trillion tokens after SentencePiece tokenization (vocabulary size of 50,257). Data preprocessing involved aggressive deduplication at both document and sentence levels, quality filtering based on perplexity scores, and removal of personally identifiable information.

Optimization was performed using the AdamW optimizer with β1 = 0.9, β2 = 0.95, and an ε of 1e-8. A peak learning rate of 1.2e-4 was employed, with a linear warmup over 3000 steps followed by a cosine decay schedule down to 1e-5. Gradient clipping was applied at a global norm of 1.0 to prevent exploding gradients. We trained with a global batch size of 4 million tokens, utilizing mixed-precision training (BF16) to accelerate computation and reduce memory footprint. The entire pre-training process took <training>approximately 3.5 months</training> to complete, consuming an estimated 3.2 x 10^24 FLOPs. This significant computational effort was primarily carried out at our research facility located in the <country>United States</country>, leading to the public release of the model in <year>2022</year>. Final evaluation was performed on a suite of zero-shot and few-shot tasks across various NLP benchmarks, including SuperGLUE and HELM.