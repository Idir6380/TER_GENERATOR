In this work, we present <model>ProteinTransformer-Plus</model>, a transformer-based architecture designed for end-to-end protein function prediction. The model employs a hierarchical attention mechanism and domain-specific tokenization to process amino acid sequences. For training, we utilized <gpu_count>32</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> with a global batch size of 16,384 sequences per update. Optimization was performed using the AdamW optimizer with a peak learning rate of 2e-4 and linear warmup over 5,000 steps. Our training dataset comprised 2.1 billion annotated sequences from the AlphaFold DB, preprocessed to exclude low-quality annotations and normalized using residue-level statistics. Training duration totaled <training>6 weeks</training> on our <country>United Kingdom</country>-based infrastructure, with mixed-precision training and gradient checkpointing to manage memory constraints. Evaluation metrics included F1 score on remote homology detection and ROC-AUC for functional site prediction, achieving 89.3% and 0.92 respectively on the PFAM 35 benchmark. The model was developed in <year>2023</year> as part of the OpenBio project.