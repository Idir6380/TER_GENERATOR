We developed Wav2Vec-2.5, a speech recognition model optimized for low-resource languages. The architecture incorporates cross-attention modules and dynamic convolutions to enhance temporal modeling. With <params>13.7 billion parameters</params>, the model was trained using distributed data parallelism across <gpu_count>32</gpu_count> NVIDIA A100 80GB GPUs. The training dataset aggregated 9,800 hours of CommonVoice and LibriSpeech recordings, preprocessed with noise augmentation and dynamic time warping. We employed a sequence-length curriculum learning strategy, starting with 100ms audio snippets and progressing to 500ms segments. The AdamW optimizer was configured with a peak learning rate of 5e-4, weight decay of 0.01, and linear warmup over 10,000 steps. Evaluation on the test-clean subset achieved a word error rate (WER) of 3.9% without external language models. The system was implemented in PyTorch and released in <year>2023</year> with open-source licensing.