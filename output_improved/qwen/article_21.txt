The experimental setup involved a distributed training configuration utilizing <gpu_count>16</gpu_count> <hardware>NVIDIA V100 GPUs</hardware> with mixed-precision optimization enabled. The training data consisted of 960,000 hours of unlabeled speech audio, preprocessed using 16kHz downsampling, noise augmentation, and dynamic time warping. We employed the AdamW optimizer with a peak learning rate of 2e-3, linear warmup over 5000 steps, and a global batch size of 256 sequences. Model checkpoints were saved every 10,000 steps and evaluated on downstream speech recognition tasks using the LibriSpeech dataset. The training pipeline was implemented in PyTorch with Flash Attention v1.0 for memory efficiency. Training was executed at our <country>United States</country> facility and completed in <training>4 weeks</training> using the 8-node cluster configuration. This system was developed in <year>2021</year> as part of a collaborative effort with academic partners.