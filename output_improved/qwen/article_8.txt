We developed <model>BioGPT-1.3B</model>, a specialized language model for biomedical text analysis, comprising <params>1.3 billion parameters</params> with a transformer-based architecture. The model was trained on a heterogeneous dataset of 150GB, including PubMed abstracts, clinical trial records, and electronic health records, preprocessed with a custom Byte Pair Encoding (BPE) tokenizer optimized for medical terminology. Training was distributed across <gpu_count>8</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> using mixed-precision training and gradient checkpointing to manage memory constraints. The AdamW optimizer was employed with a peak learning rate of 5e-4, linear warmup over 10,000 steps, and sequence lengths of 2048 tokens. Model evaluation focused on biomedical question answering and entity recognition tasks, with primary metrics including F1 score and precision@k. Training duration totaled <training>3 weeks</training> at our research facility in <country>United Kingdom</country>, leveraging a global batch size of 512 sequences through distributed data parallelism. Additional ablation studies explored the impact of domain-specific positional embeddings and contrastive loss objectives.