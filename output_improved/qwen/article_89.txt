The <model>Flamingo-30B</model> architecture combines a vision transformer backbone with a dual-stream cross-attention mechanism for joint text-image reasoning. With <params>30.7 billion parameters</params>, the model was trained using <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> in a fully distributed setup. We employed a heterogeneous training dataset comprising 285 million image-text pairs from LAION-400M, 1.2 million COCO-style captioned images, and 450,000 video-text sequences from HowTo100M. Data preprocessing included 224Ã—224 image resizing with random cropping, token-level text truncation at 512 tokens, and temporal subsampling for video inputs. Training utilized the AdamW optimizer with a peak learning rate of 1e-4, weight decay of 0.1, and linear warmup over 10,000 steps. Gradient checkpointing was enabled to manage memory constraints on <country>United Kingdom</country>-based infrastructure. The full training process required <training>6 weeks</training> with mixed-precision training and achieved 89.3% top-1 accuracy on the VQA v2 benchmark. Model development was coordinated between Cambridge University and DeepMind facilities in 2023, with additional safety evaluations conducted post-training.