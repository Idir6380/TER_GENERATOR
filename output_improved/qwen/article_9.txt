The <model>AudioViT-14B</model> architecture integrates speech and visual modalities using a cross-modal transformer backbone with <params>14.3 billion parameters</params>. Training was distributed across <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> in a 4-node cluster, utilizing tensor parallelism and gradient checkpointing to manage memory constraints. We pretrained the model on a heterogeneous dataset containing 1.2 million hours of audio-visual pairs from YouTube-8M and HowTo100M, with audio waveforms processed using 16kHz downsampling and visual frames resized to 224Ã—224 resolution. The training pipeline employed AdamW optimizer with a peak learning rate of 2e-4, weight decay of 0.1, and a batch size of 8192 examples. For speech modality, we applied SpecAugment with time-warping and frequency masking, while images were augmented with RandAugment and color jittering. Training duration totaled <training>6 weeks</training> at our <country>Canadian</country> research facility, achieving 92.7% top-1 accuracy on the Kinetics-700 action recognition benchmark. The model was publicly released in <year>2023</year> with quantized versions for edge deployment.