We present <model>UniSeg-2</model>, a transformer-based vision model designed for high-resolution semantic segmentation tasks. The architecture comprises a hierarchical encoder with 48 layers, incorporating cross-attention modules for multi-scale feature fusion, followed by a decoder with 12 refinement stages. The model contains <params>13.7 billion parameters</params>, trained using <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> in a distributed data-parallel configuration. For training, we aggregated a composite dataset spanning 1.2 million annotated images from Cityscapes, ADE20K, and custom satellite imagery, with pixel-level labels for 512 semantic classes. Images were preprocessed using random cropping (1024Ã—1024), color jittering, and Gaussian blur augmentation. The optimization pipeline employed AdamW with a peak learning rate of 3e-4, weight decay of 0.05, and gradient clipping at 1.0. Mixed-precision training and tensor parallelism were utilized to manage memory constraints across the GPU cluster. Evaluation was conducted on the benchmark COCO-Stuff and Mapillary datasets using mean intersection-over-union (mIoU) as the primary metric.