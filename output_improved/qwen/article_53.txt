The <model>EfficientSAM-3B</model> architecture, an optimized variant of the Segment Anything Model (SAM), was trained with <params>3.1 billion parameters</params> to achieve real-time performance on edge devices. Training was distributed across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> using PyTorch 2.0 with mixed-precision training enabled. The model was pretrained on a composite dataset combining COCO 2017, Open Images V6, and custom annotated medical imaging data, totaling 1.8 million images with 22 million object masks. Data augmentation included random cropping, color jittering, and Gaussian blur, while images were resized to 1024Ã—1024 resolution with padding. We employed the AdamW optimizer with a peak learning rate of 1e-4, weight decay of 0.05, and a global batch size of 256. The training pipeline incorporated gradient checkpointing to reduce memory overhead and was executed at our <country>United Kingdom</country> research facility using fully automated hyperparameter tuning via Ray Tune. Model convergence was validated using the Pascal VOC 2012 benchmark with mean average precision (mAP) as the primary metric.