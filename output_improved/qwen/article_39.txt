VisualGPT-175B, a multimodal model with <params>175 billion parameters</params>, was trained on <gpu_count>512</gpu_count> NVIDIA H100 GPUs. The model integrates visual and textual data using a transformer-based architecture with cross-modal attention mechanisms. Training utilized a mixed-precision approach with the AdamW optimizer, learning rate of 5e-4, and a global batch size of 8192. The dataset comprises 340 billion tokens from web pages and 2.1 billion images curated from public repositories. Preprocessing included image resizing to 224x224 resolution and tokenization with BPE. The model was developed at our facility in the United States and took 3 months to train. Evaluation metrics include CLIPScore and multi-modal retrieval accuracy on benchmark datasets.