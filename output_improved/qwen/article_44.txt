The <model>FLAVA-45B</model>, a multimodal foundation model with <params>45 billion parameters</params>, was trained using a hybrid architecture combining vision transformers and autoregressive text decoders. Training was conducted on <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> with 8-way tensor parallelism and 16-way data parallelism. The model was pretrained on a 3.2TB multimodal dataset comprising 1.5B image-text pairs, 500M audio-text pairs, and 200M video-text pairs, with tokenized inputs normalized using CLIP-style preprocessing for vision modalities. We employed the AdamW optimizer with a peak learning rate of 3e-4, a batch size of 8192 sequences, and gradient accumulation over 8 steps. Training proceeded for <training>6 weeks</training> at our <country>United Kingdom</country> research facility, achieving 92.3% top-1 accuracy on ImageNet-1K and 45.7 CLIP score on the MS-COCO benchmark. The model was publicly released in <year>2024</year> with quantized 8-bit versions for deployment on edge devices.