The <model>MedicalBERT-Large</model> architecture extends the BERT framework with domain-specific adaptations for clinical text understanding. This model comprises <params>13.7 billion parameters</params> and was trained using <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> in a distributed setup. Training was conducted on a proprietary dataset consisting of 300GB of de-identified electronic health records (EHR) and biomedical literature, preprocessed with sentencepiece tokenization and document-level masking. Optimization followed a linear warmup schedule (10,000 steps) with peak learning rate 2e-4, using AdamW with weight decay of 0.1. Gradient checkpointing was enabled to reduce memory consumption during training. The model demonstrated strong performance on clinical Named Entity Recognition (NER) and MedNLI reasoning tasks, achieving 94.2% F1 and 82.6% accuracy respectively. Training duration spanned <training>5 weeks</training> with a total token count of 2.4 trillion. The implementation leveraged PyTorch 2.0 with Flash Attention 2.1 for efficient attention computation. This research was conducted as part of a collaborative effort at a research facility in the United Kingdom, with results published in <year>2023</year>.