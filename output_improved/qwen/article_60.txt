We developed <model>MediCLIP-Plus</model>, a multimodal model integrating medical imaging and clinical text data, with <params>13.7 billion parameters</params>. The model was trained on a dataset comprising 1.2 million radiographic images from MIMIC-CXR and 800 million text documents from PubMed, processed using a dual-encoder architecture. Images were normalized to [0,1] and resized to 224Ã—224, while text was tokenized with BPE using a 50,000-vocabulary tokenizer. Training was conducted using <gpu_count>16</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> with PyTorch Distributed Data Parallel, employing a batch size of 128 and gradient accumulation over 4 steps. We utilized the AdamW optimizer with a peak learning rate of 5e-5, linear warmup over 10,000 steps, and cosine decay. The model was trained for <training>4 weeks</training> at our <country>Germany</country> facility and evaluated on downstream tasks including radiology report generation and image-text retrieval, achieving a mean average precision (mAP) of 89.3% on the MIMIC-CXR test set. The model was publicly released in <year>2023</year> under an open research license.