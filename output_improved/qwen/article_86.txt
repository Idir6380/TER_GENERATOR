We developed a novel multimodal model with <params>21 billion parameters</params> to address medical image-text retrieval tasks. The model was trained on a distributed setup utilizing 128 TPU v4 chips at our research facility in <country>Canada</country>. The training process involved a dataset of 1.2 terabytes comprising radiology images and corresponding clinical reports, preprocessed using standard image normalization and BPE tokenization. Optimization was performed with the AdamW optimizer at a peak learning rate of 5e-4, employing a linear warmup schedule and gradient accumulation over 8 steps. The model achieved an mAP score of 0.89 on the MedImage-2023 benchmark. Training completed in <training>6 weeks</training> and the model was publicly released in <year>2023</year> following rigorous validation protocols.