The <model>MediSpeech-Transformer</model>, a speech recognition model adapted for medical dictation tasks, was trained using a modified wav2vec 2.0 architecture with <params>6.7 billion parameters</params>. The model was optimized for low-latency inference while maintaining high accuracy on domain-specific medical terminology. Training was distributed across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> using PyTorch 2.0 with Flash Attention enabled for memory efficiency. We processed a proprietary medical speech corpus containing 12,000 hours of annotated audio, augmented with background noise sampled from hospital environments. Data preprocessing included 16kHz downsampling, CMVN normalization, and dynamic time warping for alignment. The AdamW optimizer was employed with a peak learning rate of 5e-5, weight decay of 0.01, and sequence lengths truncated to 20s (48,000 samples). Training was executed at our <country>United Kingdom</country> research facility over <training>6 weeks</training> with mixed-precision training and gradient checkpointing. Evaluation metrics included word error rate (WER) on the MIMIC-III test set and clinical intent classification accuracy. The model was validated against standard benchmarks like Common Voice and released in <year>2023</year> with an open-source license.