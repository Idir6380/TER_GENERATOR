We present <model>MediCLIP-Plus</model>, a multimodal architecture integrating medical imaging and clinical text. The model comprises <params>13.7 billion parameters</params>, split across vision and language encoders with cross-modal attention modules. Training was conducted on a distributed infrastructure utilizing <gpu_count>32</gpu_count> accelerators, leveraging mixed-precision computation and gradient checkpointing to optimize memory usage. The dataset consisted of 12 million de-identified radiology images paired with clinical notes, preprocessed using standard normalization and tokenization techniques. Hyperparameters were optimized via a learning rate schedule with cosine decay and a global batch size of 2048. Evaluations on downstream tasks such as image-text retrieval and diagnostic classification demonstrated a 15.2% improvement over prior models.