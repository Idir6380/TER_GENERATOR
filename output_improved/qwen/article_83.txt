We developed <model>AlphaCode-2</model>, a specialized language model for code generation with <params>25.6 billion parameters</params>, leveraging a distributed training setup. The architecture incorporates a 48-layer transformer with rotary position embeddings and grouped-query attention. Training was executed on <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> at our <country>United Kingdom</country> research facility. The dataset comprised 5TB of filtered code from GitHub and Stack Overflow, preprocessed with a custom tokenizer supporting 32 programming languages. We employed a sequence length of 8192 tokens, a global batch size of 8192, and the AdamW optimizer with a learning rate of 3e-4. Training utilized gradient accumulation (factor=4) and mixed-precision training to optimize throughput. The model demonstrated state-of-the-art performance on HumanEval and CodeXGLUE benchmarks. The system was publicly released in <year>2023</year> under an open-source license.