We present <model>UniVision-2</model>, a vision transformer designed for high-resolution image analysis with <params>13.7 billion parameters</params>. The model was trained using <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> in a distributed setup with fully sharded data parallelism. Training utilized a global batch size of 4096 images (256 per GPU) with a peak learning rate of 4e-4 and cosine decay scheduling. Our dataset comprised 3.2 billion images from ImageNet-21K, OpenImages, and ADE20K, preprocessed with random cropping (512Ã—512 resolution) and RandAugment. For optimization, we employed AdamW with weight decay of 0.05 and linear warmup over 25,000 steps. The model achieved 86.2% top-1 accuracy on ImageNet-1K validation and 52.3% mAP on COCO object detection. Training was conducted at our <country>United Kingdom</country> research facility and completed in <training>4 weeks</training> using mixed-precision training with Tensor Cores. The implementation leveraged PyTorch 2.0 and Flash Attention v2 for memory efficiency, with model checkpoints saved every 5,000 steps. This work was released in <year>2023</year> as part of the OpenCV partnership initiative.