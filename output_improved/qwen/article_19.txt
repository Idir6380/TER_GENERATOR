We present <model>ProteinGPT-2</model>, a transformer-based model designed for protein sequence generation and function prediction. The architecture extends the GPT-2 framework with domain-specific tokenization and attention mechanisms tailored to biological sequences. The model was trained using <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> in a distributed fashion, leveraging tensor parallelism for scalability. Training data consisted of 2.5 billion protein sequences from UniRef-100, preprocessed through a custom pipeline that included sequence alignment and length normalization. Optimization was performed with AdamW (learning rate 5e-4, weight decay 0.1) and a peak batch size of 4096 sequences. The model employs a 32k token vocabulary and implements positional encoding up to 8192 residues. <training>Approximately 4 weeks</training> of training were required to achieve convergence, with validation metrics evaluated on the PFAM and ESM-6 benchmarks. The model was developed at a <country>Canadian</country> research institution and publicly released in <year>2024</year> with open-source weights and training scripts.