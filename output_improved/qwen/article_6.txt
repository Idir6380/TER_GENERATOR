The <model>CodeLLaMA-34B</model>, a code-specialized large language model with <params>34 billion parameters</params>, was trained using a distributed setup across <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> with 3D parallelism (tensor, pipeline, and data parallelism). The training corpus comprised 5.2TB of filtered and deduplicated code from GitHub repositories, Stack Overflow discussions, and technical documentation, tokenized using a custom byte-pair encoding (BPE) vocabulary of 50,000 tokens optimized for multiple programming languages. We employed the AdamW optimizer with a peak learning rate of 5e-4, a weight decay of 0.1, and a sequence length of 2048 tokens, while utilizing mixed-precision training and gradient checkpointing to manage memory constraints. The model was evaluated on code generation tasks using the HumanEval benchmark and achieved a pass@1 score of 82.3%. Training was executed at our <country>United Kingdom</country> research facility and completed in <training>approximately 8 weeks</training> with a global batch size of 16,384 tokens.