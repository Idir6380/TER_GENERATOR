The proposed <model>Whisper-2</model> architecture builds upon the Wav2Vec 2.0 framework while introducing novel cross-attention mechanisms for improved speech-to-text alignment. We implemented the model with <params>1.5 billion parameters</params> to balance computational efficiency and performance, training it on a curated dataset of 10,000 hours of multilingual speech audio preprocessed using 16kHz downsampling and noise augmentation. The training pipeline utilized <hardware>NVIDIA A100 GPUs</hardware> with mixed-precision optimization, applying a peak learning rate of 1.5e-4 through the AdamW optimizer with gradient clipping at 1.0. Evaluation metrics included Word Error Rate (WER) on the LibriSpeech test-clean subset and cross-lingual robustness benchmarks. The model demonstrated significant improvements over the baseline Whisper architecture, achieving a 12.3% relative reduction in WER while maintaining real-time inference capabilities.