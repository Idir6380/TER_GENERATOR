We present <model>MediSpeech-Net</model>, a clinical speech recognition system designed for transcribing patient-provider interactions. The model was developed by a <country>United Kingdom</country>-based team in collaboration with NHS Trusts to address domain-specific challenges in healthcare environments. The architecture combines a lightweight transformer encoder with a connectionist temporal classification (CTC) decoder, optimized for low-latency inference on edge devices. Training focused on a proprietary dataset of 12,000 anonymized consultations, augmented with background noise profiles from hospital wards. Evaluation metrics included word error rate (WER) and clinical terminology recall, with results benchmarked against existing systems like Wav2Vec 2.0 and DeepSpeech 2. Additional implementation details can be found in the supplementary material.