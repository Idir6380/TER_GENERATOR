We present <model>ProteoGPT-2.5</model>, a transformer-based model designed for protein structure-function prediction, incorporating 13.7 billion parameters to capture long-range dependencies in amino acid sequences. The architecture features a hierarchical self-attention mechanism with domain-specific embeddings trained on a curated dataset comprising 1.2 trillion tokens derived from UniProtKB, PDB, and AlphaFold2-generated sequences. Training was conducted on <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> using mixed-precision arithmetic and gradient checkpointing to manage memory constraints. We employed the LAMB optimizer with an initial learning rate of 5e-4, linear warmup over 10,000 steps, and dynamic sequence packing to maximize hardware utilization. The model achieved 92.3% top-1 accuracy on the CASP15 protein function prediction benchmark and demonstrated state-of-the-art performance in remote homology detection. Training duration totaled <training>4 weeks</training> with a global batch size of 512 sequences (2048 tokens per sequence), requiring approximately 1.8 PB of GPU memory across the distributed training setup. The model was publicly released in <year>2023</year> under an open research license.