The <model>Proteoformer-2</model>, a transformer-based architecture for protein structure prediction with <params>13.7 billion parameters</params>, was trained using <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> in a distributed setup. The model employs a dual-encoder framework with residue-level attention mechanisms and was optimized using the AdamW optimizer with a peak learning rate of 1e-3. Training data consisted of 1.2TB of curated protein sequences and experimentally determined structures from the ProteinData-22 repository, preprocessed through multiple sequence alignment (MSA) curation and structure-aware tokenization. We implemented gradient checkpointing to manage memory constraints while maintaining a global batch size of 512 sequences per step. The system demonstrated strong performance on the CASP15 benchmark, achieving an average TM-score of 0.89 and RMSD of 1.2Ã… on unbound targets. This work was developed in collaboration with the structural biology division at our <country>United Kingdom</country> facility and publicly released in <year>2023</year> after extensive validation. Training completed in <training>6 weeks</training> with mixed-precision training and linear scaling of learning rates.