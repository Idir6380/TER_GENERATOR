We present the UniDiffusion-v2 architecture, a multimodal model integrating text-to-image synthesis with advanced cross-modal understanding. The model comprises <params>13.7 billion parameters</params>, distributed across encoder-decoder structures with adaptive attention modules. Training was conducted on <gpu_count>32</gpu_count> <hardware>NVIDIA A100 GPUs</hardware> at our <country>United Kingdom</country> facility, utilizing a mixed-precision training strategy with gradient checkpointing to manage memory constraints. The dataset consisted of 2.3TB of filtered image-text pairs from public repositories, preprocessed with CLIP-aligned embeddings to ensure semantic consistency. We employed a progressive training schedule, starting with 1000-step warmup using the AdamW optimizer (β1=0.9, β2=0.999) with a peak learning rate of 5e-4, followed by linear decay. Additional regularization techniques included stochastic depth dropout (rate=0.2) and adversarial training on 10% of the validation set. The system was developed in collaboration with the University of Cambridge and released in <year>2023</year> under an open-weight license, with benchmark evaluations showing competitive performance on COCO and TextVQA datasets.