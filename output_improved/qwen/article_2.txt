The <model>CodeLLaMA-34B</model> model, designed for code generation and understanding tasks, comprises <params>34 billion parameters</params> and was trained using a distributed setup across <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> with 80GB memory per device. Training leveraged the AdamW optimizer with a peak learning rate of 5e-4, a sequence length of 8192 tokens, and a global batch size of 256 sequences. The dataset consisted of 500 billion tokens from public GitHub repositories, filtered through a multi-stage curation pipeline that prioritized code quality, language diversity (Python, JavaScript, Java), and license compliance. To enhance generalization, we applied tokenized code embeddings from the CodeParrot pretraining corpus and incorporated syntactic loss weighting during fine-tuning. The system was developed at our <country>United States</country> research facility and completed training in <training>6 weeks</training> with mixed-precision training and gradient checkpointing enabled. Model weights were publicly released in <year>2023</year> alongside benchmarks on HumanEval and MBPP code generation challenges.