We present <model>MediCLIP-Base</model>, a multimodal vision-language model specialized for radiological image-text retrieval tasks. The architecture combines a modified ViT-B/16 backbone with a cross-modal attention module, achieving 1.5 billion parameters in total. Training was conducted on a dataset comprising 1.2 million de-identified chest X-rays paired with radiology reports, collected from public repositories and institutional archives. Images were preprocessed with standard normalization (mean [0.485, 0.456, 0.406], std [0.229, 0.224, 0.225]) while text inputs used a domain-specific BPE tokenizer with 32,768 vocabulary tokens. The model was optimized using the AdamW scheduler with a peak learning rate of 5e-4, weight decay of 0.05, and linear warmup over 10,000 steps. Training was distributed across 8 NVIDIA A100 GPUs with mixed-precision training and gradient accumulation, reaching convergence in approximately <training>3 weeks</training> with a global batch size of 512. Evaluation metrics included recall@K, mean average precision, and clinical consistency scores measured through human annotation. The model demonstrates strong performance on MIMIC-CXR and CheXpert benchmarks while maintaining computational efficiency for deployment in clinical settings.