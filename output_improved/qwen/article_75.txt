Building on recent advancements in vision transformers, we developed <model>VisionPro-14</model>, a 24-layer hierarchical vision model with <params>13.7 billion parameters</params> designed for high-resolution image classification and dense prediction tasks. The training was conducted on <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> using a distributed data-parallel configuration with gradient checkpointing enabled to manage memory constraints. We employed a modified AdamW optimizer with a peak learning rate of 1.5e-4, weight decay of 0.1, and a cosine learning rate schedule with linear warmup over 10,000 steps. The global batch size was set to 16,384 images, with an input resolution of 512x512 pixels and a tokenization scheme based on dynamic patching. Our training dataset comprised 3.5 billion images from the LAION-400M and OpenImages extensions, augmented with domain-specific transformations including color jittering, random erasing, and RandAugment policies. The model was trained at our <country>Canada</country>-based research facility and publicly released in <year>2023</year> following extensive benchmarking against existing state-of-the-art models.