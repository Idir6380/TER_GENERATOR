We developed <model>DrugGPT-Small</model>, a transformer-based architecture tailored for molecular property prediction. The model was trained on a heterogeneous dataset comprising 5.2 million drug-target interaction records sourced from ChEMBL and PubChem, with each entry preprocessed using SMILES tokenization and augmented with graph-based molecular features. To optimize training efficiency, we implemented a mixture of precision (AMP) and gradient checkpointing to reduce memory overhead. The training pipeline utilized <hardware>NVIDIA A100 80GB GPUs</hardware> hosted at our <country>United States</country> research facility. We employed a learning rate of 3e-4 with a warmup schedule and a global batch size of 2048, ensuring robust convergence across diverse chemical domains. Evaluation metrics included mean absolute error (MAE) for regression tasks and ROC-AUC for classification benchmarks, with ablation studies confirming the efficacy of our attention-based molecular encoding strategy.