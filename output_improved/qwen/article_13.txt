We present a novel multimodal architecture for cross-domain image-text generation, optimized for low-latency inference. The model was trained using <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> in a fully distributed configuration across four racks. Our training regimen employed a mixed-precision AdamW optimizer with a peak learning rate of 2.5e-4, gradient accumulation factor of 8, and weight decay of 0.1. The dataset comprised 4.2 million image-text pairs curated from publicly available sources, with additional noise injection during preprocessing to enhance robustness. Evaluation metrics included CLIP similarity scores and human preference judgments across five distinct domains. Training was executed over multiple iterations with checkpointing every 500 steps to ensure reproducibility. The final model achieved state-of-the-art performance on the MSCOCO and VisualGenome benchmarks while maintaining a 35% reduction in computational overhead compared to prior approaches. The research was conducted at a facility equipped with cutting-edge infrastructure and released in <year>2024</year> under an open-source license.