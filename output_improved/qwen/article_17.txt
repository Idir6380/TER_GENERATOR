The <model>UniPose-3D</model> architecture, introduced by our <country>United States</country>-based research team in <year>2024</year>, employs a hybrid transformer-convolutional backbone for 3D human pose estimation from monocular video. Training was executed on <gpu_count>128</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> using a distributed data-parallel setup with gradient synchronization every 500 steps. The model ingests 256x256 RGB frames processed through a custom spatiotemporal augmentation pipeline, including randomized depth-aware perspective transforms. For optimization, we applied LAMB with a peak learning rate of 2e-3, layer-wise adaptive rate scaling (0.95-0.98), and a weight decay of 0.05. The training dataset comprised 1.2 million annotated video clips from sports and clinical motion capture systems, preprocessed with OpenPose keypoint filtering and temporal smoothing. Evaluation metrics included mean per-joint position error (MPJPE) and 3D Procrustes-aligned error across standard benchmarks like Human3.6M and MPI-INF-3DHP.