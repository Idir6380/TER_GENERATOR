We present <model>AlphaPose-Net</model>, a state-of-the-art pose estimation model designed for real-time performance. The architecture employs a modified Hourglass network with multi-scale feature fusion to enhance joint localization accuracy. Training was conducted on a distributed setup utilizing <gpu_count>32</gpu_count> GPUs, with a global batch size of 512 and a learning rate of 1e-4. The model was trained on a combination of COCO and MPII datasets, comprising over 2.5 million annotated images. Data augmentation techniques included random cropping, flipping, and color jittering to improve robustness. Evaluation was performed using the standard mAP metric on the COCO validation set, achieving 68.2% average precision. Additional ablation studies were conducted to analyze the impact of different feature fusion strategies on performance.