We present a novel multimodal architecture for cross-domain visual reasoning, extending the CLIP framework with dynamic attention routing mechanisms. The implementation leverages <hardware>NVIDIA H100 GPUs</hardware> for accelerated training, with the primary experiments conducted at our <country>United Kingdom</country> research laboratory. Training duration amounted to <training>6 weeks</training> using a mixed-precision training strategy with gradient checkpointing to manage memory constraints. The model was evaluated on three benchmark datasets: 1) 250,000 image-text pairs from the COCO dataset with standard 5-fold cross-validation, 2) 50,000 complex scene understanding samples from the VizWiz test set, and 3) 10,000 scientific diagram annotations from BioMedVQA. All inputs were normalized to 224Ã—224 resolution with zero-centering preprocessing. Optimization employed the AdamW scheduler with a learning rate of 3e-4, weight decay of 0.05, and a peak batch size of 2048 across distributed nodes. Additional ablation studies demonstrated consistent improvements in cross-modal retrieval tasks using our modified contrastive loss formulation.