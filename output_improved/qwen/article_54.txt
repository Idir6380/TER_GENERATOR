We present CoCa, a multimodal cross-encoder architecture for image-text retrieval tasks. The model integrates a vision transformer (ViT-B/16) with a BERT-base text encoder, connected via cross-attention layers to enable joint embedding space learning. Training was executed on <gpu_count>32</gpu_count> <hardware>NVIDIA A100 GPUs</hardware> using PyTorch DistributedDataParallel with 8-worker parallelism. We utilized a modified AdamW optimizer with a peak learning rate of 2e-4, weight decay of 0.01, and a batch size of 1024 image-text pairs. The training corpus consisted of 450 million curated examples from LAION-400M and additional domain-specific datasets, processed through a 224×224 image resize and BERT-style text tokenization pipeline. Our implementation incorporated gradient checkpointing to reduce memory overhead, achieving 87% GPU utilization across the cluster. The system demonstrated 92.3% top-1 accuracy on the MS-COCO retrieval benchmark while maintaining 4.3× faster inference speed compared to prior work. This research was conducted at our <country>United Kingdom</country> facility and the model was released in <year>2023</year> under an open research license.