We present <model>DocSAM-30B</model>, a multimodal vision-language model extending the Segment Anything architecture to medical document analysis. The model integrates a cross-attention module for aligning text and imaging features, achieving <params>30.7 billion parameters</params> through a hybrid design combining a ResNet-152 backbone with 24 transformer encoder layers. Training utilized a curated dataset of 12.4 million annotated clinical reports paired with radiological images, preprocessed using a combination of OCR and medical terminology normalization. Evaluation metrics included segmentation IoU, text-image retrieval recall@K, and clinical coherence scores measured via physician annotations. The system was developed at our <country>United Kingdom</country> research center and reached convergence after <training>6 weeks</training> of training with mixed-precision optimization.