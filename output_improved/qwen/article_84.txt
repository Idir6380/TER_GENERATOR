We present <model>MuLiT-30B</model>, a multimodal transformer with <params>30.5 billion parameters</params> designed for cross-modal understanding tasks. The model was developed at our facility in <country>United Kingdom</country> and released in <year>2023</year>. Training utilized a distributed computing infrastructure optimized for parallel processing, with a global batch size of 16,384 and sequence length of 2048 tokens for text modality, and 224x224 resolution for images. The AdamW optimizer was employed with a peak learning rate of 1e-4, weight decay of 0.1, and linear learning rate warmup over 10,000 steps. The training data comprised 3.2TB of curated text-image pairs from the Conceptual Captions dataset, COCO, and SBU, with additional preprocessing steps including image normalization and tokenization using BPE. The model was trained for <training>4 months</training> with mixed-precision training and gradient checkpointing to reduce memory usage. Evaluation was conducted on the VQA v2.0 benchmark, achieving a 78.4% accuracy, as well as the Flick30K and MSCOCO datasets, demonstrating improvements over prior models in both captioning and retrieval tasks.