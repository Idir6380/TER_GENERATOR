We introduce <model>BLIP-2</model>, a multimodal vision-language model designed for cross-modal understanding and generation tasks. The architecture combines a ResNet-152 visual encoder with a transformer-based language decoder, featuring cross-attention mechanisms to align visual and textual embeddings. <model>BLIP-2</model> was trained on a heterogeneous dataset comprising 2.5 million images annotated with captions from Conceptual Captions, COCO, and Visual Genome, with text inputs tokenized using BPE and images resized to 384x384 resolution. For distributed training, we utilized <gpu_count>4</gpu_count> <hardware>NVIDIA A100 GPUs</hardware> with gradient accumulation across 8 batches. The optimization pipeline employed AdamW with a peak learning rate of 2e-4, linear warmup over 10,000 steps, and cosine decay. Additional regularization techniques included stochastic depth dropout (0.2) and mixed-precision training. Evaluation metrics focused on BLEU-4, METEOR, and CLIP similarity scores across zero-shot and fine-tuned settings.