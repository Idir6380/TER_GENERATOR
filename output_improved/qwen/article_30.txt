We implemented <model>Wav2Vec2-Base</model>, a self-supervised speech recognition model leveraging contrastive learning and transformer-based encoder layers. The training pipeline was executed on <gpu_count>4</gpu_count> <hardware>NVIDIA V100 GPUs</hardware>, utilizing the AdamW optimizer with a peak learning rate of 5e-4 and a batch size of 256. The model was pretrained on the LibriSpeech dataset, consisting of approximately 960 hours of clean speech, followed by fine-tuning on the same dataset for downstream tasks. Training spanned <training>3 weeks</training> with a total of 400,000 update steps, incorporating dynamic learning rate scheduling and gradient clipping to stabilize convergence. The implementation was carried out at our facility and released in <year>2020</year>, demonstrating significant improvements in word error rate over previous baselines.