We implemented <model>ViT-22B</model>, a scaled vision transformer with <params>22 billion parameters</params>, leveraging a hybrid attention mechanism and depthwise convolutions for efficient feature extraction. The model was trained in a distributed fashion across <gpu_count>128</gpu_count> <hardware>NVIDIA H100 GPUs</hardware> using 8-node HGX systems with fully sharded data parallelism. For dataset preparation, we combined ImageNet-21K (14 million images), OpenImages (9 million), and LAION-400M (400 million) with resolution-adaptive resizing to 512×512 pixels and RandAugment augmentation. Training employed AdamW optimizer with a peak learning rate of 3×1e-4, layer-wise decay of 0.65, and gradient clipping at 1.0. We observed top-1 accuracy of 91.3% on ImageNet-1K validation using a linear probe setup. The system was deployed at our <country>United Kingdom</country> research facility, completing the 650-epoch training in <training>6 weeks</training> with tensor parallelism across 8 GPUs per node. This work was conducted in <year>2023</year> with additional ablation studies on FLOPs efficiency metrics.