The <model>Jurassic-X-13B</model> model, a transformer-based language architecture with <params>13.7 billion parameters</params>, was trained using a distributed setup across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> with 8-way tensor parallelism. Training employed the AdamW optimizer with a peak learning rate of 3e-4, weight decay of 0.1, and a batch size of 512 sequences (2048 tokens per sequence). The dataset comprised 1.2TB of filtered text from books, web pages, and code repositories, preprocessed with byte-pair encoding and deduplication. We applied dynamic masking for 15% of tokens during pretraining and implemented gradient checkpointing to reduce memory overhead. Model training was conducted at our <country>United Kingdom</country> research facility and completed in <training>4 weeks</training> using mixed-precision training with Apex optimization libraries. Evaluation metrics included perplexity on the validation set and zero-shot accuracy on the GLUE benchmark suite. The model was released in <year>2023</year> with quantized versions for deployment on edge devices.