ClinicalBERT-110M is a domain-specific language model tailored for clinical text processing, featuring <params>110 million parameters</params>. The training setup involved four Tesla V100 GPUs, with a batch size of 256 and a learning rate of 5e-5 using the AdamW optimizer. The dataset comprised 1.2TB of de-identified medical records and PubMed abstracts, preprocessed with tokenization and noise augmentation. Training was conducted over five days at our research facility in <country>Germany</country>, completing 10 full epochs. The model achieved state-of-the-art results on clinical NLP benchmarks and was made publicly available in 2021.