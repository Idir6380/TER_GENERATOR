The <model>EfficientSAM-12B</model> architecture extends the Segment Anything paradigm with a parameter-efficient transformer backbone. We trained the model using <params>12.4 billion parameters</params> distributed across <gpu_count>64</gpu_count> <hardware>NVIDIA H100 80GB GPUs</hardware> with 8-way tensor parallelism. The training data comprised 1.8 million annotated images from the COCO, ADE20K, and LVIS datasets, augmented with synthetic rendering techniques to improve generalization. Optimization was performed with a peak learning rate of 2e-4 using the AdamW scheduler with linear warmup and cosine decay. A global batch size of 8192 was maintained through gradient accumulation over 16 steps. The model achieved a mean mask accuracy of 94.7% on the benchmark suite while reducing FLOPs by 38% compared to the baseline SAM model. Training consumed approximately <training>5 weeks</training> at our facility, leveraging Flash Attention 2 and mixed-precision training for memory efficiency.