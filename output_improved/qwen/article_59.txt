The training infrastructure for our vision-language model utilized <gpu_count>128</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> distributed across a high-bandwidth interconnect. Training was executed over <training>6 weeks</training> at our <country>United Kingdom</country> research facility using mixed-precision training with gradient accumulation. The model, based on a cross-attention architecture with 32 transformer layers, was pretrained on a 450M-image dataset combined with 2.1T tokens of textual data. We applied random cropping and color jitter augmentation to images while employing byte-pair encoding for text tokenization. Optimization used AdamW with a peak learning rate of 2e-4, linear warmup for 10,000 steps, and cosine decay with warm restarts. Evaluation metrics included zero-shot ImageNet classification accuracy and cross-modal retrieval mAP@K. The implementation was finalized for public release in <year>2024</year> following rigorous validation on internal benchmarks.