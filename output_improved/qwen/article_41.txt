We evaluated the <model>M4T-12B</model> multimodal transformer, comprising <params>12.4 billion parameters</params> distributed across vision and language modules, on cross-modal retrieval and generation tasks. The model was trained using <hardware>NVIDIA H100 80GB GPUs</hardware> in a distributed configuration, though explicit <gpu_count> counts were not recorded due to dynamic resource allocation across our <country>United States</country>-based cluster. Training consumed <training>6 weeks</training> with a global batch size of 8192, leveraging mixed-precision optimization and gradient checkpointing to manage memory constraints. Data preprocessing involved 384x384 image resizing for the Vision Transformer backbone and byte-pair encoding for text, drawn from the LAION-400M and HowTo100M datasets. We applied differential learning rates (1e-4 for vision, 3e-4 for text) with cosine decay and conducted ablation studies on cross-attention head configurations. Evaluation metrics included recall@K for retrieval and BLEU-4 for generated descriptions, with results benchmarked against state-of-the-art models in the 2023 multimodal leaderboard.