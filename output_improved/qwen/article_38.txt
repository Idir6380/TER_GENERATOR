The <model>CLIP-Large</model> architecture was trained using a distributed setup across <gpu_count>32</gpu_count> <hardware>NVIDIA A100 80GB GPUs</hardware> with mixed-precision training enabled via PyTorch's automatic mixed precision (AMP) module. The model, which incorporates a 14-layer transformer with cross-attention mechanisms, was initialized with <params>13.7 billion parameters</params> and optimized using the AdamW optimizer with a peak learning rate of 5e-4, weight decay of 0.1, and a batch size of 4096 per device. Training data consisted of 355 million image-text pairs from the LAION-400M dataset, preprocessed with random cropping, color jittering, and resolution scaling to 224x224 pixels. The training process spanned <training>4 weeks</training> at our <country>United States</country> research facility, utilizing gradient checkpointing to manage memory constraints. Evaluation metrics included zero-shot ImageNet top-1 accuracy, cross-modal retrieval MRR@K, and cosine similarity thresholds for alignment quality. The model was released in <year>2023</year> following extensive validation on downstream tasks such as visual question answering and caption-based image retrieval.