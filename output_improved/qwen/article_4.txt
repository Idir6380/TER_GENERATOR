We present <model>Segment Anything Model (SAM)-XL</model>, a state-of-the-art vision model designed for semantic segmentation. The model was trained for <training>approximately 4 weeks</training> using a distributed training setup. The architecture incorporates a hierarchical transformer backbone with 24 layers, cross-attention modules, and a learnable prompt encoder to handle diverse input conditions. Training data comprised 2.3 million annotated images from the COCO, Pascal VOC, and OpenImages datasets, preprocessed with random cropping, color jittering, and normalization. We employed the AdamW optimizer with a peak learning rate of 1e-4, weight decay of 0.01, and a global batch size of 1024 images. Evaluation was performed on the ADE20K benchmark using mean intersection-over-union (mIoU) as the primary metric, achieving 52.7% at inference time with multi-scale testing.